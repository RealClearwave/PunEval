{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Recognition&Explanation Task"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load Package"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2024-03-28T07:57:51.815943Z",
     "end_time": "2024-03-28T07:58:01.321393Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from random import shuffle\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "import google.generativeai as genai\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from _api_key import get_openai_api_key, get_google_api_key, get_claude_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Basic Function"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def load_json_file(file_path):\n",
    "    \"\"\"\n",
    "    Load json file\n",
    "    \"\"\"\n",
    "    with open(file_path,'r',encoding='utf-8') as f:\n",
    "        file = json.load(f)\n",
    "        f.close()\n",
    "    return file\n",
    "\n",
    "def save_json_file(file, file_path):\n",
    "    \"\"\"\n",
    "    Save json file\n",
    "    \"\"\"\n",
    "    with open(file_path,'w',encoding='utf-8') as f:\n",
    "        json.dump(file, f, indent=4, ensure_ascii=False)\n",
    "        f.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-28T07:58:01.321393Z",
     "end_time": "2024-03-28T07:58:01.337062Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def add_human_performance(whole_dataset, save:bool=False, path_rec='pun_recognition.json',\n",
    "                         path_expl='pun_explanation.json'):\n",
    "    \"\"\"\n",
    "    Add human performance (gold of recognition & explanation)\n",
    "    \"\"\"\n",
    "    path_rec = './results/' + path_rec\n",
    "    path_expl = './results/' + path_expl\n",
    "    if os.path.exists(path_rec):\n",
    "        pun_recognition = load_json_file(path_rec)\n",
    "    else:\n",
    "        pun_recognition = dict()\n",
    "    if os.path.exists(path_expl):\n",
    "        pun_explanation = load_json_file(path_expl)\n",
    "    else:\n",
    "        pun_explanation = dict()\n",
    "    # Distinguish between pun and non-pun\n",
    "    for ID in whole_dataset:\n",
    "        data = whole_dataset[ID]\n",
    "        # Puns\n",
    "        if data.get('pun_word', False):\n",
    "            # recognition\n",
    "            if ID not in pun_recognition:\n",
    "                pun_recognition[ID] = {'human_judge':1}\n",
    "            else:\n",
    "                pun_recognition[ID].update({'human_judge':1})\n",
    "            # explanation\n",
    "            explanation = data['human_explanation']\n",
    "            if ID not in pun_explanation:\n",
    "                pun_explanation[ID] = {'human_explanation':explanation}\n",
    "            else:\n",
    "                pun_explanation[ID].update({'human_explanation':explanation})\n",
    "        # Non-puns\n",
    "        else:\n",
    "            # recognition\n",
    "            if ID not in pun_recognition:\n",
    "                pun_recognition[ID] = {'human_judge':0}\n",
    "            else:\n",
    "                pun_recognition[ID].update({'human_judge':0})\n",
    "            # explanation\n",
    "            if ID not in pun_explanation:\n",
    "                pun_explanation[ID] = {'human_explanation':'None'}\n",
    "            else:\n",
    "                pun_explanation[ID].update({'human_explanation':'None'})\n",
    "    if save:\n",
    "        save_json_file(pun_recognition, path_rec)\n",
    "        save_json_file(pun_explanation, path_expl)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-28T07:58:01.337062Z",
     "end_time": "2024-03-28T07:58:01.399978Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Function of recognition\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def call_llm_to_recognize(model, dataset, model_name:str=None, add_def:bool=False, add_CoT:bool=False, add_examples:dict=None,\n",
    "                          save:bool=False, path_rec='pun_recognition1.json', path_expl='pun_explanation1.json',\n",
    "                          batch_size:int=1):\n",
    "    \"\"\"\n",
    "    Pun recognition task, with a focus on evaluating accuracy(TPR,TNR) and consistency(kappa)  \\n\n",
    "    Possible parts of prompt: definition, instruction, examples, testing \\n\n",
    "    Collect the 'reason' part of CoT as explanation of the text (pun/non-pun explanation)\n",
    "    \"\"\"\n",
    "    def parse_output(ID, output:str, with_expl:bool):\n",
    "        # Parse the output and get result\n",
    "        try:\n",
    "            output = output[output.index('{'): output.index('}')+1]\n",
    "        except:\n",
    "            output = output\n",
    "        try:\n",
    "            output = eval(output)\n",
    "            choice = output['Choice']\n",
    "            if with_expl:\n",
    "                expl =  output['Reason']\n",
    "        except:\n",
    "            try:\n",
    "                choice = output.split('The given text is a')[-1]\n",
    "                if with_expl:\n",
    "                    expl = output.split('Reason')[1].split('Choice')[0]\n",
    "                # print(ID, output)\n",
    "            except:\n",
    "                choice = 'No-result'\n",
    "                if with_expl:\n",
    "                    expl = \"No correctly parsed result.\"\n",
    "        choice = choice.lower()\n",
    "        if 'non-pun' in choice:\n",
    "            judge = 0\n",
    "        elif 'pun' in choice:\n",
    "            judge = 1\n",
    "        else:\n",
    "            judge = -1\n",
    "        if with_expl:\n",
    "            return judge, expl\n",
    "        else:\n",
    "            return judge\n",
    "\n",
    "    path_rec = './results/' + path_rec\n",
    "    path_expl = './results/' + path_expl\n",
    "    if os.path.exists(path_rec):  # pun recognition\n",
    "        record_rec = load_json_file(path_rec)\n",
    "    else:\n",
    "        record_rec = dict()\n",
    "    if add_CoT:  # pun explanation\n",
    "        if os.path.exists(path_expl):\n",
    "            record_expl = load_json_file(path_expl)\n",
    "        else:\n",
    "            record_expl = dict()\n",
    "    # [A]. Construct the prompt\n",
    "    # 1. Give a definition or not\n",
    "    if add_def:\n",
    "        definition = \"\"\"<*Definition*>\\nPuns are a form of wordplay exploiting different meanings of a word or similar-sounding words, while non-puns are jokes or statements that don't rely on such linguistic ambiguities.\\n\\n\"\"\"\n",
    "    else:\n",
    "        definition = ''\n",
    "    # 2. CoT or not\n",
    "    if add_CoT:\n",
    "        instruction = \"\"\"<*Instruction*>\\nDetermine whether the given Text is a {side}. Give your reasons first, then make your final decision clearly. You should either say \"The given text is a pun\" or say \"The given text is a non-pun\". You must output the current status in a parsable JSON format. An example output looks like:\\n{{\"Reason\": \"XXX\", \"Choice\": \"The given text is a XXX\"}}\"\"\"\n",
    "    else:\n",
    "        instruction = \"\"\"<*Instruction*>\\nDetermine whether the given Text is a {side}. You should either say \"The given text is a pun\" or say \"The given text is a non-pun\". You must output the current status in a parsable JSON format. An example output looks like:\\n{{\"Choice\": \"The given text is a XXX\"}}\"\"\"\n",
    "    # 3. Add examples or not (zero/6-shot)\n",
    "    if add_examples is not None:\n",
    "        examples_temp = []\n",
    "        for ID in add_examples:\n",
    "            example = add_examples[ID]\n",
    "            if add_CoT:\n",
    "                examples_temp.append(f\"Text: {example['text']}\\nOutput:\\n\"\n",
    "                                     f\"{{{{\\\"Reason\\\": \\\"{example['reason']}\\\", \"\n",
    "                                     f\"\\\"Choice\\\": \\\"{example['label']}\\\"}}}}\")\n",
    "            else:\n",
    "                examples_temp.append(f\"Text: {example['text']}\\nOutput:\\n\"\n",
    "                                     f\"{{{{\\\"Choice\\\": \\\"{example['label']}\\\"}}}}\")\n",
    "        examples_string = '\\n\\n<*Examples*>\\n' + '\\n\\n'.join(examples_temp)\n",
    "    else:\n",
    "        examples_string = ''\n",
    "    # 4. Test data\n",
    "    if add_examples is not None:\n",
    "        testing = \"\\n\\n<*Your Response*>\\nText: {text}\\nOutput:\"\n",
    "    else:\n",
    "        testing = \"\\n\\n<*Your Response*>\\nText: {text}\\nOutput:\"\n",
    "    # 5. Combine all parts together\n",
    "    prompt_string = definition + instruction + examples_string + testing\n",
    "    chat_prompt = ChatPromptTemplate.from_template(prompt_string)\n",
    "    # [B]. Call LLM to respond\n",
    "    if model_name is None:\n",
    "        model_name = model.model_name if hasattr(model,'model_name') else model.model\n",
    "        model_name = model_name.split('/')[-1]\n",
    "    key_rec = f\"{model_name}_judge def_{str(add_def).lower()} \" \\\n",
    "              f\"CoT_{str(add_CoT).lower()} examples_{str(add_examples is not None).lower()}\"\n",
    "    key_expl = f\"{model_name}_explanation def_{str(add_def).lower()} \" \\\n",
    "                   f\"CoT_{str(add_CoT).lower()} examples_{str(add_examples is not None).lower()}\"\n",
    "    IDs = list(dataset.keys())\n",
    "    shuffle(IDs); shuffle(IDs)\n",
    "    IDs_loaded = []\n",
    "    for ID in record_rec:\n",
    "        if record_rec[ID].get(key_rec, False):\n",
    "            IDs_loaded.append(ID)\n",
    "    all_ind = list(range(0,len(IDs)))\n",
    "    batch_ind = list(range(0,len(IDs),batch_size))\n",
    "    for ind in tqdm(all_ind):\n",
    "        if ind not in batch_ind:\n",
    "            continue\n",
    "        IDs_batch = IDs[ind: ind+batch_size]\n",
    "        # Remove the data that has already been run\n",
    "        IDs_batch = list(set(IDs_batch)-set(IDs_loaded))\n",
    "        if len(IDs_batch) == 0:\n",
    "            continue\n",
    "        biased_to = {1:'pun',2:'non-pun'}\n",
    "        _inputs1, _outputs1 = [], []\n",
    "        _inputs2, _outputs2 = [], []\n",
    "        for ID in IDs_batch:\n",
    "            data = dataset[ID]\n",
    "            text = data['human_text']\n",
    "            _inputs1.append(chat_prompt.format_messages(text=text, side=biased_to[1]))\n",
    "            _inputs2.append(chat_prompt.format_messages(text=text, side=biased_to[2]))\n",
    "        # Gemini's native SDK does not support batch\n",
    "        if 'gemini' in model_name:\n",
    "            for _input in _inputs1:\n",
    "                _outputs1.append(model.generate_content(_input[0].content).text)\n",
    "            for _input in _inputs2:\n",
    "                _outputs2.append(model.generate_content(_input[0].content).text)\n",
    "        # Other models can use batch\n",
    "        else:\n",
    "            _outputs1 = [o1.content for o1 in model.batch(_inputs1)]\n",
    "            _outputs2 = [o2.content for o2 in model.batch(_inputs2)]\n",
    "        # print(_inputs1[0][0].content)\n",
    "        # print(_outputs1[0])\n",
    "        # print()\n",
    "        # print(_inputs2[0][0].content)\n",
    "        # print(_outputs2[0])\n",
    "        # break\n",
    "        for ID,o1,o2 in zip(IDs_batch, _outputs1, _outputs2):\n",
    "            if add_CoT:\n",
    "                rec1, expl1 = parse_output(ID, o1, with_expl=True)\n",
    "                rec2, expl2 = parse_output(ID, o2, with_expl=True)\n",
    "                recognition = {f'biased_to_{biased_to[1]}':rec1, f'biased_to_{biased_to[2]}':rec2}\n",
    "                explanation = {f'biased_to_{biased_to[1]}':expl1, f'biased_to_{biased_to[2]}':expl2}\n",
    "                if ID not in record_rec:\n",
    "                    record_rec[ID] = {key_rec: recognition}\n",
    "                else:\n",
    "                    record_rec[ID].update({key_rec: recognition})\n",
    "                if ID not in record_expl:\n",
    "                    record_expl[ID] = {key_expl: explanation}\n",
    "                else:\n",
    "                    record_expl[ID].update({key_expl: explanation})\n",
    "            else:\n",
    "                rec1 = parse_output(ID, o1, with_expl=False)\n",
    "                rec2 = parse_output(ID, o2, with_expl=False)\n",
    "                recognition = {f'biased_to_{biased_to[1]}':rec1, f'biased_to_{biased_to[2]}':rec2}\n",
    "                if ID not in record_rec:\n",
    "                    record_rec[ID] = {key_rec: recognition}\n",
    "                else:\n",
    "                    record_rec[ID].update({key_rec: recognition})\n",
    "        if save:\n",
    "            save_json_file(record_rec, path_rec)\n",
    "            if add_CoT:\n",
    "                save_json_file(record_expl, path_expl)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-28T07:58:01.352638Z",
     "end_time": "2024-03-28T07:58:01.399978Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def call_llm_to_give_pun_definition(model, model_name:str=None, save:bool=False, path='pun_definition.json'):\n",
    "    \"\"\"\n",
    "    Check if the LLM knows the difference between pun and non-pun\n",
    "    \"\"\"\n",
    "    path = './results/' + path\n",
    "    if os.path.exists(path):\n",
    "        record = load_json_file(path)\n",
    "    else:\n",
    "        record = dict()\n",
    "    if model_name is None:\n",
    "        model_name = model.model_name if hasattr(model,'model_name') else model.model\n",
    "        model_name = model_name.split('/')[-1]\n",
    "    key_def = f'{model_name}_definition'\n",
    "    # Call LLM to respond\n",
    "    _input = \"Tell me the difference between puns and non-puns, using no more than 60 words.\"\n",
    "    if 'gemini' in model_name:\n",
    "        _output = model.generate_content(_input).text\n",
    "    else:\n",
    "        _output = model.invoke(_input).content\n",
    "    print(_output)\n",
    "    record[key_def] = _output\n",
    "    if save:\n",
    "        save_json_file(record, path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-28T07:58:01.368265Z",
     "end_time": "2024-03-28T07:58:01.399978Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset and Examples"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "hom_path = r'./dataset/hom_dataset.json'\n",
    "het_path = r'./dataset/het_dataset.json'\n",
    "hom_dataset = load_json_file(hom_path)\n",
    "het_dataset = load_json_file(het_path)\n",
    "\n",
    "# add_human_performance(dict(**hom_dataset,**het_dataset), save=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-28T07:58:01.388463Z",
     "end_time": "2024-03-28T07:58:01.415660Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Choose data from examples manually\n",
    "hom_examples = {\n",
    "    \"hom_705\":{\"text\":\"Driving on so many turnpikes was taking its toll .\",\n",
    "               \"reason\":\"The text is using the word 'toll' in a double entendre. It refers both to the physical tolls paid on turnpikes and to 'taking its toll' as in having a negative effect or cost.\",\n",
    "               \"label\":\"The given text is a pun\"},\n",
    "    \"hom_533\":{\"text\":\"Don ' t kill the goose that lays the golden eggs .\",\n",
    "               \"reason\":\"The text is a proverb warning against destructive greed and does not exploit different meanings of a word or similar-sounding words for humorous effect.\",\n",
    "               \"label\":\"The given text is a non-pun\"},\n",
    "    \"hom_488\":{\"text\":\"A carpenter sat on his drill and was bored to tears .\",\n",
    "               \"reason\":\"The text plays on the double meaning of 'bored'. A carpenter using a drill creates a bore or hole, while 'bored to tears' is an expression used when someone is extremely bored. Thus, it exploits the different meanings of the word 'bored'.\",\n",
    "               \"label\":\"The given text is a pun\"},\n",
    "    \"hom_639\":{\"text\":\"When all is said and done , more is said than done .\",\n",
    "               \"reason\":\"The text plays on the juxtaposition of the concepts of speaking and doing to highlight a common human behavior of talking more than taking action. It does not rely on the different meanings of a single word or similar sounding words.\",\n",
    "               \"label\":\"The given text is a non-pun\"},\n",
    "    \"hom_1556\":{\"text\":\"One leftover said to another ' foiled again . '\",\n",
    "                \"reason\":\"The joke is based on the double meaning of the word 'foiled.' One meaning is to be thwarted or defeated, and the other refers to being wrapped in foil, which is what often happens to leftovers.\",\n",
    "                \"label\":\"The given text is a pun\"},\n",
    "    \"hom_1167\":{\"text\":\"Nothing ventured , nothing gained .\",\n",
    "                \"reason\":\"The given text is a proverb that expresses a general truth or piece of advice and does not exploit different meanings of a word or similar-sounding words.\",\n",
    "                \"label\":\"The given text is a non-pun\"}\n",
    "}\n",
    "\n",
    "het_examples = {\n",
    "    \"het_621\":{\"text\":\"When the waiter told me they were out of corn I said , ' That really shucks . '\",\n",
    "               \"reason\":\"The text plays on the double meaning of the word 'shucks'. 'Shucks' refers to both the act of removing the husk from corn and is a homophone for 'sucks', which is used colloquially to express disappointment.\",\n",
    "               \"label\":\"The given text is a pun\"},\n",
    "    \"het_41\":{\"text\":\"Desperate times call for desperate measures .\",\n",
    "              \"reason\":\"The text is an idiomatic expression meaning that one may need to take drastic actions in difficult situations. It does not exploit different meanings of a word or similar-sounding words.\",\n",
    "              \"label\":\"The given text is a non-pun\"},\n",
    "    \"het_530\":{\"text\":\"A tangled bell ringer tolled himself off .\",\n",
    "             \"reason\":\"The text plays on the homophones 'tolled' and 'told', using the word 'tolled' in the context of a bell ringer (which relates to the ringing or tolling of bells) and 'told' as in scolding oneself (told sb off). This creates a humorous double meaning.\",\n",
    "             \"label\":\"The given text is a pun\"},\n",
    "    \"het_225\":{\"text\":\"Don ' t bite the hand that feeds you .\",\n",
    "               \"reason\":\"The text is an idiomatic expression meaning one should not act ungratefully towards those who provide for them. It does not rely on a play on words or different meanings of the same word.\",\n",
    "                \"label\":\"The given text is a non-pun\"},\n",
    "    \"het_325\":{\"text\":\"An illiterate fisherman was lost at c .\",\n",
    "               \"reason\":\"The text exploits the homophonic nature of the letter 'C' and the word 'sea', playing on the expectation that 'lost at sea' is a common expression, but humorously substituting 'sea' for 'C' to suggest that the fisherman, being illiterate, is lost at the letter.\",\n",
    "               \"label\":\"The given text is a pun\"},\n",
    "    \"het_563\":{\"text\":\"Better go about than fall into the ditch .\",\n",
    "              \"reason\":\"The text is an idiomatic expression that suggests it's better to be cautious than to get into trouble. It does not rely on the ambiguity of words or similar-sounding words for a humorous effect.\",\n",
    "              \"label\":\"The given text is a non-pun\"}\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-28T07:58:01.415660Z",
     "end_time": "2024-03-28T07:58:01.478118Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Recognition"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### gpt3.5"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# Connect gpt-3.5-turbo-1106\n",
    "gpt35_name = 'gpt-3.5-turbo-1106'\n",
    "temperature = 0.0\n",
    "openai_api_key = get_openai_api_key()  # use your api key\n",
    "gpt35 = ChatOpenAI(model_name=gpt35_name, temperature=temperature,\n",
    "                   openai_api_key=openai_api_key, request_timeout=120)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-04T21:16:15.576695Z",
     "end_time": "2024-02-04T21:16:15.641324Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Puns are a form of wordplay that relies on the multiple meanings of a word or the similarity in sound between different words to create humor. Non-puns, on the other hand, do not rely on wordplay for their humor. They may use other comedic devices such as irony, exaggeration, or situational humor to elicit laughter from the audience.\n"
     ]
    }
   ],
   "source": [
    "call_llm_to_give_pun_definition(model=gpt35, save=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-04T21:16:40.406687Z",
     "end_time": "2024-02-04T21:16:42.873397Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### bare (0-shot)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 159/1443 [17:35<2:51:59,  8.04s/it] Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised ServiceUnavailableError: The server is overloaded or not ready yet..\n",
      " 61%|██████    | 878/1443 [1:30:57<45:44,  4.86s/it]  Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=120.0).\n",
      " 77%|███████▋  | 1113/1443 [1:54:20<24:56,  4.53s/it] Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised ServiceUnavailableError: The server is overloaded or not ready yet..\n",
      "100%|██████████| 1443/1443 [2:25:23<00:00,  6.05s/it]  \n",
      "100%|██████████| 1146/1146 [1:25:00<00:00,  4.45s/it]\n"
     ]
    }
   ],
   "source": [
    "call_llm_to_recognize(model=gpt35, dataset=hom_dataset, save=True)\n",
    "call_llm_to_recognize(model=gpt35, dataset=het_dataset, save=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### +def (0-shot)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 732/1443 [54:03<1:01:28,  5.19s/it]Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised ServiceUnavailableError: The server is overloaded or not ready yet..\n",
      "100%|██████████| 1443/1443 [1:45:17<00:00,  4.38s/it]\n",
      "100%|██████████| 1146/1146 [1:08:24<00:00,  3.58s/it]\n"
     ]
    }
   ],
   "source": [
    "call_llm_to_recognize(model=gpt35, dataset=hom_dataset, save=True,\n",
    "                      add_def=True)\n",
    "call_llm_to_recognize(model=gpt35, dataset=het_dataset, save=True,\n",
    "                      add_def=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### +def (6-shot)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 1435/1443 [1:56:38<00:37,  4.73s/it]Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised ServiceUnavailableError: The server is overloaded or not ready yet..\n",
      "100%|██████████| 1443/1443 [1:59:17<00:00,  4.96s/it]\n",
      " 22%|██▏       | 255/1146 [24:18<1:09:46,  4.70s/it]Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=120.0).\n",
      " 68%|██████▊   | 777/1146 [1:13:50<33:25,  5.43s/it] Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=120.0).\n",
      "100%|██████████| 1146/1146 [1:47:35<00:00,  5.63s/it] \n"
     ]
    }
   ],
   "source": [
    "call_llm_to_recognize(model=gpt35, dataset=hom_dataset, save=True,\n",
    "                      add_def=True, add_examples=hom_examples)\n",
    "call_llm_to_recognize(model=gpt35, dataset=het_dataset, save=True,\n",
    "                      add_def=True, add_examples=het_examples)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### +def&CoT (6-shot)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5/1443 [00:46<3:31:55,  8.84s/it]Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=120.0).\n",
      " 83%|████████▎ | 1193/1443 [2:33:53<32:11,  7.72s/it] Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=120.0).\n",
      "100%|██████████| 1443/1443 [3:06:49<00:00,  7.77s/it]  \n",
      " 14%|█▎        | 155/1146 [17:45<1:45:45,  6.40s/it]Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=120.0).\n",
      "100%|██████████| 1146/1146 [2:00:50<00:00,  6.33s/it] \n"
     ]
    }
   ],
   "source": [
    "call_llm_to_recognize(model=gpt35, dataset=hom_dataset, save=True,\n",
    "                      add_def=True, add_CoT=True, add_examples=hom_examples)\n",
    "call_llm_to_recognize(model=gpt35, dataset=het_dataset, save=True,\n",
    "                      add_def=True, add_CoT=True, add_examples=het_examples)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### gpt4"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# Connect gpt-4-1106-preview\n",
    "gpt4_name = 'gpt-4-1106-preview'\n",
    "temperature = 0.0\n",
    "openai_api_key = get_openai_api_key()  # use your api key\n",
    "gpt4 = ChatOpenAI(model_name=gpt4_name, temperature=temperature,\n",
    "                  openai_api_key=openai_api_key, request_timeout=120)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-04T21:17:08.506044Z",
     "end_time": "2024-02-04T21:17:08.566287Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Puns are a form of wordplay that exploit multiple meanings of a term or similar-sounding words for an intended humorous or rhetorical effect. Non-puns are straightforward language without such double meanings or sound-based humor.\n"
     ]
    }
   ],
   "source": [
    "call_llm_to_give_pun_definition(model=gpt4, save=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-04T21:17:21.039668Z",
     "end_time": "2024-02-04T21:17:24.830624Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### bare (0-shot)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 821/1443 [1:01:06<38:54,  3.75s/it]  Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised APIError: Timed out generating response. Please try again with a shorter prompt or with `max_tokens` set to a lower value. {\n",
      "    \"error\": {\n",
      "        \"message\": \"Timed out generating response. Please try again with a shorter prompt or with `max_tokens` set to a lower value.\",\n",
      "        \"type\": \"internal_error\",\n",
      "        \"param\": null,\n",
      "        \"code\": \"request_timeout\"\n",
      "    }\n",
      "}\n",
      " 500 {'error': {'message': 'Timed out generating response. Please try again with a shorter prompt or with `max_tokens` set to a lower value.', 'type': 'internal_error', 'param': None, 'code': 'request_timeout'}} {'Date': 'Mon, 22 Jan 2024 16:21:11 GMT', 'Content-Type': 'application/json; charset=utf-8', 'Content-Length': '251', 'Connection': 'keep-alive', 'vary': 'Origin', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '600000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '599891', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '10ms', 'x-request-id': 'fbf1e1887d5417e619b7d257fd6f7fbe', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'CF-Cache-Status': 'DYNAMIC', 'Server': 'cloudflare', 'CF-RAY': '849920d51cc50ec4-HKG', 'alt-svc': 'h3=\":443\"; ma=86400'}.\n",
      " 58%|█████▊    | 832/1443 [1:02:05<54:18,  5.33s/it]  Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=120.0).\n",
      "100%|██████████| 1443/1443 [1:46:40<00:00,  4.44s/it] \n",
      "100%|██████████| 1146/1146 [1:20:02<00:00,  4.19s/it]\n"
     ]
    }
   ],
   "source": [
    "call_llm_to_recognize(model=gpt4, dataset=hom_dataset, save=True)\n",
    "call_llm_to_recognize(model=gpt4, dataset=het_dataset, save=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### +def (0-shot)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 251/1443 [16:34<1:03:40,  3.20s/it]Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=120.0).\n",
      " 33%|███▎      | 469/1443 [33:22<1:09:26,  4.28s/it] Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised ServiceUnavailableError: The server is overloaded or not ready yet..\n",
      "100%|██████████| 1443/1443 [1:34:33<00:00,  3.93s/it] \n",
      " 13%|█▎        | 153/1146 [09:06<53:05,  3.21s/it]  Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised ServiceUnavailableError: The server is overloaded or not ready yet..\n",
      "100%|██████████| 1146/1146 [1:07:45<00:00,  3.55s/it]\n"
     ]
    }
   ],
   "source": [
    "call_llm_to_recognize(model=gpt4, dataset=hom_dataset, save=True,\n",
    "                      add_def=True)\n",
    "call_llm_to_recognize(model=gpt4, dataset=het_dataset, save=True,\n",
    "                      add_def=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### +def (6-shot)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1443/1443 [1:19:47<00:00,  3.32s/it]\n",
      "100%|██████████| 1146/1146 [1:04:20<00:00,  3.37s/it]\n"
     ]
    }
   ],
   "source": [
    "call_llm_to_recognize(model=gpt4, dataset=hom_dataset, save=True,\n",
    "                      add_def=True, add_examples=hom_examples)\n",
    "call_llm_to_recognize(model=gpt4, dataset=het_dataset, save=True,\n",
    "                      add_def=True, add_examples=het_examples)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### +def&CoT (6-shot)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1443/1443 [4:14:14<00:00, 10.57s/it]  \n",
      " 41%|████▏     | 473/1146 [1:37:43<2:33:09, 13.66s/it]Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised APIConnectionError: Error communicating with OpenAI: HTTPSConnectionPool(host='api.openai.com', port=443): Max retries exceeded with url: /v1/chat/completions (Caused by ProxyError('Cannot connect to proxy.', ConnectionAbortedError(10053, '你的主机中的软件中止了一个已建立的连接。', None, 10053, None))).\n",
      "100%|██████████| 1146/1146 [4:21:46<00:00, 13.71s/it] \n"
     ]
    }
   ],
   "source": [
    "call_llm_to_recognize(model=gpt4, dataset=hom_dataset, save=True,\n",
    "                      add_def=True, add_CoT=True, add_examples=hom_examples)\n",
    "call_llm_to_recognize(model=gpt4, dataset=het_dataset, save=True,\n",
    "                      add_def=True, add_CoT=True, add_examples=het_examples)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### gemini-pro"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# Connect gemini-pro\n",
    "gemini_name = 'gemini-pro'\n",
    "temperature = 0.0\n",
    "google_api_key = get_google_api_key()  # use your api key\n",
    "genai.configure(api_key=google_api_key, transport='rest')\n",
    "safety_settings=[\n",
    "    {\n",
    "        \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
    "        \"threshold\": \"BLOCK_NONE\",\n",
    "    },\n",
    "    {\n",
    "        \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
    "        \"threshold\": \"BLOCK_NONE\",\n",
    "    },\n",
    "    {\n",
    "        \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
    "        \"threshold\": \"BLOCK_NONE\",\n",
    "    },\n",
    "    {\n",
    "        \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
    "        \"threshold\": \"BLOCK_NONE\",\n",
    "    }\n",
    "]\n",
    "generation_config = {\"temperature\": temperature}\n",
    "gemini = genai.GenerativeModel(model_name=gemini_name, safety_settings=safety_settings,\n",
    "                               generation_config=generation_config)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-04T21:18:01.507714Z",
     "end_time": "2024-02-04T21:18:01.538400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Puns are a form of wordplay that exploits multiple meanings of a term, or of similar-sounding words, for humorous or rhetorical effect. Non-puns, on the other hand, are statements or expressions that do not rely on wordplay or ambiguity for their meaning or humor.\n"
     ]
    }
   ],
   "source": [
    "call_llm_to_give_pun_definition(model=gemini, save=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-04T21:19:13.328017Z",
     "end_time": "2024-02-04T21:19:15.165649Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### bare (0-shot)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1443/1443 [1:28:50<00:00,  3.69s/it]\n",
      "100%|██████████| 1146/1146 [1:07:15<00:00,  3.52s/it]\n"
     ]
    }
   ],
   "source": [
    "call_llm_to_recognize(model=gemini, dataset=hom_dataset, save=True)\n",
    "call_llm_to_recognize(model=gemini, dataset=het_dataset, save=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-29T00:52:38.569615Z",
     "end_time": "2024-01-29T03:28:44.016837Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### +def (0-shot)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1443/1443 [1:27:24<00:00,  3.63s/it]\n",
      "100%|██████████| 1146/1146 [1:39:35<00:00,  5.21s/it] \n"
     ]
    }
   ],
   "source": [
    "call_llm_to_recognize(model=gemini, dataset=hom_dataset, save=True,\n",
    "                      add_def=True)\n",
    "call_llm_to_recognize(model=gemini, dataset=het_dataset, save=True,\n",
    "                      add_def=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-29T03:28:44.016837Z",
     "end_time": "2024-01-29T06:35:44.592452Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### +def (6-shot)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1443/1443 [25:37<00:00,  1.07s/it] \n",
      "100%|██████████| 1146/1146 [1:23:27<00:00,  4.37s/it]\n"
     ]
    }
   ],
   "source": [
    "call_llm_to_recognize(model=gemini, dataset=hom_dataset, save=True,\n",
    "                      add_def=True, add_examples=hom_examples)\n",
    "call_llm_to_recognize(model=gemini, dataset=het_dataset, save=True,\n",
    "                      add_def=True, add_examples=het_examples)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### +def&CoT (6-shot)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 1124/1443 [1:58:55<30:23,  5.72s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hom_1115 {\"Reason\": \"The text is a humorous play on the literal meaning of \"Insert disk # 3\" and the physical limitation of only being able to fit 2 disks. It exploits the different meanings of \"Insert disk # 3\" to create a humorous situation.\", \"Choice\": \"The given text is a pun\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1443/1443 [2:29:25<00:00,  6.21s/it]\n",
      "100%|██████████| 1146/1146 [1:56:08<00:00,  6.08s/it] \n"
     ]
    }
   ],
   "source": [
    "call_llm_to_recognize(model=gemini, dataset=hom_dataset, save=True,\n",
    "                      add_def=True, add_CoT=True, add_examples=hom_examples)\n",
    "call_llm_to_recognize(model=gemini, dataset=het_dataset, save=True,\n",
    "                      add_def=True, add_CoT=True, add_examples=het_examples)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### claude3"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# Connect claude-3-opus-20240229\n",
    "claude3_name = 'claude-3-opus-20240229'\n",
    "temperature = 0.0\n",
    "claude_api_key = get_claude_api_key()  # use your api key\n",
    "claude3 = ChatAnthropic(model_name=claude3_name, temperature=temperature,\n",
    "                        anthropic_api_key=claude_api_key)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-28T07:58:24.068258Z",
     "end_time": "2024-03-28T07:58:24.177767Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Puns are a form of wordplay that exploit the multiple meanings of a word or the similarity in sound between different words for humorous effect. Non-puns are straightforward statements that do not involve any wordplay or intentional ambiguity. Puns are often used to create jokes, while non-puns are used to convey information clearly and directly.\n"
     ]
    }
   ],
   "source": [
    "call_llm_to_give_pun_definition(model=claude3, save=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-25T22:15:16.309146Z",
     "end_time": "2024-03-25T22:15:22.381729Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### bare (0-shot)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1443/1443 [00:00<00:00, 30783.22it/s]\n",
      "100%|██████████| 1146/1146 [00:00<00:00, 35361.12it/s]\n"
     ]
    }
   ],
   "source": [
    "call_llm_to_recognize(model=claude3, dataset=hom_dataset, save=True)\n",
    "call_llm_to_recognize(model=claude3, dataset=het_dataset, save=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-26T12:53:20.336011Z",
     "end_time": "2024-03-26T12:53:20.455585Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### +def (0-shot)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1443/1443 [58:18<00:00,  2.42s/it]  \n",
      "100%|██████████| 1146/1146 [45:49<00:00,  2.40s/it] \n"
     ]
    }
   ],
   "source": [
    "call_llm_to_recognize(model=claude3, dataset=hom_dataset, save=True,\n",
    "                      add_def=True)\n",
    "call_llm_to_recognize(model=claude3, dataset=het_dataset, save=True,\n",
    "                      add_def=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-27T09:48:01.542637Z",
     "end_time": "2024-03-27T11:32:08.856489Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### +def (6-shot)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1443/1443 [00:00<00:00, 38829.17it/s]\n",
      "100%|██████████| 1146/1146 [19:07<00:00,  1.00s/it]\n"
     ]
    }
   ],
   "source": [
    "call_llm_to_recognize(model=claude3, dataset=hom_dataset, batch_size=4, save=True,\n",
    "                      add_def=True, add_examples=hom_examples)\n",
    "call_llm_to_recognize(model=claude3, dataset=het_dataset, batch_size=4, save=True,\n",
    "                      add_def=True, add_examples=het_examples)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-28T07:58:51.400884Z",
     "end_time": "2024-03-28T08:17:59.030050Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### +def&CoT (6-shot)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1443/1443 [00:00<00:00, 103026.26it/s]\n",
      "100%|██████████| 1146/1146 [1:36:12<00:00,  5.04s/it]\n"
     ]
    }
   ],
   "source": [
    "call_llm_to_recognize(model=claude3, dataset=hom_dataset, save=True,\n",
    "                      add_def=True, add_CoT=True, add_examples=hom_examples)\n",
    "call_llm_to_recognize(model=claude3, dataset=het_dataset, save=True,\n",
    "                      add_def=True, add_CoT=True, add_examples=het_examples)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-26T18:25:36.181285Z",
     "end_time": "2024-03-26T20:01:48.293350Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### vicuna"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# Connect to a server\n",
    "# Use local langChain with fastChat (Terminal code)\n",
    "# python3 -m fastchat.serve.controller\n",
    "# python3 -m fastchat.serve.model_worker --model-names \"gpt-3.5-turbo,text-davinci-003,text-embedding-ada-002\" --model-path lmsys/vicuna-7b-v1.5\n",
    "# python3 -m fastchat.serve.openai_api_server --host localhost --port 8000\n",
    "\n",
    "# Connect vicuna-7b-v1.5\n",
    "os.environ['OPENAI_API_BASE'] = 'http://localhost:8000/v1'\n",
    "os.environ['OPENAI_API_KEY'] = 'EMPTY'\n",
    "\n",
    "vicuna_name = 'vicuna-7b-v1.5'\n",
    "temperature = 0\n",
    "max_tokens = 200\n",
    "vicuna = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=temperature, max_tokens=max_tokens)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-04T21:26:10.564830Z",
     "end_time": "2024-02-04T21:26:10.674937Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Puns are a type of word play that exploit multiple meanings of a term, or of similar-sounding words, these meanings being at least somewhat related to the referents for which the terms stand. Non-puns, on the other hand, do not rely on multiple meanings or similar-sounding words to create their effect. They may use word play for comedic effect, but they do not rely on the dual meaning of a term or similar-sounding words.\n"
     ]
    }
   ],
   "source": [
    "call_llm_to_give_pun_definition(model=vicuna, model_name=vicuna_name, save=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-04T21:26:39.007345Z",
     "end_time": "2024-02-04T21:26:40.969158Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### bare (0-shot)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1443/1443 [13:22<00:00,  1.80it/s]\n",
      "100%|██████████| 1146/1146 [11:31<00:00,  1.66it/s]\n"
     ]
    }
   ],
   "source": [
    "call_llm_to_recognize(model=vicuna, model_name=vicuna_name, dataset=hom_dataset, batch_size=10, save=True)\n",
    "call_llm_to_recognize(model=vicuna, model_name=vicuna_name, dataset=het_dataset, batch_size=10, save=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-02T19:35:23.677473Z",
     "end_time": "2024-02-02T20:00:17.602386Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### +def (0-shot)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1443/1443 [14:15<00:00,  1.69it/s]\n",
      "100%|██████████| 1146/1146 [11:24<00:00,  1.67it/s]\n"
     ]
    }
   ],
   "source": [
    "call_llm_to_recognize(model=vicuna, model_name=vicuna_name, dataset=hom_dataset, batch_size=10, save=True,\n",
    "                      add_def=True)\n",
    "call_llm_to_recognize(model=vicuna, model_name=vicuna_name, dataset=het_dataset, batch_size=10, save=True,\n",
    "                      add_def=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-02T20:00:17.602386Z",
     "end_time": "2024-02-02T20:25:57.427155Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### +def (6-shot)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1443/1443 [16:57<00:00,  1.42it/s]\n",
      "100%|██████████| 1146/1146 [13:45<00:00,  1.39it/s]\n"
     ]
    }
   ],
   "source": [
    "call_llm_to_recognize(model=vicuna, model_name=vicuna_name, dataset=hom_dataset, batch_size=10, save=True,\n",
    "                      add_def=True, add_examples=hom_examples)\n",
    "call_llm_to_recognize(model=vicuna, model_name=vicuna_name, dataset=het_dataset, batch_size=10, save=True,\n",
    "                      add_def=True, add_examples=het_examples)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-03T00:03:42.290311Z",
     "end_time": "2024-02-03T00:34:24.949540Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### +def&CoT (6-shot)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1443/1443 [1:25:22<00:00,  3.55s/it]\n",
      "100%|██████████| 1146/1146 [1:14:37<00:00,  3.91s/it]\n"
     ]
    }
   ],
   "source": [
    "call_llm_to_recognize(model=vicuna, model_name=vicuna_name, dataset=hom_dataset, batch_size=10, save=True,\n",
    "                      add_def=True, add_CoT=True, add_examples=hom_examples)\n",
    "call_llm_to_recognize(model=vicuna, model_name=vicuna_name, dataset=het_dataset, batch_size=10, save=True,\n",
    "                      add_def=True, add_CoT=True, add_examples=het_examples)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-03T03:11:40.770249Z",
     "end_time": "2024-02-03T05:51:41.088895Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### llama2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# Connect to a server\n",
    "# Use local langChain with fastChat (Terminal code)\n",
    "# python3 -m fastchat.serve.controller\n",
    "# python3 -m fastchat.serve.model_worker --model-names \"gpt-3.5-turbo,text-davinci-003,text-embedding-ada-002\" --model-path meta-llama/Llama-2-7b-chat-hf\n",
    "# python3 -m fastchat.serve.openai_api_server --host localhost --port 8000\n",
    "\n",
    "# Connect llama-2-7b-chat\n",
    "os.environ['OPENAI_API_BASE'] = 'http://localhost:8000/v1'\n",
    "os.environ['OPENAI_API_KEY'] = 'EMPTY'\n",
    "\n",
    "llama2_name = 'llama-2-7b-chat'\n",
    "temperature = 0\n",
    "max_tokens = 200\n",
    "llama2 = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=temperature, max_tokens=max_tokens)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-05T00:44:58.263192Z",
     "end_time": "2024-02-05T00:44:58.644325Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Puns rely on wordplay, often using multiple meanings or sounds of words to create humor. Non-puns, on the other hand, use more straightforward language and rely on other forms of humor, such as irony or sarcasm.\n"
     ]
    }
   ],
   "source": [
    "call_llm_to_give_pun_definition(model=llama2, model_name=llama2_name, save=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-04T21:32:06.896582Z",
     "end_time": "2024-02-04T21:32:09.828202Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### bare (0-shot)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1443/1443 [17:50<00:00,  1.35it/s]\n",
      "100%|██████████| 1146/1146 [54:12<00:00,  2.84s/it]\n"
     ]
    }
   ],
   "source": [
    "call_llm_to_recognize(model=llama2, model_name=llama2_name, dataset=hom_dataset, batch_size=10, save=True)\n",
    "call_llm_to_recognize(model=llama2, model_name=llama2_name, dataset=het_dataset, batch_size=10, save=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-04T19:05:46.206192Z",
     "end_time": "2024-02-04T20:17:48.842244Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### +def (0-shot)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1443/1443 [00:00<00:00, 159659.72it/s]\n",
      "100%|██████████| 1146/1146 [32:47<00:00,  1.72s/it]\n"
     ]
    }
   ],
   "source": [
    "call_llm_to_recognize(model=llama2, model_name=llama2_name, dataset=hom_dataset, batch_size=10, save=True,\n",
    "                      add_def=True)\n",
    "call_llm_to_recognize(model=llama2, model_name=llama2_name, dataset=het_dataset, batch_size=10, save=True,\n",
    "                      add_def=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-04T21:33:20.707844Z",
     "end_time": "2024-02-04T22:06:08.725120Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### +def (6-shot)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1443/1443 [1:55:23<00:00,  4.80s/it] \n",
      "100%|██████████| 1146/1146 [1:18:37<00:00,  4.12s/it]\n"
     ]
    }
   ],
   "source": [
    "call_llm_to_recognize(model=llama2, model_name=llama2_name, dataset=hom_dataset, batch_size=10, save=True,\n",
    "                      add_def=True, add_examples=hom_examples)\n",
    "call_llm_to_recognize(model=llama2, model_name=llama2_name, dataset=het_dataset, batch_size=10, save=True,\n",
    "                      add_def=True, add_examples=het_examples)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-05T07:06:47.291955Z",
     "end_time": "2024-02-05T10:20:49.110954Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### +def&CoT (6-shot)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1443/1443 [2:14:40<00:00,  5.60s/it] \n",
      "100%|██████████| 1146/1146 [1:48:12<00:00,  5.67s/it]\n"
     ]
    }
   ],
   "source": [
    "call_llm_to_recognize(model=llama2, model_name=llama2_name, dataset=hom_dataset, batch_size=10, save=True,\n",
    "                      add_def=True, add_CoT=True, add_examples=hom_examples)\n",
    "call_llm_to_recognize(model=llama2, model_name=llama2_name, dataset=het_dataset, batch_size=10, save=True,\n",
    "                      add_def=True, add_CoT=True, add_examples=het_examples)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-05T14:52:42.339348Z",
     "end_time": "2024-02-05T18:55:35.316394Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### mistral"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# Connect to a server\n",
    "# Use local langChain with fastChat (Terminal code)\n",
    "# python3 -m fastchat.serve.controller\n",
    "# python3 -m fastchat.serve.model_worker --model-names \"gpt-3.5-turbo,text-davinci-003,text-embedding-ada-002\" --model-path mistralai/Mistral-7B-Instruct-v0.2\n",
    "# python3 -m fastchat.serve.openai_api_server --host localhost --port 8000\n",
    "\n",
    "# Connect mistral-7b-instruct-v0.2\n",
    "os.environ['OPENAI_API_BASE'] = 'http://localhost:8000/v1'\n",
    "os.environ['OPENAI_API_KEY'] = 'EMPTY'\n",
    "\n",
    "mistral_name = 'mistral-7b-instruct-v0.2'\n",
    "temperature = 0\n",
    "max_tokens = 200\n",
    "mistral = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=temperature, max_tokens=max_tokens)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-06T10:28:10.718618Z",
     "end_time": "2024-02-06T10:28:11.115022Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Puns are words, phrases, or sentences that exploit multiple meanings of a term, or of similar-sounding words, for an intended humorous or rhetorical effect. Non-puns lack this deliberate play on words or sounds. Puns rely on context and language nuances, while non-puns communicate straightforward meanings.\n"
     ]
    }
   ],
   "source": [
    "call_llm_to_give_pun_definition(model=mistral, model_name=mistral_name, save=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-05T20:23:38.195885Z",
     "end_time": "2024-02-05T20:23:39.737375Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### bare (0-shot)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1443/1443 [51:15<00:00,  2.13s/it]\n",
      "100%|██████████| 1146/1146 [43:27<00:00,  2.28s/it]\n"
     ]
    }
   ],
   "source": [
    "call_llm_to_recognize(model=mistral, model_name=mistral_name, dataset=hom_dataset, batch_size=10, save=True)\n",
    "call_llm_to_recognize(model=mistral, model_name=mistral_name, dataset=het_dataset, batch_size=10, save=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-05T21:06:17.801566Z",
     "end_time": "2024-02-05T22:41:00.631105Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### +def (0-shot)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1443/1443 [22:00<00:00,  1.09it/s]\n",
      "100%|██████████| 1146/1146 [35:56<00:00,  1.88s/it]\n"
     ]
    }
   ],
   "source": [
    "call_llm_to_recognize(model=mistral, model_name=mistral_name, dataset=hom_dataset, batch_size=10, save=True,\n",
    "                      add_def=True)\n",
    "call_llm_to_recognize(model=mistral, model_name=mistral_name, dataset=het_dataset, batch_size=10, save=True,\n",
    "                      add_def=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-05T23:14:46.559476Z",
     "end_time": "2024-02-06T00:12:43.842531Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### +def (6-shot)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1443/1443 [27:08<00:00,  1.13s/it]\n",
      "100%|██████████| 1146/1146 [22:00<00:00,  1.15s/it]\n"
     ]
    }
   ],
   "source": [
    "call_llm_to_recognize(model=mistral, model_name=mistral_name, dataset=hom_dataset, batch_size=10, save=True,\n",
    "                      add_def=True, add_examples=hom_examples)\n",
    "call_llm_to_recognize(model=mistral, model_name=mistral_name, dataset=het_dataset, batch_size=10, save=True,\n",
    "                      add_def=True, add_examples=het_examples)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-06T10:31:59.929585Z",
     "end_time": "2024-02-06T11:21:08.799468Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### +def&CoT (6-shot)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1443/1443 [1:22:29<00:00,  3.43s/it]\n",
      "100%|██████████| 1146/1146 [1:05:29<00:00,  3.43s/it]\n"
     ]
    }
   ],
   "source": [
    "call_llm_to_recognize(model=mistral, model_name=mistral_name, dataset=hom_dataset, batch_size=10, save=True,\n",
    "                      add_def=True, add_CoT=True, add_examples=hom_examples)\n",
    "call_llm_to_recognize(model=mistral, model_name=mistral_name, dataset=het_dataset, batch_size=10, save=True,\n",
    "                      add_def=True, add_CoT=True, add_examples=het_examples)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-06T13:48:20.055667Z",
     "end_time": "2024-02-06T16:16:18.854492Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### openchat"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# Connect to a server\n",
    "# Use local langChain with fastChat (Terminal code)\n",
    "# python3 -m fastchat.serve.controller\n",
    "# python3 -m fastchat.serve.model_worker --model-names \"gpt-3.5-turbo,text-davinci-003,text-embedding-ada-002\" --model-path openchat/openchat-3.5-0106\n",
    "# python3 -m fastchat.serve.openai_api_server --host localhost --port 8000\n",
    "\n",
    "# Connect openchat-3.5-0106\n",
    "os.environ['OPENAI_API_BASE'] = 'http://localhost:8000/v1'\n",
    "os.environ['OPENAI_API_KEY'] = 'EMPTY'\n",
    "\n",
    "openchat_name = 'openchat-3.5-0106'\n",
    "temperature = 0\n",
    "max_tokens = 200\n",
    "openchat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=temperature, max_tokens=max_tokens)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-07T11:03:56.514236Z",
     "end_time": "2024-02-07T11:03:56.745990Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Puns are words or phrases that have multiple meanings, often creating humor by exploiting the different meanings. Non-puns are words or phrases with only one clear meaning. Puns rely on wordplay, while non-puns do not. Puns can be clever and witty, while non-puns are straightforward and literal.\n"
     ]
    }
   ],
   "source": [
    "call_llm_to_give_pun_definition(model=openchat, model_name=openchat_name, save=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-06T18:04:21.964568Z",
     "end_time": "2024-02-06T18:04:23.431141Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### bare (0-shot)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1443/1443 [14:07<00:00,  1.70it/s]\n",
      "100%|██████████| 1146/1146 [11:15<00:00,  1.70it/s]\n"
     ]
    }
   ],
   "source": [
    "call_llm_to_recognize(model=openchat, model_name=openchat_name, dataset=hom_dataset, batch_size=10, save=True)\n",
    "call_llm_to_recognize(model=openchat, model_name=openchat_name, dataset=het_dataset, batch_size=10, save=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-06T18:07:55.991083Z",
     "end_time": "2024-02-06T18:33:18.926163Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### +def (0-shot)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1443/1443 [14:53<00:00,  1.62it/s]\n",
      "100%|██████████| 1146/1146 [11:49<00:00,  1.62it/s]\n"
     ]
    }
   ],
   "source": [
    "call_llm_to_recognize(model=openchat, model_name=openchat_name, dataset=hom_dataset, batch_size=10, save=True,\n",
    "                      add_def=True)\n",
    "call_llm_to_recognize(model=openchat, model_name=openchat_name, dataset=het_dataset, batch_size=10, save=True,\n",
    "                      add_def=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-06T18:33:18.926163Z",
     "end_time": "2024-02-06T19:00:01.164600Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### +def (6-shot)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1443/1443 [02:47<00:00,  8.62it/s]\n",
      "100%|██████████| 1146/1146 [13:06<00:00,  1.46it/s]\n"
     ]
    }
   ],
   "source": [
    "call_llm_to_recognize(model=openchat, model_name=openchat_name, dataset=hom_dataset, batch_size=10, save=True,\n",
    "                      add_def=True, add_examples=hom_examples)\n",
    "call_llm_to_recognize(model=openchat, model_name=openchat_name, dataset=het_dataset, batch_size=10, save=True,\n",
    "                      add_def=True, add_examples=het_examples)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-07T11:04:03.759971Z",
     "end_time": "2024-02-07T11:19:57.398846Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### +def&CoT (6-shot)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1443/1443 [1:14:52<00:00,  3.11s/it]\n",
      "100%|██████████| 1146/1146 [1:01:59<00:00,  3.25s/it]\n"
     ]
    }
   ],
   "source": [
    "call_llm_to_recognize(model=openchat, model_name=openchat_name, dataset=hom_dataset, batch_size=10, save=True,\n",
    "                      add_def=True, add_CoT=True, add_examples=hom_examples)\n",
    "call_llm_to_recognize(model=openchat, model_name=openchat_name, dataset=het_dataset, batch_size=10, save=True,\n",
    "                      add_def=True, add_CoT=True, add_examples=het_examples)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-07T13:38:11.791497Z",
     "end_time": "2024-02-07T15:55:04.053281Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
