{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Generation Task"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load Package"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI, \\\n",
    "    HarmBlockThreshold, HarmCategory\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from _api_key import get_openai_api_key, get_google_api_key, get_claude_api_key"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-27T15:16:20.462214Z",
     "end_time": "2024-03-27T15:16:20.562386Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Basic Function"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def load_json_file(file_path):\n",
    "    \"\"\"\n",
    "    Load json file\n",
    "    \"\"\"\n",
    "    with open(file_path,'r',encoding='utf-8') as f:\n",
    "        file = json.load(f)\n",
    "        f.close()\n",
    "    return file\n",
    "\n",
    "def save_json_file(file, file_path, sort_keys:bool=False):\n",
    "    \"\"\"\n",
    "    Save json file\n",
    "    \"\"\"\n",
    "    with open(file_path,'w',encoding='utf-8') as f:\n",
    "        json.dump(file, f, indent=4, ensure_ascii=False, sort_keys=sort_keys)\n",
    "        f.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-27T15:16:20.477229Z",
     "end_time": "2024-03-27T15:16:20.649565Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def splitDataset(file, puntype='hom'):\n",
    "    \"\"\"\n",
    "    Enter path or json file to separate the pun part from the non-pun part of the dataset\n",
    "    \"\"\"\n",
    "    if isinstance(file,str):\n",
    "        dataset = load_json_file(file)\n",
    "    else:\n",
    "        dataset = file\n",
    "    punDataset = dict()\n",
    "    nonpunDataset = dict()\n",
    "    for ID in dataset:\n",
    "        data = dataset[ID]\n",
    "        if puntype in ID:\n",
    "            if data.get('pun_word', False):\n",
    "                punDataset[ID] = data\n",
    "            else:\n",
    "                nonpunDataset[ID] = data\n",
    "    return punDataset, nonpunDataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-27T15:16:20.492871Z",
     "end_time": "2024-03-27T15:16:20.649565Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def add_human_pun(pun_dataset, save:bool=False, path_gen='pun_generation.json'):\n",
    "    \"\"\"\n",
    "    Add human-written puns\n",
    "    \"\"\"\n",
    "    path_gen = './results/' + path_gen\n",
    "    if os.path.exists(path_gen):\n",
    "        pun_generation = load_json_file(path_gen)\n",
    "    else:\n",
    "        pun_generation = dict()\n",
    "    for ID in pun_dataset:\n",
    "        data = pun_dataset[ID]\n",
    "        human_text = data['human_text']\n",
    "        if ID not in pun_generation:\n",
    "            pun_generation[ID] = {'human_text':human_text}\n",
    "        else:\n",
    "            pun_generation[ID].update({'human_text':human_text})\n",
    "    if save:\n",
    "        save_json_file(pun_generation, path_gen)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-27T15:16:20.508491Z",
     "end_time": "2024-03-27T15:16:20.649565Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Function of Generation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def call_llm_to_generate(model, dataset, model_name:str=None, method:int=0, examples:dict=None,\n",
    "                         save:bool=False, path_gen='pun_generation.json', batch_size:int=1):\n",
    "    \"\"\"\n",
    "    Try generating puns with llm  \\n\n",
    "    method=0: Generate non-pun sentence with only a single layer of meaning \\n\n",
    "    method=1: Given pun pair, generate pun directly \\n\n",
    "    method=2: Given specific contextual word set, generate pun directly  \\n\n",
    "    \"\"\"\n",
    "    def parse_output(ID, output:str):\n",
    "        # Parse output and get the result\n",
    "        try:\n",
    "            output = output[output.index('{'): output.index('}')+1]\n",
    "        except:\n",
    "            output = output\n",
    "        try:\n",
    "            output = eval(output)\n",
    "            sentence = output['Sentence']\n",
    "        except:\n",
    "            # print(ID, output)\n",
    "            try:\n",
    "                sentence = output.split('Sentence')[-1]\n",
    "            except:\n",
    "                sentence = 'No correctly parsed result.'\n",
    "        return sentence\n",
    "\n",
    "    def get_punchline_elements(data:dict):\n",
    "        # Get pun word/sense, alternative word/sense, etc. from data\n",
    "        pun_word = data['pun_word']\n",
    "        pun_sense = data['pun_sense']\n",
    "        alter_word = data['alter_word']\n",
    "        alter_sense = data['alter_sense']\n",
    "        context_words = data['human_keywords']\n",
    "        return pun_word, pun_sense, alter_word, alter_sense, context_words\n",
    "\n",
    "    def extend_dict(given_dict, keys):\n",
    "        # Create a null value based on the keys\n",
    "        temp = given_dict\n",
    "        for key in keys:\n",
    "            if key not in temp:\n",
    "                temp[key] = dict()\n",
    "            temp = temp[key]\n",
    "        return given_dict\n",
    "\n",
    "    assert method in [0, 1, 2]\n",
    "    path_gen = './results/' + path_gen\n",
    "    if os.path.exists(path_gen):\n",
    "        record_gen = load_json_file(path_gen)\n",
    "    else:\n",
    "        record_gen = dict()\n",
    "    # [A]. Construct the prompt\n",
    "    definition = \"\"\"<*Definition*>\\nPuns are a form of wordplay exploiting different meanings of a word or similar-sounding words, while non-puns are jokes or statements that don't rely on such linguistic ambiguities.\\n\\n\"\"\"\n",
    "    # Method=0: Generate non-pun that only expresses one sense\n",
    "    if method == 0:\n",
    "        instruction = \"\"\"<*Instruction*>\\nBelow is a keyword and one of its meanings. Please generate a non-pun sentence with the keyword that conveys the given meaning. You must output the current status in a parsable JSON format. An example output looks like:\\n{{\"Sentence\": \"XXX\"}}\"\"\"\n",
    "        examples_string = \"\"\"\"\"\"\n",
    "        testing = \"\"\"\\n\\n<*Your Response*>\\nKeyword: {punchline}\\nMeaning: {pun_word} <{pun_sense}>\\nOutput:\"\"\"\n",
    "    # Method=1: Generate pun directly\n",
    "    elif method == 1:\n",
    "        instruction = \"\"\"<*Instruction*>\\nBelow is a keyword and two of its meanings. Please generate a pun sentence with punchline on the keyword that conveys both given meanings simultaneously. Except for the keyword, the pun sentence must not utilize any words from either of the two meanings. Besides, once a keyword is used, it's strictly prohibited to use it again in the latter half of the sentence.\\nYou must output the current status in a parsable JSON format. An example output looks like:\\n{{\"Sentence\": \"XXX\"}}\"\"\"\n",
    "        if examples is not None:\n",
    "            examples_temp = []\n",
    "            for ID in examples:\n",
    "                example = examples[ID]\n",
    "                pun_word, pun_sense, alter_word, alter_sense, context_words = get_punchline_elements(data=example)\n",
    "                punchline, text = example['punchline'], example['human_text']\n",
    "                examples_temp.append(f\"Keyword: {punchline}\\n\"\n",
    "                                     f\"Meaning 1: {pun_word} <{pun_sense}>\\n\"\n",
    "                                     f\"Meaning 2: {alter_word} <{alter_sense}>\\n\"\n",
    "                                     f\"Output:\\n{{{{\\\"Sentence\\\": \\\"{text}\\\"}}}}\")\n",
    "            examples_string = '\\n\\n<*Examples*>\\n' + '\\n\\n'.join(examples_temp)\n",
    "        else:\n",
    "            examples_string = \"\"\"\"\"\"\n",
    "        testing = \"\"\"\\n\\n<*Your Response*>\\nKeyword: {punchline}\\nMeaning 1: {pun_word} <{pun_sense}>\\nMeaning 2: {alter_word} <{alter_sense}>\\nOutput:\"\"\"\n",
    "    # Method=2: Generate puns directly based on specific contextual words\n",
    "    else:\n",
    "        instruction = \"\"\"<*Instruction*>\\nBelow is a keyword, two of its meanings and a set of contextual words. Please generate a pun sentence with punchline on the keyword that conveys both given meanings simultaneously and using all the contextual words. Except for the keyword, the pun sentence must not utilize any words from either of the two meanings. Besides, once a keyword is used, it's strictly prohibited to use it again in the latter half of the sentence.\\nYou must output the current status in a parsable JSON format. An example output looks like:\\n{{\"Sentence\": \"XXX\"}}\"\"\"\n",
    "        if examples is not None:\n",
    "            examples_temp = []\n",
    "            for ID in examples:\n",
    "                example = examples[ID]\n",
    "                pun_word, pun_sense, alter_word, alter_sense, context_words = get_punchline_elements(data=example)\n",
    "                punchline, text = example['punchline'], example['human_text']\n",
    "                examples_temp.append(f\"Keyword: {punchline}\\n\"\n",
    "                                     f\"Meaning 1: {pun_word} <{pun_sense}>\\n\"\n",
    "                                     f\"Meaning 2: {alter_word} <{alter_sense}>\\n\"\n",
    "                                     f\"Contextual Words: {', '.join(context_words)}.\\n\"\n",
    "                                     f\"Output:\\n{{{{\\\"Sentence\\\": \\\"{text}\\\"}}}}\")\n",
    "            examples_string = '\\n\\n<*Examples*>\\n' + '\\n\\n'.join(examples_temp)\n",
    "        else:\n",
    "            examples_string = \"\"\"\"\"\"\n",
    "        testing = \"\"\"\\n\\n<*Your Response*>\\nKeyword: {punchline}\\nMeaning 1: {pun_word} <{pun_sense}>\\nMeaning 2: {alter_word} <{alter_sense}>\\nContextual Words: {context_words}.\\nOutput:\"\"\"\n",
    "    # Combine all parts together\n",
    "    prompt_string = definition + instruction + examples_string + testing\n",
    "    chat_prompt = ChatPromptTemplate.from_template(prompt_string)\n",
    "\n",
    "    # [B]. Call llm to generate\n",
    "    if model_name is None:\n",
    "        model_name = model.model_name if hasattr(model,'model_name') else model.model\n",
    "        model_name = model_name.split('/')[-1]\n",
    "    key_gen = f\"{model_name}_text\"\n",
    "    IDs = list(dataset.keys())\n",
    "    IDs_loaded = []\n",
    "    for ID in record_gen:\n",
    "        if record_gen[ID].get(key_gen, False) and \\\n",
    "           record_gen[ID][key_gen].get(f'method {method}', False):\n",
    "            IDs_loaded.append(ID)\n",
    "    all_ind = list(range(0,len(IDs)))\n",
    "    batch_ind = list(range(0,len(IDs),batch_size))\n",
    "    for ind in tqdm(all_ind):\n",
    "        if ind not in batch_ind:\n",
    "            continue\n",
    "        IDs_batch = IDs[ind: ind+batch_size]\n",
    "        # Remove the data that has already been run\n",
    "        IDs_batch = list(set(IDs_batch)-set(IDs_loaded))\n",
    "        if len(IDs_batch) == 0:\n",
    "            continue\n",
    "        _inputs = []\n",
    "        for ID in IDs_batch:\n",
    "            data = dataset[ID]\n",
    "            pun_ind = int(data['pun_word_ind'].split('_')[-1]) - 1\n",
    "            punchline = data['human_text'].split(' ')[pun_ind]\n",
    "            pun_word, pun_sense, alter_word, alter_sense, context_words = get_punchline_elements(data=data)\n",
    "            _inputs.append(chat_prompt.format_messages(pun_word=pun_word, pun_sense=pun_sense,\n",
    "                                                       alter_word=alter_word, alter_sense=alter_sense,\n",
    "                                                       punchline=punchline, context_words=', '.join(context_words)))\n",
    "        _outputs = [out.content for out in model.batch(_inputs)]\n",
    "        # print(_inputs[0][0].content)\n",
    "        # print(_outputs[0])\n",
    "        # break\n",
    "        for ID,out in zip(IDs_batch, _outputs):\n",
    "            sentence = parse_output(ID, out)\n",
    "            record_gen = extend_dict(record_gen, [ID, key_gen])\n",
    "            record_gen[ID][key_gen].update({f'method {method}': sentence})\n",
    "        if save:\n",
    "            save_json_file(record_gen, path_gen)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-27T15:16:20.524152Z",
     "end_time": "2024-03-27T15:16:20.649565Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset and Examples"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "hom_path = r'./dataset/hom_dataset.json'\n",
    "het_path = r'./dataset/het_dataset.json'\n",
    "hom_punDataset, hom_nonpunDataset = splitDataset(hom_path)\n",
    "het_punDataset, het_nonpunDataset = splitDataset(het_path, puntype='het')\n",
    "\n",
    "# add_human_pun(dict(**hom_punDataset, **het_punDataset), save=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-27T15:16:20.539744Z",
     "end_time": "2024-03-27T15:16:20.649565Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Choose data from examples manually\n",
    "hom_examples = {\n",
    "    \"hom_705\":{\n",
    "        \"pun_word\": \"toll\",\n",
    "        \"pun_sense\": \"a fee levied for the use of roads or bridges (used for maintenance)\",\n",
    "        \"alter_word\": \"toll\",\n",
    "        \"alter_sense\": \"value measured by what must be given or done or undergone to obtain something\",\n",
    "        \"punchline\": \"toll\",\n",
    "        \"human_text\":\"Driving on so many turnpikes was taking its toll.\",\n",
    "        \"human_keywords\": [\"Driving\", \"many\", \"turnpikes\", \"taking its toll\"],\n",
    "    },\n",
    "    \"hom_488\":{\n",
    "        \"pun_word\": \"bore\",\n",
    "        \"pun_sense\": \"make a hole, especially with a pointed power or hand tool\",\n",
    "        \"alter_word\": \"bore\",\n",
    "        \"alter_sense\": \"cause to be bored\",\n",
    "        \"punchline\": \"bored\",\n",
    "        \"human_text\":\"A carpenter sat on his drill and was bored to tears.\",\n",
    "        \"human_keywords\": [\"carpenter\", \"sat\", \"drill\", \"bored to tears\"],\n",
    "    },\n",
    "    \"hom_1556\":{\n",
    "        \"pun_word\": \"foil\",\n",
    "        \"pun_sense\": \"a piece of thin and flexible sheet metal\",\n",
    "        \"alter_word\": \"foil\",\n",
    "        \"alter_sense\": \"hinder or prevent (the efforts, plans, or desires) of\",\n",
    "        \"punchline\": \"foiled\",\n",
    "        \"human_text\":\"One leftover said to another 'foiled again.'\",\n",
    "        \"human_keywords\": [\"leftover\", \"foiled\", \"again\"],\n",
    "    }\n",
    "}\n",
    "\n",
    "het_examples = {\n",
    "    \"het_633\": {\n",
    "        \"pun_word\": \"sagely\",\n",
    "        \"pun_sense\": \"in a wise manner\",\n",
    "        \"alter_word\": \"sage\",\n",
    "        \"alter_sense\": \"aromatic fresh or dried grey-green leaves used widely as seasoning for meats and fowl and game etc\",\n",
    "        \"punchline\": \"sagely\",\n",
    "        \"human_text\": \"This fowl has been stuffed, said Tom sagely.\",\n",
    "        \"human_keywords\": [\"fowl\", \"stuffed\", \"sagely\"],\n",
    "    },\n",
    "    \"het_530\": {\n",
    "        \"pun_word\": \"toll\",\n",
    "        \"pun_sense\": \"ring slowly\",\n",
    "        \"alter_word\": \"tell off\",\n",
    "        \"alter_sense\": \"reprimand\",\n",
    "        \"punchline\": \"tolled\",\n",
    "        \"human_text\": \"A tangled bell ringer tolled himself off.\",\n",
    "        \"human_keywords\": [\"tangled\", \"bell ringer\", \"tolled himself off\"],\n",
    "    },\n",
    "    \"het_325\": {\n",
    "        \"pun_word\": \"c\",\n",
    "        \"pun_sense\": \"the 3rd letter of the Roman alphabet\",\n",
    "        \"alter_word\": \"sea\",\n",
    "        \"alter_sense\": \"a division of an ocean or a large body of salt water partially enclosed by land\",\n",
    "        \"punchline\": \"c\",\n",
    "        \"human_text\": \"An illiterate fisherman was lost at c.\",\n",
    "        \"human_keywords\": [\"illiterate\", \"fisherman\", \"c\"],\n",
    "    }\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-27T15:16:20.555373Z",
     "end_time": "2024-03-27T15:16:20.662582Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Generation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### gpt3.5"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# Connect gpt-3.5-turbo-1106\n",
    "gpt35_name = 'gpt-3.5-turbo-1106'\n",
    "temperature = 0.7\n",
    "openai_api_key = get_openai_api_key()  # use your api key\n",
    "gpt35 = ChatOpenAI(model_name=gpt35_name, temperature=temperature,\n",
    "                   openai_api_key=openai_api_key, request_timeout=120)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-21T16:31:07.616276Z",
     "end_time": "2024-03-21T16:31:07.679218Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 810/810 [02:55<00:00,  4.62it/s]\n",
      "100%|██████████| 647/647 [03:12<00:00,  3.36it/s]\n"
     ]
    }
   ],
   "source": [
    "# Generation method 0 (non-pun)\n",
    "call_llm_to_generate(model=gpt35, dataset=hom_punDataset, method=0, examples=hom_examples, batch_size=10, save=True)\n",
    "call_llm_to_generate(model=gpt35, dataset=het_punDataset, method=0, examples=het_examples, batch_size=10, save=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-18T00:11:55.564145Z",
     "end_time": "2024-03-18T00:18:03.236047Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 810/810 [03:36<00:00,  3.74it/s]\n",
      "  8%|▊         | 51/647 [00:15<02:45,  3.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "het_848 {\"Sentence\": \"I can't put this book down, it's so engaging,\" said the musician, reedily.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 101/647 [00:25<01:49,  5.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "het_708 {\"Sentence\": \"I asked the computer for a snack, and it said, 'I can't eat, but I can byte!'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▊        | 121/647 [00:29<01:45,  4.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "het_665 {\"Sentence\": \"I can't watch, it's too loud!\" the moviegoer screamed at the screen.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▎ | 541/647 [01:50<00:18,  5.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "het_1221 {\"Sentence\": \"I can only lift on the weekends,\" said the weightlifter weakly.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 647/647 [02:09<00:00,  4.98it/s]\n"
     ]
    }
   ],
   "source": [
    "# Generation method 1\n",
    "call_llm_to_generate(model=gpt35, dataset=hom_punDataset, method=1, examples=hom_examples, batch_size=10, save=True)\n",
    "call_llm_to_generate(model=gpt35, dataset=het_punDataset, method=1, examples=het_examples, batch_size=10, save=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-18T00:22:10.274644Z",
     "end_time": "2024-03-18T00:27:56.656640Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 810/810 [02:38<00:00,  5.11it/s]\n",
      "100%|██████████| 647/647 [02:06<00:00,  5.11it/s]\n"
     ]
    }
   ],
   "source": [
    "# Generation method 2\n",
    "call_llm_to_generate(model=gpt35, dataset=hom_punDataset, method=2, examples=hom_examples, batch_size=10, save=True)\n",
    "call_llm_to_generate(model=gpt35, dataset=het_punDataset, method=2, examples=het_examples, batch_size=10, save=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-18T00:43:49.404605Z",
     "end_time": "2024-03-18T00:48:34.520102Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### gpt4"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# Connect gpt-4-1106-preview\n",
    "gpt4_name = 'gpt-4-1106-preview'\n",
    "temperature = 0.7\n",
    "openai_api_key = get_openai_api_key()  # use your api key\n",
    "gpt4 = ChatOpenAI(model_name=gpt4_name, temperature=temperature,\n",
    "                    openai_api_key=openai_api_key, request_timeout=120)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-18T01:00:10.428294Z",
     "end_time": "2024-03-18T01:00:10.475141Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 810/810 [04:17<00:00,  3.14it/s]\n",
      " 30%|██▉       | 191/647 [00:47<01:44,  4.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "het_473 {\"Sentence\": \"I'll accept your gift of flowers willingly,\" she said gladly.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 647/647 [02:48<00:00,  3.83it/s]\n"
     ]
    }
   ],
   "source": [
    "# Generation method 1\n",
    "call_llm_to_generate(model=gpt4, dataset=hom_punDataset, method=1, examples=hom_examples, batch_size=10, save=True)\n",
    "call_llm_to_generate(model=gpt4, dataset=het_punDataset, method=1, examples=het_examples, batch_size=10, save=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-18T01:00:10.475141Z",
     "end_time": "2024-03-18T01:07:17.012359Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 810/810 [03:27<00:00,  3.90it/s]\n",
      "100%|██████████| 647/647 [04:26<00:00,  2.42it/s]\n"
     ]
    }
   ],
   "source": [
    "# Generation method 2\n",
    "call_llm_to_generate(model=gpt4, dataset=hom_punDataset, method=2, examples=hom_examples, batch_size=10, save=True)\n",
    "call_llm_to_generate(model=gpt4, dataset=het_punDataset, method=2, examples=het_examples, batch_size=10, save=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-18T01:24:20.403643Z",
     "end_time": "2024-03-18T01:32:15.430499Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### gemini-pro"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# Connect gemini-pro\n",
    "gemini_name = 'gemini-pro'\n",
    "temperature = 0.7\n",
    "google_api_key = get_google_api_key()  # use your api key\n",
    "gemini = ChatGoogleGenerativeAI(\n",
    "        model=gemini_name,\n",
    "        google_api_key=google_api_key,\n",
    "        temperature=temperature,\n",
    "        transport='rest',\n",
    "        safety_settings={\n",
    "            HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "            HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "            HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
    "            HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE\n",
    "            },\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-18T12:54:40.519785Z",
     "end_time": "2024-03-18T12:54:40.542515Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 649/810 [22:01<05:04,  1.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hom_1412 {\"Sentence\": \"The falling apple gravitated toward the earth and Newton.}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 810/810 [27:16<00:00,  2.02s/it]\n",
      "100%|██████████| 647/647 [21:18<00:00,  1.98s/it]\n"
     ]
    }
   ],
   "source": [
    "# Generation method 1\n",
    "call_llm_to_generate(model=gemini, dataset=hom_punDataset, method=1, examples=hom_examples, batch_size=1, save=True)\n",
    "call_llm_to_generate(model=gemini, dataset=het_punDataset, method=1, examples=het_examples, batch_size=1, save=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-18T08:20:23.758749Z",
     "end_time": "2024-03-18T09:08:58.567084Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 810/810 [30:08<00:00,  2.23s/it]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 647/647 [23:42<00:00,  2.20s/it]\n"
     ]
    }
   ],
   "source": [
    "# Generation method 2\n",
    "call_llm_to_generate(model=gemini, dataset=hom_punDataset, method=2, examples=hom_examples, batch_size=1, save=True)\n",
    "call_llm_to_generate(model=gemini, dataset=het_punDataset, method=2, examples=het_examples, batch_size=1, save=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-18T09:54:19.143295Z",
     "end_time": "2024-03-18T10:48:10.353086Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### claude3"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# Connect claude-3-opus-20240229\n",
    "claude3_name = 'claude-3-opus-20240229'\n",
    "temperature = 0.7\n",
    "claude_api_key = get_claude_api_key()  # use your api key\n",
    "claude3 = ChatAnthropic(model_name=claude3_name, temperature=temperature,\n",
    "                        anthropic_api_key=claude_api_key)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-27T15:16:31.928068Z",
     "end_time": "2024-03-27T15:16:31.991419Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 810/810 [17:20<00:00,  1.28s/it]\n",
      "100%|██████████| 647/647 [1:19:20<00:00,  7.36s/it]  \n"
     ]
    }
   ],
   "source": [
    "# Generation method 1\n",
    "call_llm_to_generate(model=claude3, dataset=hom_punDataset, method=1, examples=hom_examples, save=True)\n",
    "call_llm_to_generate(model=claude3, dataset=het_punDataset, method=1, examples=het_examples, save=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-25T22:52:45.945323Z",
     "end_time": "2024-03-26T00:29:27.210427Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 810/810 [00:00<00:00, 30117.07it/s]\n",
      "100%|██████████| 647/647 [00:00<00:00, 41475.72it/s]\n"
     ]
    }
   ],
   "source": [
    "# Generation method 2\n",
    "call_llm_to_generate(model=claude3, dataset=hom_punDataset, method=2, examples=hom_examples, save=True)\n",
    "call_llm_to_generate(model=claude3, dataset=het_punDataset, method=2, examples=het_examples, save=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-27T15:16:41.400898Z",
     "end_time": "2024-03-27T15:16:41.690372Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### vicuna"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# Connect to a server\n",
    "# Use local langChain with fastChat (Terminal code)\n",
    "# python3 -m fastchat.serve.controller\n",
    "# python3 -m fastchat.serve.model_worker --model-names \"gpt-3.5-turbo,text-davinci-003,text-embedding-ada-002\" --model-path lmsys/vicuna-7b-v1.5\n",
    "# python3 -m fastchat.serve.openai_api_server --host localhost --port 8000\n",
    "\n",
    "# Connect vicuna-7b-v1.5\n",
    "os.environ['OPENAI_API_BASE'] = 'http://localhost:8000/v1'\n",
    "os.environ['OPENAI_API_KEY'] = 'EMPTY'\n",
    "\n",
    "vicuna_name = 'vicuna-7b-v1.5'\n",
    "temperature = 0.7\n",
    "max_tokens = 300\n",
    "vicuna = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=temperature, max_tokens=max_tokens)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-19T22:13:49.324108Z",
     "end_time": "2024-03-19T22:13:49.418658Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 810/810 [00:00<00:00, 187018.95it/s]\n",
      "100%|██████████| 647/647 [00:00<00:00, 195161.07it/s]\n"
     ]
    }
   ],
   "source": [
    "# Generation method 1\n",
    "call_llm_to_generate(model=vicuna, model_name=vicuna_name, dataset=hom_punDataset, method=1, examples=hom_examples, batch_size=10, save=True)\n",
    "call_llm_to_generate(model=vicuna, model_name=vicuna_name, dataset=het_punDataset, method=1, examples=het_examples, batch_size=10, save=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-18T20:53:32.959651Z",
     "end_time": "2024-03-18T20:53:33.090698Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 810/810 [03:48<00:00,  3.55it/s]\n",
      "100%|██████████| 647/647 [00:00<00:00, 194768.87it/s]\n"
     ]
    }
   ],
   "source": [
    "# Generation method 2\n",
    "call_llm_to_generate(model=vicuna, model_name=vicuna_name, dataset=hom_punDataset, method=2, examples=hom_examples, batch_size=10, save=True)\n",
    "call_llm_to_generate(model=vicuna, model_name=vicuna_name, dataset=het_punDataset, method=2, examples=het_examples, batch_size=10, save=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-18T20:55:22.158456Z",
     "end_time": "2024-03-18T20:59:10.685726Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### llama2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# Connect to a server\n",
    "# Use local langChain with fastChat (Terminal code)\n",
    "# python3 -m fastchat.serve.controller\n",
    "# python3 -m fastchat.serve.model_worker --model-names \"gpt-3.5-turbo,text-davinci-003,text-embedding-ada-002\" --model-path meta-llama/Llama-2-7b-chat-hf\n",
    "# python3 -m fastchat.serve.openai_api_server --host localhost --port 8000\n",
    "\n",
    "# Connect llama-2-7b-chat\n",
    "os.environ['OPENAI_API_BASE'] = 'http://localhost:8000/v1'\n",
    "os.environ['OPENAI_API_KEY'] = 'EMPTY'\n",
    "\n",
    "llama2_name = 'llama-2-7b-chat'\n",
    "temperature = 0.7\n",
    "max_tokens = 300\n",
    "llama2 = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=temperature, max_tokens=max_tokens)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-19T16:56:05.565129Z",
     "end_time": "2024-03-19T16:56:05.660468Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 810/810 [31:05<00:00,  2.30s/it]\n",
      "100%|██████████| 647/647 [24:04<00:00,  2.23s/it]\n"
     ]
    }
   ],
   "source": [
    "# Generation method 1\n",
    "call_llm_to_generate(model=llama2, model_name=llama2_name, dataset=hom_punDataset, method=1, examples=hom_examples, batch_size=10, save=True)\n",
    "call_llm_to_generate(model=llama2, model_name=llama2_name, dataset=het_punDataset, method=1, examples=het_examples, batch_size=10, save=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-19T01:48:54.705319Z",
     "end_time": "2024-03-19T02:44:04.575422Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 810/810 [00:00<00:00, 202720.11it/s]\n",
      "100%|██████████| 647/647 [00:00<00:00, 202621.87it/s]\n"
     ]
    }
   ],
   "source": [
    "# Generation method 2\n",
    "call_llm_to_generate(model=llama2, model_name=llama2_name, dataset=hom_punDataset, method=2, examples=hom_examples, batch_size=10, save=True)\n",
    "call_llm_to_generate(model=llama2, model_name=llama2_name, dataset=het_punDataset, method=2, examples=het_examples, batch_size=10, save=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-19T15:52:15.022433Z",
     "end_time": "2024-03-19T15:52:15.053201Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### mistral"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# Connect to a server\n",
    "# Use local langChain with fastChat (Terminal code)\n",
    "# python3 -m fastchat.serve.controller\n",
    "# python3 -m fastchat.serve.model_worker --model-names \"gpt-3.5-turbo,text-davinci-003,text-embedding-ada-002\" --model-path mistralai/Mistral-7B-Instruct-v0.2\n",
    "# python3 -m fastchat.serve.openai_api_server --host localhost --port 8000\n",
    "\n",
    "# Connect mistral-7b-instruct-v0.2\n",
    "os.environ['OPENAI_API_BASE'] = 'http://localhost:8000/v1'\n",
    "os.environ['OPENAI_API_KEY'] = 'EMPTY'\n",
    "\n",
    "mistral_name = 'mistral-7b-instruct-v0.2'\n",
    "temperature = 0.7\n",
    "max_tokens = 300\n",
    "mistral = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=temperature, max_tokens=max_tokens)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-19T19:34:31.708363Z",
     "end_time": "2024-03-19T19:34:32.344222Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 810/810 [12:00<00:00,  1.12it/s]\n",
      "100%|██████████| 647/647 [11:15<00:00,  1.04s/it]\n"
     ]
    }
   ],
   "source": [
    "# Generation method 1\n",
    "call_llm_to_generate(model=mistral, model_name=mistral_name, dataset=hom_punDataset, method=1, examples=hom_examples, batch_size=10, save=True)\n",
    "call_llm_to_generate(model=mistral, model_name=mistral_name, dataset=het_punDataset, method=1, examples=het_examples, batch_size=10, save=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-19T08:58:40.087260Z",
     "end_time": "2024-03-19T09:21:56.305984Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 810/810 [22:17<00:00,  1.65s/it]\n",
      "100%|██████████| 647/647 [19:21<00:00,  1.80s/it]\n"
     ]
    }
   ],
   "source": [
    "# Generation method 2\n",
    "call_llm_to_generate(model=mistral, model_name=mistral_name, dataset=hom_punDataset, method=2, examples=hom_examples, batch_size=10, save=True)\n",
    "call_llm_to_generate(model=mistral, model_name=mistral_name, dataset=het_punDataset, method=2, examples=het_examples, batch_size=10, save=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-19T12:31:43.292695Z",
     "end_time": "2024-03-19T13:13:22.890630Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### openchat"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# Connect to a server\n",
    "# Use local langChain with fastChat (Terminal code)\n",
    "# python3 -m fastchat.serve.controller\n",
    "# python3 -m fastchat.serve.model_worker --model-names \"gpt-3.5-turbo,text-davinci-003,text-embedding-ada-002\" --model-path openchat/openchat-3.5-0106\n",
    "# python3 -m fastchat.serve.openai_api_server --host localhost --port 8000\n",
    "\n",
    "# Connect openchat-3.5-0106\n",
    "os.environ['OPENAI_API_BASE'] = 'http://localhost:8000/v1'\n",
    "os.environ['OPENAI_API_KEY'] = 'EMPTY'\n",
    "\n",
    "openchat_name = 'openchat-3.5-0106'\n",
    "temperature = 0.7\n",
    "max_tokens = 300\n",
    "openchat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=temperature, max_tokens=max_tokens)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-20T01:06:41.943768Z",
     "end_time": "2024-03-20T01:06:42.053363Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 810/810 [08:28<00:00,  1.59it/s]\n",
      "100%|██████████| 647/647 [06:16<00:00,  1.72it/s]\n"
     ]
    }
   ],
   "source": [
    "# Generation method 1\n",
    "call_llm_to_generate(model=openchat, model_name=openchat_name, dataset=hom_punDataset, method=1, examples=hom_examples, batch_size=10, save=True)\n",
    "call_llm_to_generate(model=openchat, model_name=openchat_name, dataset=het_punDataset, method=1, examples=het_examples, batch_size=10, save=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-20T01:06:45.033341Z",
     "end_time": "2024-03-20T01:21:29.334409Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 810/810 [08:45<00:00,  1.54it/s]\n",
      "100%|██████████| 647/647 [06:48<00:00,  1.58it/s]\n"
     ]
    }
   ],
   "source": [
    "# Generation method 2\n",
    "call_llm_to_generate(model=openchat, model_name=openchat_name, dataset=hom_punDataset, method=2, examples=hom_examples, batch_size=10, save=True)\n",
    "call_llm_to_generate(model=openchat, model_name=openchat_name, dataset=het_punDataset, method=2, examples=het_examples, batch_size=10, save=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-20T02:24:33.111527Z",
     "end_time": "2024-03-20T02:40:06.920698Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
