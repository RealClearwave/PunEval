{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load Package"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from torch import tensor,matmul\n",
    "import xml.etree.ElementTree as ET\n",
    "from nltk.corpus import wordnet31 as wn\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from _api_key import get_openai_api_key"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-30T12:25:54.810730Z",
     "end_time": "2024-01-30T12:25:56.826294Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Basic Function"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def load_json_file(file_path):\n",
    "    \"\"\"\n",
    "    Load json file\n",
    "    \"\"\"\n",
    "    with open(file_path,'r',encoding='utf-8') as f:\n",
    "        file = json.load(f)\n",
    "        f.close()\n",
    "    return file\n",
    "\n",
    "def save_json_file(file, file_path):\n",
    "    \"\"\"\n",
    "    Save json file\n",
    "    \"\"\"\n",
    "    with open(file_path,'w',encoding='utf-8') as f:\n",
    "        json.dump(file, f, indent=4, ensure_ascii=False)\n",
    "        f.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def getCorpus(filepath):\n",
    "    \"\"\"\n",
    "    Enter the path of the .xml data file, Return dataset(list)\n",
    "    \"\"\"\n",
    "    # Open the XML format file\n",
    "    assert '.xml' in filepath\n",
    "    tree = ET.parse(filepath)\n",
    "    # Get the root element\n",
    "    root = tree.getroot()\n",
    "    # Iterate through the sub-elements to get dataset of all sentences\n",
    "    # corpus[ID] = (word_id_list, split_sentence, pun_id)\n",
    "    corpus = dict()\n",
    "    for item in root:\n",
    "        ID = item.attrib['id']\n",
    "        data = []\n",
    "        word_id_list, sentence, sense_list= [], [], []\n",
    "        # Splice sentence\n",
    "        for word in item:\n",
    "            word_id_list.append(word.attrib['id'])\n",
    "            sentence.append(word.text)\n",
    "            # Check the location of the pun word\n",
    "            if 'senses' in word.attrib:\n",
    "                sense_list.append(list(word.attrib.values()))\n",
    "        data.append(word_id_list)\n",
    "        data.append(sentence)\n",
    "        if len(sense_list) != 0:\n",
    "            ind = [sense for __, sense in sense_list].index('2')\n",
    "            pun_id = sense_list[ind][0]\n",
    "            data.append(pun_id)\n",
    "        corpus[ID] = tuple(data)\n",
    "    return corpus\n",
    "\n",
    "\n",
    "def getGold(filepath):\n",
    "    \"\"\"\n",
    "    Get the gold standard for dataset (whether is a pun or not/double sense of the pun)\n",
    "    \"\"\"\n",
    "    assert '.gold' in filepath\n",
    "    with open(filepath, 'r') as f:\n",
    "        temp = f.readlines()\n",
    "        f.close()\n",
    "    # golds[ID]= ((gold)label, )\n",
    "    golds = dict()\n",
    "    for item in temp:\n",
    "        sp = '\\t' if '\\t' in item else ' '\n",
    "        item = item.split('\\n')[0].split(sp)\n",
    "        ID = item[0]\n",
    "        label = item[1:]\n",
    "        if len(ID.split('_')) > 2:\n",
    "            ID = '_'.join(ID.split('_')[0:2])\n",
    "        golds[ID] = tuple(label)\n",
    "    return golds\n",
    "\n",
    "\n",
    "def getExPun(filepath):\n",
    "    \"\"\"\n",
    "    Get the ID, the longest explanation, the best keyword set and the average funniness rating from the ExPun dataset\n",
    "    \"\"\"\n",
    "    dataset = load_json_file(filepath)\n",
    "    # ExPun[ID] = (explanation, keywords, rating)\n",
    "    ExPun = dict()\n",
    "    for data in dataset:\n",
    "        ID = data['ID']\n",
    "        try:\n",
    "            is_a_joke = data['Is a Joke?']\n",
    "        except:\n",
    "            is_a_joke = data['Understand the Joke?']\n",
    "        explanations = data['Natural language explanation']\n",
    "        joke_keywords = data['Joke keywords']\n",
    "        funniness = data['Funniness (1-5)']\n",
    "        # Take out the joke explanation, keywords, and funniness rating\n",
    "        explanations = [expl for i,expl in zip(is_a_joke,explanations) if i == 1 and len(expl) > 0]\n",
    "        joke_keywords = [kw for i,kw in zip(is_a_joke,joke_keywords) if i == 1 and type(kw) == list]\n",
    "        funniness = [f for i,f in zip(is_a_joke,funniness) if i == 1]\n",
    "        if len(explanations) * len(joke_keywords) != 0:\n",
    "            # Calculate the length of explanation and keywords\n",
    "            expl_len = [len(expl) for expl in explanations]\n",
    "            keywords_len = [[len(w.split(' ')) for w in kw] for kw in joke_keywords]\n",
    "            keywords_len = [len(kw)-sum(kwl)/len(kwl) for kw, kwl in\n",
    "                            zip(joke_keywords, keywords_len)]\n",
    "            # Choose the longest explanation\n",
    "            chosen_expl = explanations[expl_len.index(max(expl_len))]\n",
    "            # Pick the keyword set with the largest number of keywords but a relatively small number of words per keyword\n",
    "            chosen_keywords = joke_keywords[keywords_len.index(max(keywords_len))]\n",
    "            rating = round(sum(funniness)/len(funniness), 2)\n",
    "            ExPun[ID] = (chosen_expl, chosen_keywords, rating)\n",
    "    return ExPun\n",
    "\n",
    "\n",
    "def getPunDataset(corpus, golds, expun, puntype='hom'):\n",
    "    \"\"\"\n",
    "    Obtain pun dataset[ID] = (pun word, sense1, alternative word, sense2, human (pun) text,\n",
    "    human explanation, human keywords, human rating) \\n\n",
    "    Alternative word of homographic pun is just pun word\n",
    "    \"\"\"\n",
    "    # corpus[ID] = (word_id_list, split_sentence, pun_id)\n",
    "    # golds[ID]= ((gold)label, )\n",
    "    # expun[ID] = (explanation, keywords, rating)\n",
    "    IDs = list(set(corpus.keys()) & set(golds.keys()) & set(expun.keys()))\n",
    "    IDs = sorted([ID for ID in IDs if puntype in ID], reverse=True)\n",
    "    punDataset = dict()\n",
    "    for ID in IDs:\n",
    "        data = dict()\n",
    "        # Pun word and its sense key\n",
    "        pun_sense_key = golds[ID][0].split(';')\n",
    "        pun_word = ' '.join(pun_sense_key[0].split('%')[0].split('_'))\n",
    "        # Alternative word and its sense key\n",
    "        alter_sense_key = golds[ID][1].split(';')\n",
    "        alter_word = ' '.join(alter_sense_key[0].split('%')[0].split('_'))\n",
    "        # Make sure the word sense can be retrieved by sense key\n",
    "        try:\n",
    "            pun_sense = [wn.synset_from_sense_key(sk).definition() for sk in pun_sense_key]\n",
    "            alter_sense = [wn.synset_from_sense_key(sk).definition() for sk in alter_sense_key]\n",
    "        except:\n",
    "            continue\n",
    "        # Human pun text, explanation, keywords\n",
    "        pun_word_ind = corpus[ID][2]\n",
    "        human_text = ' '.join(corpus[ID][1])\n",
    "        human_explanation = expun[ID][0]\n",
    "        human_keywords = expun[ID][1]\n",
    "        human_rating = expun[ID][2]\n",
    "        # Construct dataset\n",
    "        data['pun_word'] = pun_word\n",
    "        data['pun_sense_key'] = ';'.join(pun_sense_key)\n",
    "        data['pun_sense'] = '; '.join(pun_sense)\n",
    "        data['alter_word'] = alter_word\n",
    "        data['alter_sense_key'] = ';'.join(alter_sense_key)\n",
    "        data['alter_sense'] = '; '.join(alter_sense)\n",
    "        data['pun_word_ind'] = pun_word_ind\n",
    "        data['human_text'] = human_text\n",
    "        data['human_explanation'] = human_explanation\n",
    "        data['human_keywords'] = human_keywords\n",
    "        data['human_rating'] = human_rating\n",
    "        punDataset[ID] = data\n",
    "    return punDataset\n",
    "\n",
    "\n",
    "def getNonpunDataset(detectionSet, detectionGold, puntype='hom'):\n",
    "    \"\"\"\n",
    "    Obtain non-pun dataset (with only text)\n",
    "    \"\"\"\n",
    "    # Get all non-pun\n",
    "    nonpunIDs = []\n",
    "    for ID in detectionGold:\n",
    "        gold = int(detectionGold[ID][0])\n",
    "        if gold == 0 and puntype in ID:\n",
    "            nonpunIDs.append(ID)\n",
    "    nonpunDataset = dict()\n",
    "    for ID in nonpunIDs:\n",
    "        human_text = ' '.join(detectionSet[ID][1])\n",
    "        nonpunDataset[ID] = {'human_text': human_text}\n",
    "    return nonpunDataset\n",
    "\n",
    "\n",
    "def splitDataset(file, puntype='hom'):\n",
    "    \"\"\"\n",
    "    Enter path or json file to separate the pun part from the non-pun part of the dataset\n",
    "    \"\"\"\n",
    "    if isinstance(file,str):\n",
    "        dataset = load_json_file(file)\n",
    "    else:\n",
    "        dataset = file\n",
    "    punDataset = dict()\n",
    "    nonpunDataset = dict()\n",
    "    for ID in dataset:\n",
    "        data = dataset[ID]\n",
    "        if puntype in ID:\n",
    "            if data.get('pun_word', False):\n",
    "                punDataset[ID] = data\n",
    "            else:\n",
    "                nonpunDataset[ID] = data\n",
    "    return punDataset, nonpunDataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-22T21:11:35.307174Z",
     "end_time": "2024-01-22T21:11:35.322219Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def select_examples(embeddings_model, dataset, top_k:int=10):\n",
    "    \"\"\"\n",
    "    Extract several typical data from the dataset (with the highest average similarity to other data) as examples of prompt words\n",
    "    \"\"\"\n",
    "    pun_embeddings = []\n",
    "    nonpun_embeddings = []\n",
    "    # Embeddings of dataset\n",
    "    IDs = list(dataset.keys())\n",
    "    for ID in tqdm(IDs):\n",
    "        data = dataset[ID]\n",
    "        human_text = data['human_text']\n",
    "        if data.get('pun_word', False):\n",
    "            pun_embeddings.append((ID,embeddings_model.embed_query(human_text)))\n",
    "        else:\n",
    "            nonpun_embeddings.append((ID,embeddings_model.embed_query(human_text)))\n",
    "    pun_embeddings_tensor = tensor([embed for ID,embed in pun_embeddings])\n",
    "    nonpun_embeddings_tensor = tensor([embed for ID,embed in nonpun_embeddings])\n",
    "    # Calculate average cos_sim as score\n",
    "    pun_scores = []\n",
    "    nonpun_scores = []\n",
    "    for ID,embed in pun_embeddings:\n",
    "        cos_sim = matmul(pun_embeddings_tensor, tensor(embed))\n",
    "        score = round(float(cos_sim.mean()),4)\n",
    "        pun_scores.append([ID, score])\n",
    "    for ID,embed in nonpun_embeddings:\n",
    "        cos_sim = matmul(nonpun_embeddings_tensor, tensor(embed))\n",
    "        score = round(float(cos_sim.mean()),4)\n",
    "        nonpun_scores.append([ID, score])\n",
    "    # Sort scores to get the highest ones\n",
    "    pun_scores.sort(key=lambda x:x[1], reverse=True)\n",
    "    nonpun_scores.sort(key=lambda x:x[1], reverse=True)\n",
    "    # Get IDs of examples\n",
    "    pun_examples = [ID for ID,score in pun_scores[0:top_k]]\n",
    "    nonpun_examples = [ID for ID,score in nonpun_scores[0:top_k]]\n",
    "    print(f\"# Examples of pun: {pun_examples}\")\n",
    "    print(f\"# Examples of non-pun: {nonpun_examples}\")\n",
    "    IDs_examples = pun_examples + nonpun_examples\n",
    "    examples = dict()\n",
    "    remainings = dict()\n",
    "    for ID in IDs:\n",
    "        if ID in IDs_examples:\n",
    "            examples[ID] = dataset[ID]\n",
    "        else:\n",
    "            remainings[ID] = dataset[ID]\n",
    "    return remainings, examples"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-22T21:11:35.322219Z",
     "end_time": "2024-01-22T21:11:35.337900Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Build Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "rootpath = os.path.dirname(os.path.dirname(os.getcwd()))\n",
    "hom_interpretation_corpuspath = os.path.join(rootpath, r'semeval2017_task7/test/subtask3-homographic-test.xml')\n",
    "hom_interpretation_goldpath = os.path.join(rootpath, r'semeval2017_task7/test/subtask3-homographic-test.gold')\n",
    "het_interpretation_corpuspath = os.path.join(rootpath, r'semeval2017_task7/test/subtask3-heterographic-test.xml')\n",
    "het_interpretation_goldpath = os.path.join(rootpath, r'semeval2017_task7/test/subtask3-heterographic-test.gold')\n",
    "\n",
    "expun_path1 = os.path.join(rootpath, r'expun/data/expunations_annotated_full.json')\n",
    "expun_path2 = os.path.join(rootpath, r'expun/data/expunations_annotated_pilot_100.json')\n",
    "\n",
    "hom_interpretation_corpus = getCorpus(hom_interpretation_corpuspath)\n",
    "hom_interpretation_golds = getGold(hom_interpretation_goldpath)\n",
    "het_interpretation_corpus = getCorpus(het_interpretation_corpuspath)\n",
    "het_interpretation_golds = getGold(het_interpretation_goldpath)\n",
    "\n",
    "expun = getExPun(expun_path1)\n",
    "expun.update(getExPun(expun_path2))\n",
    "\n",
    "hom_punDataset = getPunDataset(corpus=hom_interpretation_corpus, golds=hom_interpretation_golds,\n",
    "                               expun=expun, puntype='hom')\n",
    "het_punDataset = getPunDataset(corpus=het_interpretation_corpus, golds=het_interpretation_golds,\n",
    "                               expun=expun, puntype='het')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-22T21:11:35.337900Z",
     "end_time": "2024-01-22T21:11:37.878413Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "rootpath = os.path.dirname(os.path.dirname(os.getcwd()))\n",
    "hom_detection_corpuspath = os.path.join(rootpath, r'semeval2017_task7/test/subtask1-homographic-test.xml')\n",
    "hom_detection_goldpath = os.path.join(rootpath, r'semeval2017_task7/test/subtask1-homographic-test.gold')\n",
    "het_detection_corpuspath = os.path.join(rootpath, r'semeval2017_task7/test/subtask1-heterographic-test.xml')\n",
    "het_detection_goldpath = os.path.join(rootpath, r'semeval2017_task7/test/subtask1-heterographic-test.gold')\n",
    "\n",
    "hom_detection_corpus = getCorpus(hom_detection_corpuspath)\n",
    "hom_detection_golds = getGold(hom_detection_goldpath)\n",
    "het_detection_corpus = getCorpus(het_detection_corpuspath)\n",
    "het_detection_golds = getGold(het_detection_goldpath)\n",
    "\n",
    "hom_nonpunDataset = getNonpunDataset(detectionSet=hom_detection_corpus, detectionGold=hom_detection_golds,\n",
    "                                     puntype='hom')\n",
    "het_nonpunDataset = getNonpunDataset(detectionSet=het_detection_corpus, detectionGold=het_detection_golds,\n",
    "                                     puntype='het')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-22T21:11:37.868849Z",
     "end_time": "2024-01-22T21:11:37.947607Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▎ | 1223/1463 [10:06<01:54,  2.09it/s]Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised ServiceUnavailableError: The server is overloaded or not ready yet..\n",
      "100%|██████████| 1463/1463 [11:59<00:00,  2.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Examples of pun: ['hom_1404', 'hom_1356', 'hom_4', 'hom_1556', 'hom_1477', 'hom_1182', 'hom_488', 'hom_1283', 'hom_705', 'hom_1162']\n",
      "# Examples of non-pun: ['hom_158', 'hom_533', 'hom_1167', 'hom_111', 'hom_639', 'hom_436', 'hom_750', 'hom_846', 'hom_1464', 'hom_1623']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1166/1166 [09:35<00:00,  2.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Examples of pun: ['het_1751', 'het_530', 'het_633', 'het_325', 'het_496', 'het_519', 'het_345', 'het_621', 'het_1453', 'het_1774']\n",
      "# Examples of non-pun: ['het_151', 'het_957', 'het_115', 'het_225', 'het_999', 'het_563', 'het_917', 'het_41', 'het_533', 'het_1041']\n"
     ]
    }
   ],
   "source": [
    "# Connect text-embedding-ada-002\n",
    "model_name = 'text-embedding-ada-002'\n",
    "openai_api_key = get_openai_api_key()  # use your api key\n",
    "embeddings_model = OpenAIEmbeddings(model=model_name,openai_api_key=openai_api_key, request_timeout=120)\n",
    "\n",
    "# Select examples\n",
    "hom_dataset = dict(**hom_punDataset, **hom_nonpunDataset)\n",
    "het_dataset = dict(**het_punDataset, **het_nonpunDataset)\n",
    "hom_dataset, hom_examples = select_examples(embeddings_model=embeddings_model, dataset=hom_dataset)\n",
    "# Examples of pun: ['hom_1404', 'hom_1356', 'hom_4', 'hom_1556', 'hom_1477', 'hom_1182', 'hom_488', 'hom_1283', 'hom_705', 'hom_1162']\n",
    "# Examples of non-pun: ['hom_158', 'hom_533', 'hom_1167', 'hom_111', 'hom_639', 'hom_436', 'hom_750', 'hom_846', 'hom_1464', 'hom_1623']\n",
    "het_dataset, het_examples = select_examples(embeddings_model=embeddings_model, dataset=het_dataset)\n",
    "# Examples of pun: ['het_1751', 'het_530', 'het_633', 'het_325', 'het_496', 'het_519', 'het_345', 'het_621', 'het_1453', 'het_1774']\n",
    "# Examples of non-pun: ['het_151', 'het_957', 'het_115', 'het_225', 'het_999', 'het_563', 'het_917', 'het_41', 'het_533', 'het_1041']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-22T21:11:37.947607Z",
     "end_time": "2024-01-22T21:33:13.728236Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hom: pun 810, non-pun 633\n",
      "het: pun 647, non-pun 499\n"
     ]
    }
   ],
   "source": [
    "# Count\n",
    "hom_punDataset, hom_nonpunDataset = splitDataset(hom_dataset, puntype='hom')\n",
    "het_punDataset, het_nonpunDataset = splitDataset(het_dataset, puntype='het')\n",
    "print(f'hom: pun {len(hom_punDataset)}, non-pun {len(hom_nonpunDataset)}')\n",
    "print(f'het: pun {len(het_punDataset)}, non-pun {len(het_nonpunDataset)}')\n",
    "# Save\n",
    "hom_dataset_save = r'./dataset/hom_dataset.json'\n",
    "het_dataset_save = r'./dataset/het_dataset.json'\n",
    "hom_examples_save = r'./dataset/hom_examples.json'\n",
    "het_examples_save = r'./dataset/het_examples.json'\n",
    "\n",
    "save_json_file(hom_dataset, hom_dataset_save)\n",
    "save_json_file(het_dataset, het_dataset_save)\n",
    "save_json_file(hom_examples, hom_examples_save)\n",
    "save_json_file(het_examples, het_examples_save)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-22T21:37:50.791232Z",
     "end_time": "2024-01-22T21:37:50.836402Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
