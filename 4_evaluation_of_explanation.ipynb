{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load Package"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from random import seed, shuffle\n",
    "from collections import Counter\n",
    "import scipy.stats as stats\n",
    "\n",
    "from tqdm import tqdm\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from _api_key import get_openai_api_key"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-12T02:00:44.161723Z",
     "end_time": "2024-04-12T02:00:45.273246Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Basic Function"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def load_json_file(file_path):\n",
    "    \"\"\"\n",
    "    Load json file\n",
    "    \"\"\"\n",
    "    with open(file_path,'r',encoding='utf-8') as f:\n",
    "        file = json.load(f)\n",
    "        f.close()\n",
    "    return file\n",
    "\n",
    "def save_json_file(file, file_path, sort_keys:bool=False):\n",
    "    \"\"\"\n",
    "    Save json file\n",
    "    \"\"\"\n",
    "    with open(file_path,'w',encoding='utf-8') as f:\n",
    "        json.dump(file, f, indent=4, ensure_ascii=False, sort_keys=sort_keys)\n",
    "        f.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-12T02:00:45.274132Z",
     "end_time": "2024-04-12T02:00:45.287158Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def splitDataset(file, puntype='hom'):\n",
    "    \"\"\"\n",
    "    Enter path or json file to separate the pun part from the non-pun part of the dataset\n",
    "    \"\"\"\n",
    "    if isinstance(file,str):\n",
    "        dataset = load_json_file(file)\n",
    "    else:\n",
    "        dataset = file\n",
    "    punDataset = dict()\n",
    "    nonpunDataset = dict()\n",
    "    for ID in dataset:\n",
    "        data = dataset[ID]\n",
    "        if puntype in ID:\n",
    "            if data.get('pun_word', False):\n",
    "                punDataset[ID] = data\n",
    "            else:\n",
    "                nonpunDataset[ID] = data\n",
    "    return punDataset, nonpunDataset\n",
    "\n",
    "def ids_sampling(ids:list, sample_size:int=100):\n",
    "    \"\"\"\n",
    "    Random sampling from the IDs\n",
    "    \"\"\"\n",
    "    seed(2024)\n",
    "    shuffle(ids); shuffle(ids)\n",
    "    return ids[0:sample_size]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-12T02:00:45.290145Z",
     "end_time": "2024-04-12T02:00:45.303151Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Function of Evaluating Explanation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Punchline Check"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def evaluate_explanation_by_punchline(dataset, explanations, target:str, sample_fn=None,\n",
    "                                      save:bool=False):\n",
    "    \"\"\"\n",
    "    Fine-grained manual evaluation of explanation\n",
    "    Pun: pun_word, alter_word, pun_sense, alter_sense\n",
    "    Non-pun: rationality, informativeness\n",
    "    \"\"\"\n",
    "    def get_expl(explanation, target):\n",
    "        if target == 'human':\n",
    "            expl = explanation[f'{target}_explanation']\n",
    "        else:\n",
    "            expl = explanation[f'{target}_explanation def_true CoT_true examples_true']['biased_to_pun']\n",
    "        return expl\n",
    "\n",
    "    target_range = ['human','gpt-3.5-turbo-1106','gpt-4-1106-preview','gemini-pro','claude-3-opus-20240229',\n",
    "                    'vicuna-7b-v1.5','llama-2-7b-chat','mistral-7b-instruct-v0.2','openchat-3.5-0106']\n",
    "    assert target in target_range\n",
    "    path = f'./results/pun_explanation_punchline_check({target}).json'\n",
    "    if os.path.exists(path):\n",
    "        record = load_json_file(path)\n",
    "    else:\n",
    "        record = dict()\n",
    "    # Whether to conduct sampling\n",
    "    IDs = list(dataset.keys())\n",
    "    if sample_fn is not None:\n",
    "        IDs = sample_fn(IDs)\n",
    "    for ID in tqdm(IDs):\n",
    "        # Get text and explanation\n",
    "        data = dataset[ID]\n",
    "        text = data['human_text']\n",
    "        expl = get_expl(explanations[ID], target)\n",
    "        # Integrate into record\n",
    "        if data.get('pun_word',False):\n",
    "            pun_word = data['pun_word']\n",
    "            pun_sense = data['pun_sense']\n",
    "            alter_word = data['alter_word']\n",
    "            alter_sense = data['alter_sense']\n",
    "            if 'hom' in ID:\n",
    "                record[ID] = {'text': text,\n",
    "                              'pun_word':pun_word,\n",
    "                              'pun_sense':pun_sense,\n",
    "                              'alter_word':alter_word,\n",
    "                              'alter_sense':alter_sense,\n",
    "                              'explanation': expl,\n",
    "                              'human_eval':{'pun_word':None, 'alter_word':-1,\n",
    "                                            'pun_sense':None, 'alter_sense':None}}\n",
    "            else:\n",
    "                record[ID] = {'text': text,\n",
    "                              'pun_word':pun_word,\n",
    "                              'pun_sense':pun_sense,\n",
    "                              'alter_word':alter_word,\n",
    "                              'alter_sense':alter_sense,\n",
    "                              'explanation': expl,\n",
    "                              'human_eval':{'pun_word':None, 'alter_word':None,\n",
    "                                            'pun_sense':None, 'alter_sense':None}}\n",
    "        else:\n",
    "            if target == 'human':\n",
    "                record[ID] = {'text': text,\n",
    "                              'explanation': expl,\n",
    "                              'human_eval':{'rationality':-1, 'informativeness':-1}}\n",
    "            else:\n",
    "                record[ID] = {'text': text,\n",
    "                              'explanation': expl,\n",
    "                              'human_eval':{'rationality':None, 'informativeness':None}}\n",
    "    if save:\n",
    "        save_json_file(record,path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-12T02:00:45.303151Z",
     "end_time": "2024-04-12T02:00:45.320121Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def punchline_check_integration(*file_paths, evaluator:str='human_eval', save:bool=False,\n",
    "                                path:str='pun_explanation_punchline_check.json'):\n",
    "    \"\"\"\n",
    "    Integrate the results of punchline check into one file\n",
    "    \"\"\"\n",
    "    if 'results' not in path:\n",
    "        path = './results/' + path\n",
    "    integration = dict()\n",
    "    for file_path in file_paths:\n",
    "        if os.path.exists(file_path):\n",
    "            file = load_json_file(file_path)\n",
    "            target = file_path.split('(')[-1].split(')')[0]\n",
    "            key = f'{evaluator} {target}_explanation'\n",
    "            for ID in file:\n",
    "                if ID not in integration:\n",
    "                    integration[ID] = dict()\n",
    "                # Data of the evaluation\n",
    "                eval = file[ID][evaluator]\n",
    "                integration[ID].update({key: eval})\n",
    "        else:\n",
    "            raise AssertionError(f\"{file_path} is not a valid file\")\n",
    "    if save:\n",
    "        save_json_file(integration, path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-12T02:00:45.320121Z",
     "end_time": "2024-04-12T02:00:45.366127Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def punchline_check_summary(evaluations, save:bool=False, path:str='pun_explanation_punchline_check_metrics.json'):\n",
    "    \"\"\"\n",
    "    Calculate the various indicators of punchline check  \\n\n",
    "    Pun: mean of pun_word, alter_word, pun_sense, alter_sense  \\n\n",
    "    Non-pun: mean of rationality, informativeness, R1I0, R0I1\n",
    "    \"\"\"\n",
    "    def dict_union(*dicts):\n",
    "        # Add values when dictionary keys are the same\n",
    "        keys = set() # union of keys\n",
    "        for d in dicts:\n",
    "            # Make sure inputs are all dictionary type\n",
    "            assert isinstance(d, dict)\n",
    "            keys = keys | d.keys()\n",
    "        if 'pun_word' in keys:\n",
    "            keys = sorted(list(keys), key=lambda x:x[::-1])\n",
    "        else:\n",
    "            keys = sorted(list(keys), reverse=True)\n",
    "        union = dict()\n",
    "        for key in keys:\n",
    "            union[key] = sum([d.get(key,0) for d in dicts])\n",
    "        return union\n",
    "\n",
    "    if 'results' not in path:\n",
    "        path = './results/' + path\n",
    "    IDs = list(evaluations.keys())\n",
    "    eval_keys = list(evaluations[IDs[0]].keys())\n",
    "    summary = dict()\n",
    "    counts = dict()\n",
    "    # Summarize the results of punchline check of different models\n",
    "    for eval_key in eval_keys:\n",
    "        hom_pun_evals = [evaluations[ID][eval_key] for ID in IDs if ('hom' in ID and 'pun_word' in evaluations[ID][eval_key])]\n",
    "        het_pun_evals = [evaluations[ID][eval_key] for ID in IDs if ('het' in ID and 'pun_word' in evaluations[ID][eval_key])]\n",
    "        non_pun_evals = [evaluations[ID][eval_key] for ID in IDs if (evaluations[ID][eval_key] not in hom_pun_evals + het_pun_evals)]\n",
    "        # Add additional indicators of non-puns\n",
    "        for i in range(len(non_pun_evals)):\n",
    "            non_pun_eval = non_pun_evals[i]\n",
    "            rationality = non_pun_eval['rationality']\n",
    "            informativeness = non_pun_eval['informativeness']\n",
    "            R1I0 = 1 if (rationality == 1 and informativeness == 0) else 0\n",
    "            R0I1 = 1 if (rationality == 0 and informativeness == 1) else 0\n",
    "            non_pun_eval.update({'R1I0':R1I0, 'R0I1':R0I1})\n",
    "            non_pun_evals[i] = non_pun_eval\n",
    "        summary[eval_key] = {'hom_pun': dict_union(*hom_pun_evals),\n",
    "                             'het_pun': dict_union(*het_pun_evals),\n",
    "                             'non_pun': dict_union(*non_pun_evals)}\n",
    "        counts[eval_key] = {'hom_pun': len(hom_pun_evals),\n",
    "                            'het_pun': len(het_pun_evals),\n",
    "                            'non_pun': len(non_pun_evals)}\n",
    "        # For homographic puns, the pun word and alternative word are the same\n",
    "        summary[eval_key]['hom_pun']['alter_word'] = summary[eval_key]['hom_pun']['pun_word']\n",
    "    # Normalization\n",
    "    for eval_key in summary:\n",
    "        for type_key in summary[eval_key]:\n",
    "            temp = summary[eval_key][type_key]\n",
    "            num = counts[eval_key][type_key]\n",
    "            temp ={k:(round(v/num,4) if v>=0 else None) for k,v in temp.items()}\n",
    "            summary[eval_key][type_key] = temp\n",
    "    print(json.dumps(summary, indent=4))\n",
    "    if save:\n",
    "        save_json_file(summary, path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-12T02:00:45.337815Z",
     "end_time": "2024-04-12T02:00:45.367653Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Pairwise Comparison"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def evaluate_explanation_by_comparison(dataset, explanations, target1:str, target2:str, swap:bool=False, sample_fn=None,\n",
    "                                       eval_model=None, examples:dict=None, batch_size:int=1, save:bool=False):\n",
    "    \"\"\"\n",
    "    Coarse-grained pairwise comparison of human explanation and model explanation  \\n\n",
    "    Small batch manual eval to align the performance of model(gpt4) eval\n",
    "    \"\"\"\n",
    "    def get_expl(explanation, target):\n",
    "        # Get corresponding explanation\n",
    "        if target == 'human':\n",
    "            expl = explanation[f'{target}_explanation']\n",
    "        else:\n",
    "            expl = explanation[f'{target}_explanation def_true CoT_true examples_true']['biased_to_pun']\n",
    "        return expl\n",
    "    def parse_output(output:str, swap:bool=False):\n",
    "        # Parse the output and get the choice\n",
    "        try:\n",
    "            output = output[output.index('{'): output.index('}')+1]\n",
    "        except:\n",
    "            output = output\n",
    "        try:\n",
    "            choice_str = eval(output)['Choice']\n",
    "        except:\n",
    "            choice_str = output\n",
    "        if \"Explanation 1 is much better\" in choice_str:\n",
    "            choice = 1\n",
    "        elif \"Explanation 2 is much better\" in choice_str:\n",
    "            choice = 2\n",
    "        else:\n",
    "            choice = 0\n",
    "        if swap and choice > 0:\n",
    "            choice = 3 - choice\n",
    "        return choice\n",
    "\n",
    "    target_range = ['human','gpt-3.5-turbo-1106','gpt-4-1106-preview','gemini-pro','claude-3-opus-20240229',\n",
    "                    'vicuna-7b-v1.5','llama-2-7b-chat','mistral-7b-instruct-v0.2','openchat-3.5-0106']\n",
    "    assert target1 in target_range\n",
    "    assert target2 in target_range\n",
    "    # Path of the output file\n",
    "    if eval_model is None:\n",
    "        path = f'./results/pun_explanation_pairwise_comparison({target1}_vs_{target2}).json'\n",
    "    else:\n",
    "        if sample_fn is None:\n",
    "            path = f'./results/pun_explanation_pairwise_comparison.json'\n",
    "        else:\n",
    "            path = f'./results/pun_explanation_pairwise_comparison_pilot.json'\n",
    "    if os.path.exists(path):\n",
    "        record = load_json_file(path)\n",
    "    else:\n",
    "        record = dict()\n",
    "    # Whether to conduct sampling\n",
    "    IDs = list(dataset.keys())\n",
    "    if sample_fn is not None:\n",
    "        IDs = sample_fn(IDs)\n",
    "\n",
    "    # [A]. Evaluate by human\n",
    "    if eval_model is None:\n",
    "        for ID in tqdm(IDs):\n",
    "            data = dataset[ID]\n",
    "            assert data.get('pun_word',False) # Make sure it's a pun\n",
    "            text = data['human_text']\n",
    "            pun_word = data['pun_word']\n",
    "            pun_sense = data['pun_sense']\n",
    "            alter_word = data['alter_word']\n",
    "            alter_sense = data['alter_sense']\n",
    "            expl1 = get_expl(explanations[ID], target1)\n",
    "            expl2 = get_expl(explanations[ID], target2)\n",
    "            record[ID] = {'text': text,\n",
    "                          'pun_word':pun_word,\n",
    "                          'pun_sense':pun_sense,\n",
    "                          'alter_word':alter_word,\n",
    "                          'alter_sense':alter_sense,\n",
    "                          'explanation1': expl1,\n",
    "                          'explanation2': expl2,\n",
    "                          'human_eval':{'winner':None}}\n",
    "        if save:\n",
    "            save_json_file(record, path)\n",
    "    # [B]. Evaluate by model (llm)\n",
    "    else:\n",
    "        # <B.a>. Construct the prompt\n",
    "        definition = \"\"\"<*Definition*>\\nPuns are a form of wordplay exploiting different meanings of a word or similar-sounding words.\\n\\n\"\"\"\n",
    "        instruction = \"\"\"<*Instruction*>\\nBelow is a pun text, double meanings of the pun and two corresponding explanations. Please carefully judge which explanation is of better quality. Any explanation that fails to indicate the correct pun, misses the potential phonetic similarity between pun-alternative word pair, misses a layer of correct meaning in the pun or contains other errors is a worse explanation. Meanwhile, explanations without the above errors are better explanations. To complete the task, you must cautiously choose from one of the three answers: \"Explanation 1 is much better\", \"Explanation 2 is much better\", \"Explanation 1 and 2 are of similar quality\". Additionally, You must output the current status in a parsable JSON format. An example output looks like:\\n{{\"Choice\": \"XXX\"}}\"\"\"\n",
    "        if examples is not None:\n",
    "            examples_temp = []\n",
    "            for ID in examples:\n",
    "                example = examples[ID]\n",
    "                examples_temp.append(f\"Text: {example['text']}\\nDouble Meanings of the Pun: \"\n",
    "                                     f\"1. pun word and its meaning: {example['pun_word']} <{example['pun_sense']}>. \"\n",
    "                                     f\"2. alternative word and its meaning: {example['alter_word']} <{example['alter_sense']}>.\\n\"\n",
    "                                     f\"Explanation 1: {example['explanation1']}\\n\"\n",
    "                                     f\"Explanation 2: {example['explanation2']}\\n\"\n",
    "                                     f\"Output:\\n{{{{\\\"Choice\\\": \\\"{example['choice']}\\\"}}}}\")\n",
    "            examples_string = '\\n\\n<*Examples*>\\n' + '\\n\\n'.join(examples_temp)\n",
    "        else:\n",
    "            examples_string = ''\n",
    "        testing = \"\\n\\n<*Your Response*>\\nText: {text}\\nDouble Meanings of the Pun: \" \\\n",
    "                  \"1. pun word and its meaning: {pun_word} <{pun_sense}>. \" \\\n",
    "                  \"2. alternative word and its meaning: {alter_word} <{alter_sense}>.\\n\" \\\n",
    "                  \"Explanation 1: {expl1}\\nExplanation 2: {expl2}\\n\" \\\n",
    "                  \"Output:\"\n",
    "        prompt_string = definition + instruction + examples_string + testing\n",
    "        chat_prompt = ChatPromptTemplate.from_template(prompt_string)\n",
    "        # <B.b>. Call LLM to respond\n",
    "        model_name = eval_model.model_name\n",
    "        key_eval = f'{model_name}_eval {target1}_vs_{target2} swap' if swap else f'{model_name}_eval {target1}_vs_{target2}'\n",
    "        IDs_loaded = []\n",
    "        for ID in record:\n",
    "            if record[ID].get(key_eval, False):\n",
    "                IDs_loaded.append(ID)\n",
    "        all_ind = list(range(0,len(IDs)))\n",
    "        batch_ind = list(range(0,len(IDs),batch_size))\n",
    "        for ind in tqdm(all_ind):\n",
    "            if ind not in batch_ind:\n",
    "                continue\n",
    "            # Remove the data that has already been evaluated\n",
    "            IDs_batch = IDs[ind: ind+batch_size]\n",
    "            IDs_batch = list(set(IDs_batch)-set(IDs_loaded))\n",
    "            if len(IDs_batch) == 0:\n",
    "                continue\n",
    "            _inputs = []\n",
    "            for ID in IDs_batch:\n",
    "                # Get text and explanation\n",
    "                data = dataset[ID]\n",
    "                assert data.get('pun_word',False) # Make sure it's a pun\n",
    "                text = data['human_text']\n",
    "                pun_word = data['pun_word']\n",
    "                pun_sense = data['pun_sense']\n",
    "                alter_word = data['alter_word']\n",
    "                alter_sense = data['alter_sense']\n",
    "                expl1 = get_expl(explanations[ID], target1)\n",
    "                expl2 = get_expl(explanations[ID], target2)\n",
    "                if not swap:\n",
    "                    _inputs.append(chat_prompt.format_messages(text=text, pun_word=pun_word, pun_sense=pun_sense,\n",
    "                                                                 alter_word=alter_word, alter_sense=alter_sense,\n",
    "                                                                 expl1=expl1, expl2=expl2))\n",
    "                else:\n",
    "                    _inputs.append(chat_prompt.format_messages(text=text, pun_word=pun_word, pun_sense=pun_sense,\n",
    "                                                                 alter_word=alter_word, alter_sense=alter_sense,\n",
    "                                                                 expl1=expl2, expl2=expl1))\n",
    "            # call the llm\n",
    "            _outputs = [out.content for out in eval_model.batch(_inputs)]\n",
    "            # print(_inputs[0][0].content)\n",
    "            # print(_outputs[0])\n",
    "            # break\n",
    "            for ID, out in zip(IDs_batch, _outputs):\n",
    "                # 1 means target1 is better, 2 means target2 is better, 0 means model is not sure which is better,\n",
    "                winner = parse_output(out, swap=swap)\n",
    "                evaluation = {'winner':winner}\n",
    "                if ID not in record:\n",
    "                    record[ID] = {key_eval: evaluation}\n",
    "                else:\n",
    "                    record[ID].update({key_eval: evaluation})\n",
    "            if save:\n",
    "                save_json_file(record, path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-12T02:00:45.350444Z",
     "end_time": "2024-04-12T02:00:45.367653Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def pairwise_comparison_integration(*file_paths, save:bool=False,\n",
    "                                    path:str='pun_explanation_pairwise_comparison_pilot.json'):\n",
    "    \"\"\"\n",
    "    Integrate the results of pairwise comparison into one file (for manual pilot)\n",
    "    \"\"\"\n",
    "    if 'results' not in path:\n",
    "        path = './results/' + path\n",
    "    integration = dict()\n",
    "    for file_path in file_paths:\n",
    "        if os.path.exists(file_path):\n",
    "            file = load_json_file(file_path)\n",
    "            # Part of human eval in pilot\n",
    "            if '(' in file_path and ')' in file_path:\n",
    "                evaluator = 'human_eval'\n",
    "                targets = file_path.split('(')[-1].split(')')[0]\n",
    "                key = f'{evaluator} {targets}'\n",
    "                for ID in file:\n",
    "                    if ID not in integration:\n",
    "                        integration[ID] = dict()\n",
    "                    eval = file[ID][evaluator]\n",
    "                    integration[ID].update({key: eval})\n",
    "            # Part of model eval in pilot\n",
    "            else:\n",
    "                IDs = list(file.keys())\n",
    "                evaluators = list(file[IDs[0]].keys())\n",
    "                for evaluator in evaluators:\n",
    "                    for ID in file:\n",
    "                        if ID not in integration:\n",
    "                            integration[ID] = dict()\n",
    "                        eval = file[ID][evaluator]\n",
    "                        integration[ID].update({evaluator: eval})\n",
    "        else:\n",
    "            raise AssertionError(f\"{file_path} is not a valid file\")\n",
    "    if save:\n",
    "        save_json_file(integration, path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-12T02:00:45.367653Z",
     "end_time": "2024-04-12T02:00:45.384475Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def pairwise_comparison_summary(evaluations, is_pilot:bool=False, save:bool=False):\n",
    "    \"\"\"\n",
    "    Calculate the various indicators of pairwise comparison  \\n\n",
    "    Pilot: consistency between model and human and consistency after exchanging explanations  \\n\n",
    "    Full-scale: average winning rate\n",
    "    \"\"\"\n",
    "    if is_pilot:\n",
    "        # Pilot\n",
    "        path = r'./results/pun_explanation_pairwise_comparison_pilot_metrics.json'\n",
    "        IDs = list(evaluations.keys())\n",
    "        eval_keys = list(evaluations[IDs[0]].keys())\n",
    "        evaluators = {eval_key.split(' ')[0] for eval_key in eval_keys} - {'human_eval'}\n",
    "        targets = {eval_key.split(' ')[1] for eval_key in eval_keys}\n",
    "        summary = dict()\n",
    "        for target in targets:\n",
    "            if target not in summary:\n",
    "                summary[target] = dict()\n",
    "            for evaluator in evaluators:\n",
    "                # Consistency with human\n",
    "                key = 'consistency_with_human'\n",
    "                human_eval = [evaluations[ID][f'human_eval {target}']['winner'] for ID in IDs]\n",
    "                model_eval = [evaluations[ID][f'{evaluator} {target}']['winner'] for ID in IDs]\n",
    "                CWH = sum([h==m for h,m in zip(human_eval, model_eval)])/len(human_eval)\n",
    "                summary[target][key] = round(CWH, 4)\n",
    "                # Consistency between swap\n",
    "                key = 'consistency_between_swap'\n",
    "                model_eval1 = [evaluations[ID][f'{evaluator} {target}']['winner'] for ID in IDs]\n",
    "                model_eval2 = [evaluations[ID][f'{evaluator} {target} swap']['winner'] for ID in IDs]\n",
    "                CBS = sum([m1==m2 for m1,m2 in zip(model_eval1, model_eval2)])/len(model_eval1)\n",
    "                summary[target][key] = round(CBS, 4)\n",
    "        # Human and model agreement\n",
    "        human_eval, model_eval = [], []\n",
    "        for target in targets:\n",
    "            for evaluator in evaluators:\n",
    "                human_eval.extend([evaluations[ID][f'human_eval {target}']['winner'] for ID in IDs])\n",
    "                model_eval.extend([evaluations[ID][f'{evaluator} {target}']['winner'] for ID in IDs])\n",
    "        # Various correlation coefficients\n",
    "        pearson_corr, p_value_pearson = stats.pearsonr(human_eval, model_eval)\n",
    "        spearman_corr, p_value_spearman = stats.spearmanr(human_eval, model_eval)\n",
    "        kendall_tau, p_value_kendall = stats.kendalltau(human_eval, model_eval)\n",
    "        print(f'pearson_corr:{pearson_corr}, p_value_pearson:{p_value_pearson}\\n'\n",
    "              f'spearman_corr:{spearman_corr}, p_value_spearman:{p_value_spearman}\\n'\n",
    "              f'kendall_tau:{kendall_tau}, p_value_kendall:{p_value_kendall}')\n",
    "    else:\n",
    "        # Full-scale\n",
    "        path = r'./results/pun_explanation_pairwise_comparison_metrics.json'\n",
    "        winner = {0:\"tie\", 1:\"human_win\", 2:\"model_win\", }\n",
    "        IDs = list(evaluations.keys())\n",
    "        targets = list(evaluations[IDs[0]].keys())\n",
    "        summary = dict()\n",
    "        for target in targets:\n",
    "            if target not in summary:\n",
    "                summary[target] = dict()\n",
    "            hom_eval = [evaluations[ID][target]['winner'] for ID in IDs if 'hom' in ID]\n",
    "            het_eval = [evaluations[ID][target]['winner'] for ID in IDs if 'het' in ID]\n",
    "            hom_num =len(hom_eval)\n",
    "            het_num = len(het_eval)\n",
    "            hom_counter = Counter(hom_eval)\n",
    "            het_counter = Counter(het_eval)\n",
    "            hom_summary = {w:round(hom_counter[i]/hom_num,4) for i,w in winner.items()}\n",
    "            het_summary = {w:round(het_counter[i]/het_num,4) for i,w in winner.items()}\n",
    "            summary[target]['hom'] = hom_summary\n",
    "            summary[target]['het'] = het_summary\n",
    "    print(json.dumps(summary, indent=4))\n",
    "    if save:\n",
    "        save_json_file(summary, path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-12T02:00:45.384475Z",
     "end_time": "2024-04-12T02:00:45.399838Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset and Examples"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "explanation_path = r'./results/pun_explanation.json'\n",
    "pun_explanation = load_json_file(explanation_path)\n",
    "\n",
    "hom_path = r'./dataset/hom_dataset.json'\n",
    "het_path = r'./dataset/het_dataset.json'\n",
    "hom_punDataset, hom_nonpunDataset = splitDataset(hom_path, puntype='hom')\n",
    "het_punDataset, het_nonpunDataset = splitDataset(het_path, puntype='het')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-27T21:48:36.154547Z",
     "end_time": "2024-03-27T21:48:36.390321Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# Choose data from examples manually\n",
    "hom_examples = {\n",
    "    \"hom_1404\":{\"text\":\"He had trouble in his new job making tents and got himself into a flap .\",\n",
    "                \"pun_word\": \"flap\",\n",
    "                \"pun_sense\": \"an excited state of agitation\",\n",
    "                \"alter_word\": \"flap\",\n",
    "                \"alter_sense\": \"any broad thin and limber covering attached at one edge; hangs loose or projects freely\",\n",
    "                \"explanation1\":\"To get in a flap means to get upset, or unhappy about something. Flaps can also refer to cloth that is hinged on one side that cover an opening. A tent is a shelter made from fabric, and has flaps. This joke is playing on the word 'flaps' and its different meanings.\",\n",
    "                \"explanation2\":\"The text exploits the double meaning of 'flap,' which can refer to both a problem or commotion and a part of a tent. It plays on the ambiguity of the word in the context of the job, creating a pun.\",\n",
    "                \"choice\":\"Explanation 1 is much better\"},\n",
    "    \"hom_1162\":{\"text\":\"Old carpenters never die , they just lumber around .\",\n",
    "                \"pun_word\": \"lumber\",\n",
    "                \"pun_sense\": \"move heavily or clumsily\",\n",
    "                \"alter_word\": \"lumber\",\n",
    "                \"alter_sense\": \"cut lumber, as in woods and forests\",\n",
    "                \"explanation1\":\"Carpenters make and repair wooden objects and structures. Lumber is a type of wood. Lumber also means to move in a slow, heavy, awkward way. Carpenters lumber around.\",\n",
    "                \"explanation2\":\"The text is a play on the phrase 'Old [people] never die, they just...' where the second part humorously reinterprets the expected outcome. In this case, 'lumber around' plays on the idea of old carpenters working with lumber while also suggesting a slow or clumsy movement, creating a pun on the expected phrase.\",\n",
    "                \"choice\":\"Explanation 2 is much better\"},\n",
    "    \"hom_705\":{\"text\":\"Driving on so many turnpikes was taking its toll .\",\n",
    "               \"pun_word\": \"toll\",\n",
    "               \"pun_sense\": \"a fee levied for the use of roads or bridges (used for maintenance)\",\n",
    "               \"alter_word\": \"toll\",\n",
    "               \"alter_sense\": \"value measured by what must be given or done or undergone to obtain something\",\n",
    "               \"explanation1\":\"When someone is 'taking its toll' it means that something is having a bad effect on someone. A toll is a charge payable for permission to use a particular bridge or road. The word 'toll' is being used in two different ways here.\",\n",
    "               \"explanation2\":\"The text is using the word 'toll' in a double entendre. It refers both to the physical tolls paid on turnpikes and to 'taking its toll' as in having a negative effect or cost.\",\n",
    "               \"choice\":\"Explanation 1 and 2 are of similar quality\"}\n",
    "}\n",
    "\n",
    "het_examples = {\n",
    "    \"het_1453\":{\"text\":\"The whistling fisherman was always out of tuna .\",\n",
    "                \"pun_word\": \"tuna\",\n",
    "                \"pun_sense\": \"any very large marine food and game fish of the genus Thunnus; related to mackerel; chiefly of warm waters\",\n",
    "                \"alter_word\": \"tune\",\n",
    "                \"alter_sense\": \"the property of producing accurately a note of a given pitch\",\n",
    "                \"explanation1\":\"The joke is a play on words. The word 'tuna' sounds like 'tune'.  The fisherman who whistles is always 'out of tune' and the word tune was replaced with a word for a type of fish.\",\n",
    "                \"explanation2\":\"The text exploits the double meaning of 'tuna' - it can refer to both the fish and the musical instrument. This creates a play on words.\",\n",
    "                \"choice\":\"Explanation 1 is much better\"},\n",
    "    \"het_1774\":{\"text\":\"In a billiard hall life can be pretty rough at the wrong end of the queue .\",\n",
    "                \"pun_word\": \"queue\",\n",
    "                \"pun_sense\": \"a line of people or vehicles waiting for something\",\n",
    "                \"alter_word\": \"cue\",\n",
    "                \"alter_sense\": \"sports implement consisting of a tapering rod used to strike a cue ball in pool or billiards\",\n",
    "                \"explanation1\":\"This is a play on words. The word 'queue' means a line of people waiting for something but a 'queue' is also a term for equipment used in billiards.\",\n",
    "                \"explanation2\":\"The text plays on the homophones 'queue' and 'cue', creating a humorous double meaning. 'Queue' refers to the waiting line, while 'cue' is a stick used in billiards. The phrase 'wrong end of the queue' is a common expression, but the play on words adds a comedic twist.\",\n",
    "               \"choice\":\"Explanation 2 is much better\"},\n",
    "    \"het_530\":{\"text\":\"A tangled bell ringer tolled himself off .\",\n",
    "               \"pun_word\": \"toll\",\n",
    "               \"pun_sense\": \"ring slowly\",\n",
    "               \"alter_word\": \"tell off\",\n",
    "               \"alter_sense\": \"reprimand\",\n",
    "               \"explanation1\":\"The joke is a pun. A bell can be 'tolled' by ringing it.  'Tolled' sounds like 'told' which makes it sound like the bell ringer told himself off, or scolded himself.\",\n",
    "               \"explanation2\":\"The text plays on the homophones 'tolled' and 'told', using the word 'tolled' in the context of a bell ringer (which relates to the ringing or tolling of bells) and 'told' as in scolding oneself (told sb off). This creates a humorous double meaning.\",\n",
    "               \"choice\":\"Explanation 1 and 2 are of similar quality\"},\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-27T21:48:36.390321Z",
     "end_time": "2024-03-27T21:48:36.405982Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluate Explanation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Punchline Check"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### gpt3.5"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<?, ?it/s]\n",
      "100%|██████████| 100/100 [00:00<?, ?it/s]\n",
      "100%|██████████| 100/100 [00:00<?, ?it/s]\n",
      "100%|██████████| 100/100 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "target='gpt-3.5-turbo-1106'\n",
    "evaluate_explanation_by_punchline(hom_punDataset, pun_explanation, target, ids_sampling, save=True)\n",
    "evaluate_explanation_by_punchline(hom_nonpunDataset, pun_explanation, target, ids_sampling, save=True)\n",
    "evaluate_explanation_by_punchline(het_punDataset, pun_explanation, target, ids_sampling, save=True)\n",
    "evaluate_explanation_by_punchline(het_nonpunDataset, pun_explanation, target, ids_sampling, save=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-06T19:04:10.697559Z",
     "end_time": "2024-03-06T19:04:10.766517Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### gpt4"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<?, ?it/s]\n",
      "100%|██████████| 100/100 [00:00<?, ?it/s]\n",
      "100%|██████████| 100/100 [00:00<?, ?it/s]\n",
      "100%|██████████| 100/100 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "target='gpt-4-1106-preview'\n",
    "evaluate_explanation_by_punchline(hom_punDataset, pun_explanation, target, ids_sampling, save=True)\n",
    "evaluate_explanation_by_punchline(hom_nonpunDataset, pun_explanation, target, ids_sampling, save=True)\n",
    "evaluate_explanation_by_punchline(het_punDataset, pun_explanation, target, ids_sampling, save=True)\n",
    "evaluate_explanation_by_punchline(het_nonpunDataset, pun_explanation, target, ids_sampling, save=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-06T19:04:12.530614Z",
     "end_time": "2024-03-06T19:04:12.602756Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### gemini-pro"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<?, ?it/s]\n",
      "100%|██████████| 100/100 [00:00<?, ?it/s]\n",
      "100%|██████████| 100/100 [00:00<?, ?it/s]\n",
      "100%|██████████| 100/100 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "target='gemini-pro'\n",
    "evaluate_explanation_by_punchline(hom_punDataset, pun_explanation, target, ids_sampling, save=True)\n",
    "evaluate_explanation_by_punchline(hom_nonpunDataset, pun_explanation, target, ids_sampling, save=True)\n",
    "evaluate_explanation_by_punchline(het_punDataset, pun_explanation, target, ids_sampling, save=True)\n",
    "evaluate_explanation_by_punchline(het_nonpunDataset, pun_explanation, target, ids_sampling, save=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-06T19:04:14.569967Z",
     "end_time": "2024-03-06T19:04:14.619479Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### claude3"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<?, ?it/s]\n",
      "100%|██████████| 100/100 [00:00<?, ?it/s]\n",
      "100%|██████████| 100/100 [00:00<?, ?it/s]\n",
      "100%|██████████| 100/100 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "target='claude-3-opus-20240229'\n",
    "evaluate_explanation_by_punchline(hom_punDataset, pun_explanation, target, ids_sampling, save=True)\n",
    "evaluate_explanation_by_punchline(hom_nonpunDataset, pun_explanation, target, ids_sampling, save=True)\n",
    "evaluate_explanation_by_punchline(het_punDataset, pun_explanation, target, ids_sampling, save=True)\n",
    "evaluate_explanation_by_punchline(het_nonpunDataset, pun_explanation, target, ids_sampling, save=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-26T20:15:09.895601Z",
     "end_time": "2024-03-26T20:15:09.954038Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### vicuna"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<?, ?it/s]\n",
      "100%|██████████| 100/100 [00:00<?, ?it/s]\n",
      "100%|██████████| 100/100 [00:00<?, ?it/s]\n",
      "100%|██████████| 100/100 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "target='vicuna-7b-v1.5'\n",
    "evaluate_explanation_by_punchline(hom_punDataset, pun_explanation, target, ids_sampling, save=True)\n",
    "evaluate_explanation_by_punchline(hom_nonpunDataset, pun_explanation, target, ids_sampling, save=True)\n",
    "evaluate_explanation_by_punchline(het_punDataset, pun_explanation, target, ids_sampling, save=True)\n",
    "evaluate_explanation_by_punchline(het_nonpunDataset, pun_explanation, target, ids_sampling, save=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-06T19:04:17.897820Z",
     "end_time": "2024-03-06T19:04:17.951527Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### llama2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<?, ?it/s]\n",
      "100%|██████████| 100/100 [00:00<?, ?it/s]\n",
      "100%|██████████| 100/100 [00:00<?, ?it/s]\n",
      "100%|██████████| 100/100 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "target='llama-2-7b-chat'\n",
    "evaluate_explanation_by_punchline(hom_punDataset, pun_explanation, target, ids_sampling, save=True)\n",
    "evaluate_explanation_by_punchline(hom_nonpunDataset, pun_explanation, target, ids_sampling, save=True)\n",
    "evaluate_explanation_by_punchline(het_punDataset, pun_explanation, target, ids_sampling, save=True)\n",
    "evaluate_explanation_by_punchline(het_nonpunDataset, pun_explanation, target, ids_sampling, save=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-06T19:04:22.594109Z",
     "end_time": "2024-03-06T19:04:22.685954Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### mistral"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<?, ?it/s]\n",
      "100%|██████████| 100/100 [00:00<?, ?it/s]\n",
      "100%|██████████| 100/100 [00:00<?, ?it/s]\n",
      "100%|██████████| 100/100 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "target='mistral-7b-instruct-v0.2'\n",
    "evaluate_explanation_by_punchline(hom_punDataset, pun_explanation, target, ids_sampling, save=True)\n",
    "evaluate_explanation_by_punchline(hom_nonpunDataset, pun_explanation, target, ids_sampling, save=True)\n",
    "evaluate_explanation_by_punchline(het_punDataset, pun_explanation, target, ids_sampling, save=True)\n",
    "evaluate_explanation_by_punchline(het_nonpunDataset, pun_explanation, target, ids_sampling, save=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-06T19:04:24.499652Z",
     "end_time": "2024-03-06T19:04:24.561373Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### openchat"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<?, ?it/s]\n",
      "100%|██████████| 100/100 [00:00<?, ?it/s]\n",
      "100%|██████████| 100/100 [00:00<?, ?it/s]\n",
      "100%|██████████| 100/100 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "target='openchat-3.5-0106'\n",
    "evaluate_explanation_by_punchline(hom_punDataset, pun_explanation, target, ids_sampling, save=True)\n",
    "evaluate_explanation_by_punchline(hom_nonpunDataset, pun_explanation, target, ids_sampling, save=True)\n",
    "evaluate_explanation_by_punchline(het_punDataset, pun_explanation, target, ids_sampling, save=True)\n",
    "evaluate_explanation_by_punchline(het_nonpunDataset, pun_explanation, target, ids_sampling, save=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-06T19:04:26.639938Z",
     "end_time": "2024-03-06T19:04:26.735929Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### human"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<?, ?it/s]\n",
      "100%|██████████| 100/100 [00:00<?, ?it/s]\n",
      "100%|██████████| 100/100 [00:00<?, ?it/s]\n",
      "100%|██████████| 100/100 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "target='human'\n",
    "evaluate_explanation_by_punchline(hom_punDataset, pun_explanation, target, ids_sampling, save=True)\n",
    "evaluate_explanation_by_punchline(hom_nonpunDataset, pun_explanation, target, ids_sampling, save=True)\n",
    "evaluate_explanation_by_punchline(het_punDataset, pun_explanation, target, ids_sampling, save=True)\n",
    "evaluate_explanation_by_punchline(het_nonpunDataset, pun_explanation, target, ids_sampling, save=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-06T19:04:29.844965Z",
     "end_time": "2024-03-06T19:04:29.906868Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### *summary"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"human_eval human_explanation\": {\n",
      "        \"hom_pun\": {\n",
      "            \"pun_word\": 0.95,\n",
      "            \"alter_word\": 0.95,\n",
      "            \"pun_sense\": 0.95,\n",
      "            \"alter_sense\": 0.95\n",
      "        },\n",
      "        \"het_pun\": {\n",
      "            \"pun_word\": 0.97,\n",
      "            \"alter_word\": 0.97,\n",
      "            \"pun_sense\": 0.94,\n",
      "            \"alter_sense\": 0.93\n",
      "        },\n",
      "        \"non_pun\": {\n",
      "            \"rationality\": null,\n",
      "            \"informativeness\": null,\n",
      "            \"R1I0\": 0.0,\n",
      "            \"R0I1\": 0.0\n",
      "        }\n",
      "    },\n",
      "    \"human_eval gpt-3.5-turbo-1106_explanation\": {\n",
      "        \"hom_pun\": {\n",
      "            \"pun_word\": 0.88,\n",
      "            \"alter_word\": 0.88,\n",
      "            \"pun_sense\": 0.81,\n",
      "            \"alter_sense\": 0.81\n",
      "        },\n",
      "        \"het_pun\": {\n",
      "            \"pun_word\": 0.91,\n",
      "            \"alter_word\": 0.55,\n",
      "            \"pun_sense\": 0.82,\n",
      "            \"alter_sense\": 0.57\n",
      "        },\n",
      "        \"non_pun\": {\n",
      "            \"rationality\": 0.51,\n",
      "            \"informativeness\": 0.65,\n",
      "            \"R1I0\": 0.05,\n",
      "            \"R0I1\": 0.19\n",
      "        }\n",
      "    },\n",
      "    \"human_eval gpt-4-1106-preview_explanation\": {\n",
      "        \"hom_pun\": {\n",
      "            \"pun_word\": 0.98,\n",
      "            \"alter_word\": 0.98,\n",
      "            \"pun_sense\": 0.96,\n",
      "            \"alter_sense\": 0.93\n",
      "        },\n",
      "        \"het_pun\": {\n",
      "            \"pun_word\": 0.96,\n",
      "            \"alter_word\": 0.9,\n",
      "            \"pun_sense\": 0.93,\n",
      "            \"alter_sense\": 0.85\n",
      "        },\n",
      "        \"non_pun\": {\n",
      "            \"rationality\": 0.905,\n",
      "            \"informativeness\": 0.965,\n",
      "            \"R1I0\": 0.025,\n",
      "            \"R0I1\": 0.085\n",
      "        }\n",
      "    },\n",
      "    \"human_eval gemini-pro_explanation\": {\n",
      "        \"hom_pun\": {\n",
      "            \"pun_word\": 0.92,\n",
      "            \"alter_word\": 0.92,\n",
      "            \"pun_sense\": 0.87,\n",
      "            \"alter_sense\": 0.81\n",
      "        },\n",
      "        \"het_pun\": {\n",
      "            \"pun_word\": 0.89,\n",
      "            \"alter_word\": 0.42,\n",
      "            \"pun_sense\": 0.83,\n",
      "            \"alter_sense\": 0.42\n",
      "        },\n",
      "        \"non_pun\": {\n",
      "            \"rationality\": 0.635,\n",
      "            \"informativeness\": 0.58,\n",
      "            \"R1I0\": 0.18,\n",
      "            \"R0I1\": 0.125\n",
      "        }\n",
      "    },\n",
      "    \"human_eval vicuna-7b-v1.5_explanation\": {\n",
      "        \"hom_pun\": {\n",
      "            \"pun_word\": 0.71,\n",
      "            \"alter_word\": 0.71,\n",
      "            \"pun_sense\": 0.64,\n",
      "            \"alter_sense\": 0.59\n",
      "        },\n",
      "        \"het_pun\": {\n",
      "            \"pun_word\": 0.85,\n",
      "            \"alter_word\": 0.21,\n",
      "            \"pun_sense\": 0.81,\n",
      "            \"alter_sense\": 0.29\n",
      "        },\n",
      "        \"non_pun\": {\n",
      "            \"rationality\": 0.4,\n",
      "            \"informativeness\": 0.5,\n",
      "            \"R1I0\": 0.105,\n",
      "            \"R0I1\": 0.205\n",
      "        }\n",
      "    },\n",
      "    \"human_eval llama-2-7b-chat_explanation\": {\n",
      "        \"hom_pun\": {\n",
      "            \"pun_word\": 0.63,\n",
      "            \"alter_word\": 0.63,\n",
      "            \"pun_sense\": 0.45,\n",
      "            \"alter_sense\": 0.42\n",
      "        },\n",
      "        \"het_pun\": {\n",
      "            \"pun_word\": 0.69,\n",
      "            \"alter_word\": 0.11,\n",
      "            \"pun_sense\": 0.47,\n",
      "            \"alter_sense\": 0.13\n",
      "        },\n",
      "        \"non_pun\": {\n",
      "            \"rationality\": 0.2,\n",
      "            \"informativeness\": 0.14,\n",
      "            \"R1I0\": 0.18,\n",
      "            \"R0I1\": 0.12\n",
      "        }\n",
      "    },\n",
      "    \"human_eval mistral-7b-instruct-v0.2_explanation\": {\n",
      "        \"hom_pun\": {\n",
      "            \"pun_word\": 0.78,\n",
      "            \"alter_word\": 0.78,\n",
      "            \"pun_sense\": 0.73,\n",
      "            \"alter_sense\": 0.68\n",
      "        },\n",
      "        \"het_pun\": {\n",
      "            \"pun_word\": 0.69,\n",
      "            \"alter_word\": 0.22,\n",
      "            \"pun_sense\": 0.68,\n",
      "            \"alter_sense\": 0.22\n",
      "        },\n",
      "        \"non_pun\": {\n",
      "            \"rationality\": 0.77,\n",
      "            \"informativeness\": 0.635,\n",
      "            \"R1I0\": 0.19,\n",
      "            \"R0I1\": 0.055\n",
      "        }\n",
      "    },\n",
      "    \"human_eval openchat-3.5-0106_explanation\": {\n",
      "        \"hom_pun\": {\n",
      "            \"pun_word\": 0.81,\n",
      "            \"alter_word\": 0.81,\n",
      "            \"pun_sense\": 0.72,\n",
      "            \"alter_sense\": 0.71\n",
      "        },\n",
      "        \"het_pun\": {\n",
      "            \"pun_word\": 0.77,\n",
      "            \"alter_word\": 0.28,\n",
      "            \"pun_sense\": 0.74,\n",
      "            \"alter_sense\": 0.33\n",
      "        },\n",
      "        \"non_pun\": {\n",
      "            \"rationality\": 0.61,\n",
      "            \"informativeness\": 0.46,\n",
      "            \"R1I0\": 0.25,\n",
      "            \"R0I1\": 0.1\n",
      "        }\n",
      "    },\n",
      "    \"human_eval claude-3-opus-20240229_explanation\": {\n",
      "        \"hom_pun\": {\n",
      "            \"pun_word\": 0.96,\n",
      "            \"alter_word\": 0.96,\n",
      "            \"pun_sense\": 0.95,\n",
      "            \"alter_sense\": 0.92\n",
      "        },\n",
      "        \"het_pun\": {\n",
      "            \"pun_word\": 0.95,\n",
      "            \"alter_word\": 0.84,\n",
      "            \"pun_sense\": 0.94,\n",
      "            \"alter_sense\": 0.78\n",
      "        },\n",
      "        \"non_pun\": {\n",
      "            \"rationality\": 0.91,\n",
      "            \"informativeness\": 0.915,\n",
      "            \"R1I0\": 0.055,\n",
      "            \"R0I1\": 0.06\n",
      "        }\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "punchline_check_human = r'./results/human_eval/pun_explanation_punchline_check(human).json'\n",
    "punchline_check_gpt4 = r'./results/human_eval/pun_explanation_punchline_check(gpt-4-1106-preview).json'\n",
    "punchline_check_gpt35 = r'./results/human_eval/pun_explanation_punchline_check(gpt-3.5-turbo-1106).json'\n",
    "punchline_check_gemini = r'./results/human_eval/pun_explanation_punchline_check(gemini-pro).json'\n",
    "punchline_check_claude3 = r'./results/human_eval/pun_explanation_punchline_check(claude-3-opus-20240229).json'\n",
    "punchline_check_vicuna = r'./results/human_eval/pun_explanation_punchline_check(vicuna-7b-v1.5).json'\n",
    "punchline_check_llama2 = r'./results/human_eval/pun_explanation_punchline_check(llama-2-7b-chat).json'\n",
    "punchline_check_mistral = r'./results/human_eval/pun_explanation_punchline_check(mistral-7b-instruct-v0.2).json'\n",
    "punchline_check_openchat = r'./results/human_eval/pun_explanation_punchline_check(openchat-3.5-0106).json'\n",
    "path=r'./results/pun_explanation_punchline_check.json'\n",
    "\n",
    "# punchline_check_integration(punchline_check_human, punchline_check_gpt35, punchline_check_gpt4, punchline_check_gemini,\n",
    "#                             punchline_check_vicuna, punchline_check_llama2, punchline_check_mistral, punchline_check_openchat,\n",
    "#                             punchline_check_claude3, save=True, path=path)\n",
    "punchline_check_summary(evaluations=load_json_file(path), save=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-27T21:48:41.633108Z",
     "end_time": "2024-03-27T21:48:41.682908Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### By comparison"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# Connect gpt-4-0613\n",
    "gpt4_name = 'gpt-4-0613'\n",
    "temperature = 0.0\n",
    "openai_api_key = get_openai_api_key()  # use your api key\n",
    "gpt4 = ChatOpenAI(model_name=gpt4_name, temperature=temperature,\n",
    "                    openai_api_key=openai_api_key, request_timeout=120)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-26T20:18:39.868084Z",
     "end_time": "2024-03-26T20:18:39.929191Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### pilot (small batch)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<?, ?it/s]\n",
      "100%|██████████| 100/100 [00:00<?, ?it/s]\n",
      "100%|██████████| 100/100 [00:00<?, ?it/s]\n",
      "100%|██████████| 100/100 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate by human\n",
    "target1 = 'human'\n",
    "target2 = 'gpt-3.5-turbo-1106'\n",
    "evaluate_explanation_by_comparison(hom_punDataset, pun_explanation, target1, target2, sample_fn=ids_sampling, save=True)\n",
    "evaluate_explanation_by_comparison(het_punDataset, pun_explanation, target1, target2, sample_fn=ids_sampling, save=True)\n",
    "\n",
    "target1 = 'human'\n",
    "target2 = 'gpt-4-1106-preview'\n",
    "evaluate_explanation_by_comparison(hom_punDataset, pun_explanation, target1, target2, sample_fn=ids_sampling, save=True)\n",
    "evaluate_explanation_by_comparison(het_punDataset, pun_explanation, target1, target2, sample_fn=ids_sampling, save=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-11T22:39:02.156446Z",
     "end_time": "2024-03-11T22:39:02.221983Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:19<00:00,  5.21it/s]\n",
      "100%|██████████| 100/100 [00:18<00:00,  5.37it/s]\n",
      "100%|██████████| 100/100 [00:18<00:00,  5.35it/s]\n",
      "100%|██████████| 100/100 [00:18<00:00,  5.38it/s]\n",
      "100%|██████████| 100/100 [00:20<00:00,  4.78it/s]\n",
      "100%|██████████| 100/100 [00:19<00:00,  5.07it/s]\n",
      "100%|██████████| 100/100 [00:18<00:00,  5.41it/s]\n",
      "100%|██████████| 100/100 [00:20<00:00,  4.92it/s]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate by model (gpt4)\n",
    "# In our early experiments, we found that the performances of the model under 0-shot and 3-shot conditions are almost the same, but 0-shot can save about half the tokens.\n",
    "target1 = 'human'\n",
    "target2 = 'gpt-3.5-turbo-1106'\n",
    "evaluate_explanation_by_comparison(hom_punDataset, pun_explanation, target1, target2, sample_fn=ids_sampling,\n",
    "                                   eval_model=gpt4, batch_size=10, save=True)\n",
    "evaluate_explanation_by_comparison(hom_punDataset, pun_explanation, target1, target2, swap=True, sample_fn=ids_sampling,\n",
    "                                   eval_model=gpt4, batch_size=10, save=True)\n",
    "evaluate_explanation_by_comparison(het_punDataset, pun_explanation, target1, target2, sample_fn=ids_sampling,\n",
    "                                   eval_model=gpt4, batch_size=10, save=True)\n",
    "evaluate_explanation_by_comparison(het_punDataset, pun_explanation, target1, target2, swap=True, sample_fn=ids_sampling,\n",
    "                                   eval_model=gpt4, batch_size=10, save=True)\n",
    "\n",
    "target1 = 'human'\n",
    "target2 = 'gpt-4-1106-preview'\n",
    "evaluate_explanation_by_comparison(hom_punDataset, pun_explanation, target1, target2, sample_fn=ids_sampling,\n",
    "                                   eval_model=gpt4, batch_size=10, save=True)\n",
    "evaluate_explanation_by_comparison(hom_punDataset, pun_explanation, target1, target2, swap=True, sample_fn=ids_sampling,\n",
    "                                   eval_model=gpt4, batch_size=10, save=True)\n",
    "evaluate_explanation_by_comparison(het_punDataset, pun_explanation, target1, target2, sample_fn=ids_sampling,\n",
    "                                   eval_model=gpt4, batch_size=10, save=True)\n",
    "evaluate_explanation_by_comparison(het_punDataset, pun_explanation, target1, target2, swap=True, sample_fn=ids_sampling,\n",
    "                                   eval_model=gpt4, batch_size=10, save=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-11T22:44:47.850082Z",
     "end_time": "2024-03-11T22:47:22.450521Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_corr:0.6874723368340706, p_value_pearson:2.925280119706293e-57\n",
      "spearman_corr:0.7179691699112483, p_value_spearman:1.3233771965771166e-64\n",
      "kendall_tau:0.7072809840255064, p_value_kendall:4.6518082528801396e-52\n",
      "{\n",
      "    \"human_vs_gpt-4-1106-preview\": {\n",
      "        \"consistency_with_human\": 0.87,\n",
      "        \"consistency_between_swap\": 0.855\n",
      "    },\n",
      "    \"human_vs_gpt-3.5-turbo-1106\": {\n",
      "        \"consistency_with_human\": 0.895,\n",
      "        \"consistency_between_swap\": 0.825\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Pilot summary and calculate consistency\n",
    "pairwise_comparison_human1 = r'./results/human_eval/pun_explanation_pairwise_comparison(human_vs_gpt-3.5-turbo-1106).json'\n",
    "pairwise_comparison_human2 = r'./results/human_eval/pun_explanation_pairwise_comparison(human_vs_gpt-4-1106-preview).json'\n",
    "pairwise_comparison_gpt4 = r'./results/human_eval/pun_explanation_pairwise_comparison_pilot.json'\n",
    "path=r'./results/pun_explanation_pairwise_comparison_pilot.json'\n",
    "\n",
    "# pairwise_comparison_integration(pairwise_comparison_human1,pairwise_comparison_human2,pairwise_comparison_gpt4,\n",
    "#                                 save=True, path=path)\n",
    "pairwise_comparison_summary(evaluations=load_json_file(path), is_pilot=True, save=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-12T02:00:51.369633Z",
     "end_time": "2024-04-12T02:00:51.401964Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### human vs gpt3.5"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 810/810 [00:00<00:00, 50437.01it/s]\n",
      "100%|██████████| 647/647 [00:00<00:00, 40531.03it/s]\n"
     ]
    }
   ],
   "source": [
    "target1 = 'human'\n",
    "target2 = 'gpt-3.5-turbo-1106'\n",
    "evaluate_explanation_by_comparison(hom_punDataset, pun_explanation, target1, target2,\n",
    "                                   eval_model=gpt4, batch_size=10, save=True)\n",
    "evaluate_explanation_by_comparison(het_punDataset, pun_explanation, target1, target2,\n",
    "                                   eval_model=gpt4, batch_size=10, save=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-13T16:22:50.053907Z",
     "end_time": "2024-03-13T16:22:50.180127Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### human vs gpt4"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 810/810 [00:00<00:00, 51356.50it/s]\n",
      "100%|██████████| 647/647 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "target1 = 'human'\n",
    "target2 = 'gpt-4-1106-preview'\n",
    "evaluate_explanation_by_comparison(hom_punDataset, pun_explanation, target1, target2,\n",
    "                                   eval_model=gpt4, batch_size=10, save=True)\n",
    "evaluate_explanation_by_comparison(het_punDataset, pun_explanation, target1, target2,\n",
    "                                   eval_model=gpt4, batch_size=10, save=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-13T16:22:52.488594Z",
     "end_time": "2024-03-13T16:22:52.601146Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### human vs gemini-pro"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 810/810 [00:00<00:00, 51897.02it/s]\n",
      "100%|██████████| 647/647 [00:00<00:00, 46334.43it/s]\n"
     ]
    }
   ],
   "source": [
    "target1 = 'human'\n",
    "target2 = 'gemini-pro'\n",
    "evaluate_explanation_by_comparison(hom_punDataset, pun_explanation, target1, target2,\n",
    "                                   eval_model=gpt4, batch_size=10, save=True)\n",
    "evaluate_explanation_by_comparison(het_punDataset, pun_explanation, target1, target2,\n",
    "                                   eval_model=gpt4, batch_size=10, save=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-13T16:22:55.053687Z",
     "end_time": "2024-03-13T16:22:55.164139Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### human vs claude3"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 810/810 [02:44<00:00,  4.93it/s]\n",
      "100%|██████████| 647/647 [02:05<00:00,  5.16it/s]\n"
     ]
    }
   ],
   "source": [
    "target1 = 'human'\n",
    "target2 = 'claude-3-opus-20240229'\n",
    "evaluate_explanation_by_comparison(hom_punDataset, pun_explanation, target1, target2,\n",
    "                                   eval_model=gpt4, batch_size=10, save=True)\n",
    "evaluate_explanation_by_comparison(het_punDataset, pun_explanation, target1, target2,\n",
    "                                   eval_model=gpt4, batch_size=10, save=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-26T20:19:22.642020Z",
     "end_time": "2024-03-26T20:24:12.563551Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### human vs vicuna"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 810/810 [00:00<00:00, 90092.45it/s]\n",
      "100%|██████████| 647/647 [00:00<00:00, 41036.06it/s]\n"
     ]
    }
   ],
   "source": [
    "target1 = 'human'\n",
    "target2 = 'vicuna-7b-v1.5'\n",
    "evaluate_explanation_by_comparison(hom_punDataset, pun_explanation, target1, target2,\n",
    "                                   eval_model=gpt4, batch_size=10, save=True)\n",
    "evaluate_explanation_by_comparison(het_punDataset, pun_explanation, target1, target2,\n",
    "                                   eval_model=gpt4, batch_size=10, save=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-13T16:22:57.492094Z",
     "end_time": "2024-03-13T16:22:57.756364Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### human vs llama2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 810/810 [00:00<00:00, 51750.77it/s]\n",
      "100%|██████████| 647/647 [00:00<00:00, 85806.45it/s]\n"
     ]
    }
   ],
   "source": [
    "target1 = 'human'\n",
    "target2 = 'llama-2-7b-chat'\n",
    "evaluate_explanation_by_comparison(hom_punDataset, pun_explanation, target1, target2,\n",
    "                                   eval_model=gpt4, batch_size=10, save=True)\n",
    "evaluate_explanation_by_comparison(het_punDataset, pun_explanation, target1, target2,\n",
    "                                   eval_model=gpt4, batch_size=10, save=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-13T16:23:00.060512Z",
     "end_time": "2024-03-13T16:23:00.154656Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### human vs mistral"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 810/810 [00:00<00:00, 296016.92it/s]\n",
      "100%|██████████| 647/647 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "target1 = 'human'\n",
    "target2 = 'mistral-7b-instruct-v0.2'\n",
    "evaluate_explanation_by_comparison(hom_punDataset, pun_explanation, target1, target2,\n",
    "                                   eval_model=gpt4, batch_size=10, save=True)\n",
    "evaluate_explanation_by_comparison(het_punDataset, pun_explanation, target1, target2,\n",
    "                                   eval_model=gpt4, batch_size=10, save=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-13T16:23:02.302726Z",
     "end_time": "2024-03-13T16:23:02.445412Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### human vs openchat"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 810/810 [00:00<00:00, 39236.22it/s]\n",
      "100%|██████████| 647/647 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "target1 = 'human'\n",
    "target2 = 'openchat-3.5-0106'\n",
    "evaluate_explanation_by_comparison(hom_punDataset, pun_explanation, target1, target2,\n",
    "                                   eval_model=gpt4, batch_size=10, save=True)\n",
    "evaluate_explanation_by_comparison(het_punDataset, pun_explanation, target1, target2,\n",
    "                                   eval_model=gpt4, batch_size=10, save=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-13T16:23:04.573649Z",
     "end_time": "2024-03-13T16:23:04.737489Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### *summary"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"gpt-4-0613_eval human_vs_gpt-3.5-turbo-1106\": {\n",
      "        \"hom\": {\n",
      "            \"tie\": 0.6679,\n",
      "            \"human_win\": 0.1716,\n",
      "            \"model_win\": 0.1605\n",
      "        },\n",
      "        \"het\": {\n",
      "            \"tie\": 0.4451,\n",
      "            \"human_win\": 0.442,\n",
      "            \"model_win\": 0.1128\n",
      "        }\n",
      "    },\n",
      "    \"gpt-4-0613_eval human_vs_gpt-4-1106-preview\": {\n",
      "        \"hom\": {\n",
      "            \"tie\": 0.7691,\n",
      "            \"human_win\": 0.0173,\n",
      "            \"model_win\": 0.2136\n",
      "        },\n",
      "        \"het\": {\n",
      "            \"tie\": 0.6847,\n",
      "            \"human_win\": 0.0866,\n",
      "            \"model_win\": 0.2287\n",
      "        }\n",
      "    },\n",
      "    \"gpt-4-0613_eval human_vs_gemini-pro\": {\n",
      "        \"hom\": {\n",
      "            \"tie\": 0.6531,\n",
      "            \"human_win\": 0.1654,\n",
      "            \"model_win\": 0.1815\n",
      "        },\n",
      "        \"het\": {\n",
      "            \"tie\": 0.4173,\n",
      "            \"human_win\": 0.4606,\n",
      "            \"model_win\": 0.1221\n",
      "        }\n",
      "    },\n",
      "    \"gpt-4-0613_eval human_vs_vicuna-7b-v1.5\": {\n",
      "        \"hom\": {\n",
      "            \"tie\": 0.3506,\n",
      "            \"human_win\": 0.5679,\n",
      "            \"model_win\": 0.0815\n",
      "        },\n",
      "        \"het\": {\n",
      "            \"tie\": 0.17,\n",
      "            \"human_win\": 0.779,\n",
      "            \"model_win\": 0.051\n",
      "        }\n",
      "    },\n",
      "    \"gpt-4-0613_eval human_vs_llama-2-7b-chat\": {\n",
      "        \"hom\": {\n",
      "            \"tie\": 0.3074,\n",
      "            \"human_win\": 0.5852,\n",
      "            \"model_win\": 0.1074\n",
      "        },\n",
      "        \"het\": {\n",
      "            \"tie\": 0.0958,\n",
      "            \"human_win\": 0.8408,\n",
      "            \"model_win\": 0.0634\n",
      "        }\n",
      "    },\n",
      "    \"gpt-4-0613_eval human_vs_mistral-7b-instruct-v0.2\": {\n",
      "        \"hom\": {\n",
      "            \"tie\": 0.3802,\n",
      "            \"human_win\": 0.5235,\n",
      "            \"model_win\": 0.0963\n",
      "        },\n",
      "        \"het\": {\n",
      "            \"tie\": 0.187,\n",
      "            \"human_win\": 0.7589,\n",
      "            \"model_win\": 0.0541\n",
      "        }\n",
      "    },\n",
      "    \"gpt-4-0613_eval human_vs_openchat-3.5-0106\": {\n",
      "        \"hom\": {\n",
      "            \"tie\": 0.5049,\n",
      "            \"human_win\": 0.3383,\n",
      "            \"model_win\": 0.1568\n",
      "        },\n",
      "        \"het\": {\n",
      "            \"tie\": 0.2396,\n",
      "            \"human_win\": 0.6893,\n",
      "            \"model_win\": 0.0711\n",
      "        }\n",
      "    },\n",
      "    \"gpt-4-0613_eval human_vs_claude-3-opus-20240229\": {\n",
      "        \"hom\": {\n",
      "            \"tie\": 0.7333,\n",
      "            \"human_win\": 0.0407,\n",
      "            \"model_win\": 0.2259\n",
      "        },\n",
      "        \"het\": {\n",
      "            \"tie\": 0.6445,\n",
      "            \"human_win\": 0.1206,\n",
      "            \"model_win\": 0.2349\n",
      "        }\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "path = r'./results/pun_explanation_pairwise_comparison.json'\n",
    "pairwise_comparison_summary(load_json_file(path), save=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-26T20:29:49.552303Z",
     "end_time": "2024-03-26T20:29:49.587936Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
