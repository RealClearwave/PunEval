{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Generation Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Load Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-27T15:16:20.562386Z",
     "start_time": "2024-03-27T15:16:20.462214Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from openai import OpenAI\n",
    "from _api_key import get_deepseek_api_key\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Basic Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-27T15:16:20.649565Z",
     "start_time": "2024-03-27T15:16:20.477229Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_json_file(file_path):\n",
    "    \"\"\"\n",
    "    Load json file\n",
    "    \"\"\"\n",
    "    with open(file_path,'r',encoding='utf-8') as f:\n",
    "        file = json.load(f)\n",
    "        f.close()\n",
    "    return file\n",
    "\n",
    "def save_json_file(file, file_path, sort_keys:bool=False):\n",
    "    \"\"\"\n",
    "    Save json file\n",
    "    \"\"\"\n",
    "    with open(file_path,'w',encoding='utf-8') as f:\n",
    "        json.dump(file, f, indent=4, ensure_ascii=False, sort_keys=sort_keys)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-27T15:16:20.649565Z",
     "start_time": "2024-03-27T15:16:20.492871Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def splitDataset(file, puntype='hom'):\n",
    "    \"\"\"\n",
    "    Enter path or json file to separate the pun part from the non-pun part of the dataset\n",
    "    \"\"\"\n",
    "    if isinstance(file,str):\n",
    "        dataset = load_json_file(file)\n",
    "    else:\n",
    "        dataset = file\n",
    "    punDataset = dict()\n",
    "    nonpunDataset = dict()\n",
    "    for ID in dataset:\n",
    "        data = dataset[ID]\n",
    "        if puntype in ID:\n",
    "            if data.get('pun_word', False):\n",
    "                punDataset[ID] = data\n",
    "            else:\n",
    "                nonpunDataset[ID] = data\n",
    "    return punDataset, nonpunDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-27T15:16:20.649565Z",
     "start_time": "2024-03-27T15:16:20.508491Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def add_human_pun(pun_dataset, save:bool=False, path_gen='pun_generation.json'):\n",
    "    \"\"\"\n",
    "    Add human-written puns\n",
    "    \"\"\"\n",
    "    path_gen = './results/' + path_gen\n",
    "    if os.path.exists(path_gen):\n",
    "        pun_generation = load_json_file(path_gen)\n",
    "    else:\n",
    "        pun_generation = dict()\n",
    "    for ID in pun_dataset:\n",
    "        data = pun_dataset[ID]\n",
    "        human_text = data['human_text']\n",
    "        if ID not in pun_generation:\n",
    "            pun_generation[ID] = {'human_text':human_text}\n",
    "        else:\n",
    "            pun_generation[ID].update({'human_text':human_text})\n",
    "    if save:\n",
    "        save_json_file(pun_generation, path_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Function of Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-27T15:16:20.649565Z",
     "start_time": "2024-03-27T15:16:20.524152Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def call_llm_to_generate(model, dataset, model_name:str=None, method:int=0, examples:dict=None,\n",
    "                         save:bool=False, path_gen='pun_generation.json', batch_size:int=1):\n",
    "    \"\"\"\n",
    "    Try generating puns with llm  \\n\n",
    "    method=0: Generate non-pun sentence with only a single layer of meaning \\n\n",
    "    method=1: Given pun pair, generate pun directly \\n\n",
    "    method=2: Given specific contextual word set, generate pun directly  \\n\n",
    "    \"\"\"\n",
    "    def parse_output(ID, output:str):\n",
    "        # Parse output and get the result\n",
    "        try:\n",
    "            output = output[output.index('{'): output.index('}')+1]\n",
    "        except:\n",
    "            output = output\n",
    "        try:\n",
    "            output = eval(output)\n",
    "            sentence = output['Sentence']\n",
    "        except:\n",
    "            # print(ID, output)\n",
    "            try:\n",
    "                sentence = output.split('Sentence')[-1]\n",
    "            except:\n",
    "                sentence = 'No correctly parsed result.'\n",
    "        return sentence\n",
    "\n",
    "    def get_punchline_elements(data:dict):\n",
    "        # Get pun word/sense, alternative word/sense, etc. from data\n",
    "        pun_word = data['pun_word']\n",
    "        pun_sense = data['pun_sense']\n",
    "        alter_word = data['alter_word']\n",
    "        alter_sense = data['alter_sense']\n",
    "        context_words = data['human_keywords']\n",
    "        return pun_word, pun_sense, alter_word, alter_sense, context_words\n",
    "\n",
    "    def extend_dict(given_dict, keys):\n",
    "        # Create a null value based on the keys\n",
    "        temp = given_dict\n",
    "        for key in keys:\n",
    "            if key not in temp:\n",
    "                temp[key] = dict()\n",
    "            temp = temp[key]\n",
    "        return given_dict\n",
    "\n",
    "    assert method in [0, 1, 2]\n",
    "    path_gen = './results/' + path_gen\n",
    "    if os.path.exists(path_gen):\n",
    "        record_gen = load_json_file(path_gen)\n",
    "    else:\n",
    "        record_gen = dict()\n",
    "    # [A]. Construct the prompt\n",
    "    definition = \"\"\"<*Definition*>\\nPuns are a form of wordplay exploiting different meanings of a word or similar-sounding words, while non-puns are jokes or statements that don't rely on such linguistic ambiguities.\\n\\n\"\"\"\n",
    "    # Method=0: Generate non-pun that only expresses one sense\n",
    "    if method == 0:\n",
    "        instruction = \"\"\"<*Instruction*>\\nBelow is a keyword and one of its meanings. Please generate a non-pun sentence with the keyword that conveys the given meaning. You must output the current status in a parsable JSON format. An example output looks like:\\n{{\"Sentence\": \"XXX\"}}\"\"\"\n",
    "        examples_string = \"\"\"\"\"\"\n",
    "        testing = \"\"\"\\n\\n<*Your Response*>\\nKeyword: {punchline}\\nMeaning: {pun_word} <{pun_sense}>\\nOutput:\"\"\"\n",
    "    # Method=1: Generate pun directly\n",
    "    elif method == 1:\n",
    "        instruction = \"\"\"<*Instruction*>\\nBelow is a keyword and two of its meanings. Please generate a pun sentence with punchline on the keyword that conveys both given meanings simultaneously. Except for the keyword, the pun sentence must not utilize any words from either of the two meanings. Besides, once a keyword is used, it's strictly prohibited to use it again in the latter half of the sentence.\\nYou must output the current status in a parsable JSON format. An example output looks like:\\n{{\"Sentence\": \"XXX\"}}\"\"\"\n",
    "        if examples is not None:\n",
    "            examples_temp = []\n",
    "            for ID in examples:\n",
    "                example = examples[ID]\n",
    "                pun_word, pun_sense, alter_word, alter_sense, context_words = get_punchline_elements(data=example)\n",
    "                punchline, text = example['punchline'], example['human_text']\n",
    "                examples_temp.append(f\"Keyword: {punchline}\\n\"\n",
    "                                     f\"Meaning 1: {pun_word} <{pun_sense}>\\n\"\n",
    "                                     f\"Meaning 2: {alter_word} <{alter_sense}>\\n\"\n",
    "                                     f\"Output:\\n{{{{\\\"Sentence\\\": \\\"{text}\\\"}}}}\")\n",
    "            examples_string = '\\n\\n<*Examples*>\\n' + '\\n\\n'.join(examples_temp)\n",
    "        else:\n",
    "            examples_string = \"\"\"\"\"\"\n",
    "        testing = \"\"\"\\n\\n<*Your Response*>\\nKeyword: {punchline}\\nMeaning 1: {pun_word} <{pun_sense}>\\nMeaning 2: {alter_word} <{alter_sense}>\\nOutput:\"\"\"\n",
    "    # Method=2: Generate puns directly based on specific contextual words\n",
    "    else:\n",
    "        instruction = \"\"\"<*Instruction*>\\nBelow is a keyword, two of its meanings and a set of contextual words. Please generate a pun sentence with punchline on the keyword that conveys both given meanings simultaneously and using all the contextual words. Except for the keyword, the pun sentence must not utilize any words from either of the two meanings. Besides, once a keyword is used, it's strictly prohibited to use it again in the latter half of the sentence.\\nYou must output the current status in a parsable JSON format. An example output looks like:\\n{{\"Sentence\": \"XXX\"}}\"\"\"\n",
    "        if examples is not None:\n",
    "            examples_temp = []\n",
    "            for ID in examples:\n",
    "                example = examples[ID]\n",
    "                pun_word, pun_sense, alter_word, alter_sense, context_words = get_punchline_elements(data=example)\n",
    "                punchline, text = example['punchline'], example['human_text']\n",
    "                examples_temp.append(f\"Keyword: {punchline}\\n\"\n",
    "                                     f\"Meaning 1: {pun_word} <{pun_sense}>\\n\"\n",
    "                                     f\"Meaning 2: {alter_word} <{alter_sense}>\\n\"\n",
    "                                     f\"Contextual Words: {', '.join(context_words)}.\\n\"\n",
    "                                     f\"Output:\\n{{{{\\\"Sentence\\\": \\\"{text}\\\"}}}}\")\n",
    "            examples_string = '\\n\\n<*Examples*>\\n' + '\\n\\n'.join(examples_temp)\n",
    "        else:\n",
    "            examples_string = \"\"\"\"\"\"\n",
    "        testing = \"\"\"\\n\\n<*Your Response*>\\nKeyword: {punchline}\\nMeaning 1: {pun_word} <{pun_sense}>\\nMeaning 2: {alter_word} <{alter_sense}>\\nContextual Words: {context_words}.\\nOutput:\"\"\"\n",
    "    # Combine all parts together\n",
    "    prompt_string = definition + instruction + examples_string + testing\n",
    "\n",
    "    # [B]. Call llm to generate\n",
    "    key_gen = f\"{model_name}_text\"\n",
    "    IDs = list(dataset.keys())\n",
    "    IDs_loaded = []\n",
    "    for ID in record_gen:\n",
    "        if record_gen[ID].get(key_gen, False) and \\\n",
    "           record_gen[ID][key_gen].get(f'method {method}', False):\n",
    "            IDs_loaded.append(ID)\n",
    "    all_ind = list(range(0,len(IDs)))\n",
    "    batch_ind = list(range(0,len(IDs),batch_size))\n",
    "    for ind in tqdm(all_ind):\n",
    "        if ind not in batch_ind:\n",
    "            continue\n",
    "        IDs_batch = IDs[ind: ind+batch_size]\n",
    "        # Remove the data that has already been run\n",
    "        IDs_batch = list(set(IDs_batch)-set(IDs_loaded))\n",
    "        if len(IDs_batch) == 0:\n",
    "            continue\n",
    "        _inputs = []\n",
    "        _human_text = []\n",
    "        for ID in IDs_batch:\n",
    "            data = dataset[ID]\n",
    "            pun_ind = int(data['pun_word_ind'].split('_')[-1]) - 1\n",
    "            punchline = data['human_text'].split(' ')[pun_ind]\n",
    "            pun_word, pun_sense, alter_word, alter_sense, context_words = get_punchline_elements(data=data)\n",
    "            _inputs.append(prompt_string.format(pun_word=pun_word, pun_sense=pun_sense,\n",
    "                                                alter_word=alter_word, alter_sense=alter_sense,\n",
    "                                                punchline=punchline, context_words=', '.join(context_words)))\n",
    "            _human_text.append(data['human_text'])\n",
    "\n",
    "        #_outputs = [out.content for out in model.batch(_inputs)]\n",
    "        _outputs = []\n",
    "        for i in range(len(_inputs)):\n",
    "            print(f'case {ind}, {i}:')\n",
    "            print(f'human text: {_human_text[i]}')\n",
    "            if 'deepseek' in model_name:\n",
    "                response = model.chat.completions.create(\n",
    "                model=\"deepseek-chat\",\n",
    "                    messages=[\n",
    "                        {\"role\": \"user\", \"content\": _inputs[i]},\n",
    "                    ],\n",
    "                    stream=False\n",
    "                )\n",
    "                \n",
    "                #decode response JSON\n",
    "                output = (json.loads(response.choices[0].message.content))['Sentence']\n",
    "                print(f'LLM output: {output}')\n",
    "                _outputs.append(output)\n",
    "            elif 'qwen2' in model_name:\n",
    "                url = \"http://localhost:1234/v1/chat/completions\"\n",
    "                headers = {\"Content-Type\": \"application/json\"}\n",
    "                payload = {\n",
    "                    \"model\": model_name,\n",
    "                    \"messages\": [\n",
    "                        {\"role\": \"user\", \"content\": _inputs[i]},\n",
    "                    ],\n",
    "                    \"temperature\": 0.7,\n",
    "                    \"max_tokens\": -1,\n",
    "                    \"stream\": False\n",
    "                }\n",
    "\n",
    "                response = requests.post(url, headers=headers, data=json.dumps(payload))\n",
    "                response = response.json()\n",
    "                try:\n",
    "                    output = json.loads(response['choices'][0]['message']['content'])['Sentence']\n",
    "                except (KeyError, json.JSONDecodeError) as e:\n",
    "                    print(f\"Error parsing response: {e}\")\n",
    "                    output = \"Error: Unable to parse response : {e}\"\n",
    "                print(f'LLM output: {output}')\n",
    "                _outputs.append(output)\n",
    "        \n",
    "        # print(_inputs[0][0].content)\n",
    "        # print(_outputs[0])\n",
    "        # break\n",
    "        for ID,out in zip(IDs_batch, _outputs):\n",
    "            sentence = parse_output(ID, out)\n",
    "            record_gen = extend_dict(record_gen, [ID, key_gen])\n",
    "            record_gen[ID][key_gen].update({f'method {method}': sentence})\n",
    "        if save:\n",
    "            save_json_file(record_gen, path_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Dataset and Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-27T15:16:20.649565Z",
     "start_time": "2024-03-27T15:16:20.539744Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hom_path = r'./dataset/hom_dataset.json'\n",
    "het_path = r'./dataset/het_dataset.json'\n",
    "hom_punDataset, hom_nonpunDataset = splitDataset(hom_path)\n",
    "het_punDataset, het_nonpunDataset = splitDataset(het_path, puntype='het')\n",
    "\n",
    "# add_human_pun(dict(**hom_punDataset, **het_punDataset), save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-27T15:16:20.662582Z",
     "start_time": "2024-03-27T15:16:20.555373Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Choose data from examples manually\n",
    "hom_examples = {\n",
    "    \"hom_705\":{\n",
    "        \"pun_word\": \"toll\",\n",
    "        \"pun_sense\": \"a fee levied for the use of roads or bridges (used for maintenance)\",\n",
    "        \"alter_word\": \"toll\",\n",
    "        \"alter_sense\": \"value measured by what must be given or done or undergone to obtain something\",\n",
    "        \"punchline\": \"toll\",\n",
    "        \"human_text\":\"Driving on so many turnpikes was taking its toll.\",\n",
    "        \"human_keywords\": [\"Driving\", \"many\", \"turnpikes\", \"taking its toll\"],\n",
    "    },\n",
    "    \"hom_488\":{\n",
    "        \"pun_word\": \"bore\",\n",
    "        \"pun_sense\": \"make a hole, especially with a pointed power or hand tool\",\n",
    "        \"alter_word\": \"bore\",\n",
    "        \"alter_sense\": \"cause to be bored\",\n",
    "        \"punchline\": \"bored\",\n",
    "        \"human_text\":\"A carpenter sat on his drill and was bored to tears.\",\n",
    "        \"human_keywords\": [\"carpenter\", \"sat\", \"drill\", \"bored to tears\"],\n",
    "    },\n",
    "    \"hom_1556\":{\n",
    "        \"pun_word\": \"foil\",\n",
    "        \"pun_sense\": \"a piece of thin and flexible sheet metal\",\n",
    "        \"alter_word\": \"foil\",\n",
    "        \"alter_sense\": \"hinder or prevent (the efforts, plans, or desires) of\",\n",
    "        \"punchline\": \"foiled\",\n",
    "        \"human_text\":\"One leftover said to another 'foiled again.'\",\n",
    "        \"human_keywords\": [\"leftover\", \"foiled\", \"again\"],\n",
    "    }\n",
    "}\n",
    "\n",
    "het_examples = {\n",
    "    \"het_633\": {\n",
    "        \"pun_word\": \"sagely\",\n",
    "        \"pun_sense\": \"in a wise manner\",\n",
    "        \"alter_word\": \"sage\",\n",
    "        \"alter_sense\": \"aromatic fresh or dried grey-green leaves used widely as seasoning for meats and fowl and game etc\",\n",
    "        \"punchline\": \"sagely\",\n",
    "        \"human_text\": \"This fowl has been stuffed, said Tom sagely.\",\n",
    "        \"human_keywords\": [\"fowl\", \"stuffed\", \"sagely\"],\n",
    "    },\n",
    "    \"het_530\": {\n",
    "        \"pun_word\": \"toll\",\n",
    "        \"pun_sense\": \"ring slowly\",\n",
    "        \"alter_word\": \"tell off\",\n",
    "        \"alter_sense\": \"reprimand\",\n",
    "        \"punchline\": \"tolled\",\n",
    "        \"human_text\": \"A tangled bell ringer tolled himself off.\",\n",
    "        \"human_keywords\": [\"tangled\", \"bell ringer\", \"tolled himself off\"],\n",
    "    },\n",
    "    \"het_325\": {\n",
    "        \"pun_word\": \"c\",\n",
    "        \"pun_sense\": \"the 3rd letter of the Roman alphabet\",\n",
    "        \"alter_word\": \"sea\",\n",
    "        \"alter_sense\": \"a division of an ocean or a large body of salt water partially enclosed by land\",\n",
    "        \"punchline\": \"c\",\n",
    "        \"human_text\": \"An illiterate fisherman was lost at c.\",\n",
    "        \"human_keywords\": [\"illiterate\", \"fisherman\", \"c\"],\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DeepSeek-v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect DeepSeek-v3\n",
    "deepseek_name = 'deepseek-chat'\n",
    "temperature = 0.7\n",
    "deepseek_api_key = get_deepseek_api_key()  # use your api key\n",
    "deepseek = OpenAI(api_key=deepseek_api_key, base_url=\"https://api.deepseek.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/810 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "case 0, 0:\n",
      "human text: Getting this job managing a country estate has put me off fried eggs . I ' m a gamekeeper turned poacher .\n",
      "LLM output: The gamekeeper caught the fried eggs poacher.\n",
      "case 0, 1:\n",
      "human text: The intelligent entrepreneur ' s idea for designing catapults meant that his boss was completely thrown .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/810 [00:06<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[95], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Generation method 2\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mcall_llm_to_generate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeepseek\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeepseek_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhom_punDataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexamples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhom_examples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m call_llm_to_generate(model\u001b[38;5;241m=\u001b[39mdeepseek, model_name\u001b[38;5;241m=\u001b[39mdeepseek_name, dataset\u001b[38;5;241m=\u001b[39mhet_punDataset, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, examples\u001b[38;5;241m=\u001b[39mhet_examples, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, save\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[94], line 131\u001b[0m, in \u001b[0;36mcall_llm_to_generate\u001b[0;34m(model, dataset, model_name, method, examples, save, path_gen, batch_size)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhuman text: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_human_text[i]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdeepseek\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m model_name:\n\u001b[0;32m--> 131\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdeepseek-chat\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m_inputs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m    137\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;66;03m#decode response JSON\u001b[39;00m\n\u001b[1;32m    140\u001b[0m     output \u001b[38;5;241m=\u001b[39m (json\u001b[38;5;241m.\u001b[39mloads(response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent))[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSentence\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/openai/_utils/_utils.py:287\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    285\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 287\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/openai/resources/chat/completions/completions.py:925\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    882\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    883\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    884\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    922\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    923\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m    924\u001b[0m     validate_response_format(response_format)\n\u001b[0;32m--> 925\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    929\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    930\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    931\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maudio\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    932\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    933\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    934\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    935\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    936\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    937\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_completion_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    938\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    939\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    940\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodalities\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    941\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    942\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprediction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreasoning_effort\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    947\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    948\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice_tier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    949\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    950\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    951\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    952\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    953\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    954\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    955\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    959\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweb_search_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[1;32m    963\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    965\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    967\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    968\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    970\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/openai/_base_client.py:1239\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1225\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1226\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1227\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1234\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1235\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1236\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1237\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1238\u001b[0m     )\n\u001b[0;32m-> 1239\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/openai/_base_client.py:969\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m    967\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    968\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 969\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    970\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_should_stream_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    973\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    974\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mTimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    975\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncountered httpx.TimeoutException\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/httpx/_client.py:928\u001b[0m, in \u001b[0;36mClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    927\u001b[0m     response\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m--> 928\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/httpx/_client.py:922\u001b[0m, in \u001b[0;36mClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    920\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    921\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n\u001b[0;32m--> 922\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    924\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[1;32m    926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/httpx/_models.py:881\u001b[0m, in \u001b[0;36mResponse.read\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    878\u001b[0m \u001b[38;5;124;03mRead and return the response content.\u001b[39;00m\n\u001b[1;32m    879\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    880\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_content\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 881\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    882\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/httpx/_models.py:897\u001b[0m, in \u001b[0;36mResponse.iter_bytes\u001b[0;34m(self, chunk_size)\u001b[0m\n\u001b[1;32m    895\u001b[0m chunker \u001b[38;5;241m=\u001b[39m ByteChunker(chunk_size\u001b[38;5;241m=\u001b[39mchunk_size)\n\u001b[1;32m    896\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request):\n\u001b[0;32m--> 897\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m raw_bytes \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_raw():\n\u001b[1;32m    898\u001b[0m         decoded \u001b[38;5;241m=\u001b[39m decoder\u001b[38;5;241m.\u001b[39mdecode(raw_bytes)\n\u001b[1;32m    899\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunker\u001b[38;5;241m.\u001b[39mdecode(decoded):\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/httpx/_models.py:951\u001b[0m, in \u001b[0;36mResponse.iter_raw\u001b[0;34m(self, chunk_size)\u001b[0m\n\u001b[1;32m    948\u001b[0m chunker \u001b[38;5;241m=\u001b[39m ByteChunker(chunk_size\u001b[38;5;241m=\u001b[39mchunk_size)\n\u001b[1;32m    950\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request):\n\u001b[0;32m--> 951\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m raw_stream_bytes \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream:\n\u001b[1;32m    952\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_bytes_downloaded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(raw_stream_bytes)\n\u001b[1;32m    953\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunker\u001b[38;5;241m.\u001b[39mdecode(raw_stream_bytes):\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/httpx/_client.py:153\u001b[0m, in \u001b[0;36mBoundSyncStream.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m typing\u001b[38;5;241m.\u001b[39mIterator[\u001b[38;5;28mbytes\u001b[39m]:\n\u001b[0;32m--> 153\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stream:\n\u001b[1;32m    154\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m chunk\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/httpx/_transports/default.py:127\u001b[0m, in \u001b[0;36mResponseStream.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m typing\u001b[38;5;241m.\u001b[39mIterator[\u001b[38;5;28mbytes\u001b[39m]:\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 127\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m part \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_httpcore_stream:\n\u001b[1;32m    128\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m part\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/httpcore/_sync/connection_pool.py:407\u001b[0m, in \u001b[0;36mPoolByteStream.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    406\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m--> 407\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/httpcore/_sync/connection_pool.py:403\u001b[0m, in \u001b[0;36mPoolByteStream.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m typing\u001b[38;5;241m.\u001b[39mIterator[\u001b[38;5;28mbytes\u001b[39m]:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 403\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m part \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stream:\n\u001b[1;32m    404\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m part\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/httpcore/_sync/http11.py:342\u001b[0m, in \u001b[0;36mHTTP11ConnectionByteStream.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ShieldCancellation():\n\u001b[1;32m    341\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m--> 342\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/httpcore/_sync/http11.py:334\u001b[0m, in \u001b[0;36mHTTP11ConnectionByteStream.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_body\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request, kwargs):\n\u001b[0;32m--> 334\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39m_receive_response_body(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    335\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m chunk\n\u001b[1;32m    336\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    337\u001b[0m     \u001b[38;5;66;03m# If we get an exception while streaming the response,\u001b[39;00m\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;66;03m# we want to close the response (and possibly the connection)\u001b[39;00m\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;66;03m# before raising that exception.\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/httpcore/_sync/http11.py:203\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_body\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    200\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeouts\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 203\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mData):\n\u001b[1;32m    205\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mbytes\u001b[39m(event\u001b[38;5;241m.\u001b[39mdata)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/httpcore/_sync/http11.py:217\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    214\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mnext_event()\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11\u001b[38;5;241m.\u001b[39mNEED_DATA:\n\u001b[0;32m--> 217\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_network_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mtheir_state \u001b[38;5;241m==\u001b[39m h11\u001b[38;5;241m.\u001b[39mSEND_RESPONSE:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/httpcore/_backends/sync.py:128\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n\u001b[0;32m--> 128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/ssl.py:1226\u001b[0m, in \u001b[0;36mSSLSocket.recv\u001b[0;34m(self, buflen, flags)\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1223\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1224\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1225\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1226\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1227\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1228\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv(buflen, flags)\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/ssl.py:1101\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1099\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[1;32m   1100\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1101\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1102\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[1;32m   1103\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msuppress_ragged_eofs:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Generation method 2\n",
    "call_llm_to_generate(model=deepseek, model_name=deepseek_name, dataset=hom_punDataset, method=2, examples=hom_examples, batch_size=10, save=True)\n",
    "call_llm_to_generate(model=deepseek, model_name=deepseek_name, dataset=het_punDataset, method=2, examples=het_examples, batch_size=10, save=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qwen2-vl-2b-instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "qwen2vl = 'qwen2-vl-2b-instruct'\n",
    "qwen2vl_name = 'qwen2-vl-2b-instruct'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/810 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "case 170, 0:\n",
      "human text: Before he sold Christmas trees , he got himself spruced up .\n",
      "LLM output: Before Christmas, I bought some spruced up Christmas trees and they looked great.\n",
      "case 170, 1:\n",
      "human text: The exhibitionist went to the store because he heard they were having a flash sale .\n",
      "LLM output: An exhibitionist flashed his sale on the street.\n",
      "case 170, 2:\n",
      "human text: Power corrupts , especially at the electric company .\n",
      "LLM output: A corrupt official used his power to corrupt.\n",
      "case 170, 3:\n",
      "human text: OLD PSYCHIATRISTS never die , they just shrink away .\n",
      "LLM output: A psychiatrist said to another 'shrink again.'\n",
      "case 170, 4:\n",
      "human text: The two guys caught drinking battery acid will soon be charged .\n",
      "LLM output: I caught him drinking and will be charged.\n",
      "case 170, 5:\n",
      "human text: Conjoined twins aren ' t the only twins with special connections .\n",
      "LLM output: Conjoined twins are a connection.\n",
      "case 170, 6:\n",
      "human text: Her decision to take up rock climbing was precipitous to say the least .\n",
      "LLM output: The climber fell down the precipitous rock.\n",
      "case 170, 7:\n",
      "human text: If pride goes before a fall , humility should arrive by that winter\n",
      "LLM output: The winter had fallen and I was feeling proud of my humility.\n",
      "case 170, 8:\n",
      "human text: He who lends to the poor gets interest from god\n",
      "LLM output: The lends are taking interest in our financial problems.\n",
      "case 170, 9:\n",
      "human text: After they had it out in the boxing ring , they called it square .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|        | 171/810 [00:10<00:38, 16.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM output: Boxer Jake LaMotta was a true square.\n",
      "case 180, 0:\n",
      "human text: Doctor , doctor , I keep thinking I ' m a spoon . - Sit there and don ' t stir . Next .\n",
      "LLM output: I sat on the table and stirred my coffee.\n",
      "case 180, 1:\n",
      "human text: My friend was fired from his job at the road department for stealing . I have to say I saw it coming . The last time I was at his house all the signs were there .\n",
      "LLM output: Fired from the road department and stolen signs were signs of a less than perfect situation.\n",
      "case 180, 2:\n",
      "human text: He had a difficult time bouncing back from his bungee cord accident .\n",
      "LLM output: He had a hard time bouncing back but eventually bounced back with flying colors.\n",
      "case 180, 3:\n",
      "human text: A wig van blew up on the motorway - police are combing the area .\n",
      "LLM output: The police combed the area for any signs of foul play.\n",
      "case 180, 4:\n",
      "human text: I made a batch of fish eye soup , it should see me through the week .\n",
      "LLM output: The fish eye soup was a great way to see through.\n",
      "case 180, 5:\n",
      "human text: I ' m blind enough to look at the situation in the wrong fashion , so I changed my clothes .\n",
      "LLM output: A blind man said to another 'blindly fashion.'\n",
      "case 180, 6:\n",
      "human text: An old professor who was never on time died and was called the late professor Brown .\n",
      "LLM output: The professor had arrived on time and had already passed away.\n",
      "case 180, 7:\n",
      "human text: Bike sales go in cycles .\n",
      "LLM output: Bike sales cycles have been foiled again.\n",
      "case 180, 8:\n",
      "human text: Reassembling the skeletons of prehistoric mammals can be a mammoth undertaking .\n",
      "LLM output: A prehistoric skeleton had been discovered in the mammoth.\n",
      "case 180, 9:\n",
      "human text: My skiing skills are really going downhill .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|       | 181/810 [00:20<01:25,  7.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM output: Skiing down the downhill was my favorite thing to do.\n",
      "case 190, 0:\n",
      "human text: He stole an invention and then told patent lies .\n",
      "LLM output: The thief stole an invention that had a patent.\n",
      "case 190, 1:\n",
      "human text: When buying a battery you should never have to charge it .\n",
      "LLM output: I bought my car, now I'm charging again.\n",
      "case 190, 2:\n",
      "human text: One evening King Arthur ' s men discovered Sir Lancelot ' s moonshine whiskey operation and shattered the still of the knight .\n",
      "LLM output: Lancelot and his loyal sidekick, Sir Galahad, were still talking about Sir Arthur's latest adventure.\n",
      "case 190, 3:\n",
      "human text: OLD NITPICKERS never die , they just feel lousy .\n",
      "LLM output: Die nitpickers are lousy.\n",
      "case 190, 4:\n",
      "human text: In the office she was frantically looking for her false nails only to discover she had filed them away .\n",
      "LLM output: False nails are filed and away.\n",
      "case 190, 5:\n",
      "human text: OLD GUITARISTS never die , they just fret their lives away\n",
      "LLM output: Guitarists would often fret about the next song on their setlist.\n",
      "case 190, 6:\n",
      "human text: Why did the agricultural presentation go so smoothly ? They planted questions .\n",
      "LLM output: The agricultural presentation was so well-planted that even the questions were planted.\n",
      "case 190, 7:\n",
      "human text: The frog went unnoticed in the milkshake because it blended so well .\n",
      "LLM output: The frog drank a blended milkshake.\n",
      "case 190, 8:\n",
      "human text: OLD WRESTLERS never die they just lose their grip .\n",
      "LLM output: The wrestlers lost their grip on the game.\n",
      "case 190, 9:\n",
      "human text: I couldn ' t quite remember how to throw a boomerang , but eventually it came back to me .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|       | 191/810 [00:31<02:21,  4.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM output: I couldn't remember how I came by the idea of throwing a boomerang.\n",
      "case 200, 0:\n",
      "human text: The bargain store promised a free abacus with every purchase , but I wouldn ' t count on it .\n",
      "LLM output: Every purchase would count on that bargain store.\n",
      "case 200, 1:\n",
      "human text: I was going to buy some loose tea , but the price was too steep .\n",
      "LLM output: The steep price of the tea was steep.\n",
      "case 200, 2:\n",
      "human text: Two cooks disagreed but decided to hash it over .\n",
      "LLM output: The cooks had a disagreement about how to hash it over.\n",
      "case 200, 3:\n",
      "human text: She ' s a composer who has scores of works to her credit .\n",
      "LLM output: A composer scored an impressive number of compositions.\n",
      "case 200, 4:\n",
      "human text: When he fell in the wet concrete he left a bad impression .\n",
      "LLM output: A wet concrete fell on the table and made for a bad impression.\n",
      "case 200, 5:\n",
      "human text: She was only a Electrician ' s daughter , but she certainly had good connections .\n",
      "LLM output: My daughter works as an electricians daughter.\n",
      "case 200, 6:\n",
      "human text: She was only a Blacksmith ' s daughter , but she knew how to forge ahead .\n",
      "LLM output: The blacksmith's daughter forged ahead as they hammers.\n",
      "case 200, 7:\n",
      "human text: A goat that ate office supplies was on a staple diet .\n",
      "LLM output: The goat in the office was staples.\n",
      "case 200, 8:\n",
      "human text: There was a report of shots fired in a local bar . The police don ' t know what triggered the commotion .\n",
      "LLM output: The local bar shot its first shot and was triggered.\n",
      "case 200, 9:\n",
      "human text: When the wino suspected his muscatel was watered down , he needed more proof .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|       | 201/810 [00:41<03:15,  3.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM output: A wine expert tasted a muscatel and found it was watered down proof.\n",
      "case 210, 0:\n",
      "human text: Early stone tools had many problems that were eventually ironed out .\n",
      "LLM output: Early problems were ironed out.\n",
      "case 210, 1:\n",
      "human text: I tried hard to get into vexillology , but , in the end , had to flag it away .\n",
      "LLM output: The vexillologist had vexed the flag.\n",
      "case 210, 2:\n",
      "human text: The decision to begin construction on the Empire State Building was a groundbreaking historical event .\n",
      "LLM output: The Empire State Building was a groundbreaking historical event.\n",
      "case 210, 3:\n",
      "human text: Old lumberjacks never die , they just pine away .\n",
      "LLM output: The lumberjack died of pine away.\n",
      "case 210, 4:\n",
      "human text: An archaeologist ' s career ended in ruins .\n",
      "LLM output: An archaeologist's career was ruined.\n",
      "case 210, 5:\n",
      "human text: The coffee tasted like mud because it was ground a couple of minutes ago .\n",
      "LLM output: A cup full of muddy ground was a cup full of coffee.\n",
      "case 210, 6:\n",
      "human text: She was only a Cattleman ' s daughter , but she couldn ' t keep her calves together .\n",
      "LLM output: A cattleman's calves were still small but would soon be big enough for sale.\n",
      "case 210, 7:\n",
      "human text: When a priest makes a mistake in church it ' s a clerical error .\n",
      "LLM output: A priest made a clerical error.\n",
      "case 210, 8:\n",
      "human text: We were so poor when I was growing up we couldn ' t even afford to pay attention .\n",
      "LLM output: The poor paid attention to his words.\n",
      "case 210, 9:\n",
      "human text: Did you see guys that Bob the ventriloquist hangs out with ? They ' re nothing special , just a bunch of dummies .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|       | 211/810 [00:50<04:07,  2.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM output: The guys who hung out with him were just dummies.\n",
      "case 220, 0:\n",
      "human text: Doctor , doctor , I keep thinking I ' m a deck of cards . - I ' ll deal with you later . Next !\n",
      "LLM output: The doctor kept thinking about dealing with you later.\n",
      "case 220, 1:\n",
      "human text: The ice at the rink has many ruts . I think the maintenance crew is slipping up .\n",
      "LLM output: The maintenance crew was slipping up on the ice.\n",
      "case 220, 2:\n",
      "human text: Two companies that manufactured rulers decided to align .\n",
      "LLM output: Companies aligned themselves with the manufactured rulers.\n",
      "case 220, 3:\n",
      "human text: Early nuclear experimenters discovered an element of surprise .\n",
      "LLM output: A group of nuclear experimenters were examining the elements.\n",
      "case 220, 4:\n",
      "human text: ' ' I ' ve run out of wool , ' ' said Tom , knitting his brow .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|       | 220/810 [00:58<02:37,  3.75it/s]\n"
     ]
    },
    {
     "ename": "JSONDecodeError",
     "evalue": "Extra data: line 3 column 1 (char 66)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[101], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Generation method 2\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mcall_llm_to_generate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mqwen2vl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mqwen2vl_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhom_punDataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexamples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhom_examples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m call_llm_to_generate(model\u001b[38;5;241m=\u001b[39mqwen2vl, model_name\u001b[38;5;241m=\u001b[39mqwen2vl_name, dataset\u001b[38;5;241m=\u001b[39mhet_punDataset, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, examples\u001b[38;5;241m=\u001b[39mhet_examples, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, save\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[98], line 158\u001b[0m, in \u001b[0;36mcall_llm_to_generate\u001b[0;34m(model, dataset, model_name, method, examples, save, path_gen, batch_size)\u001b[0m\n\u001b[1;32m    156\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mpost(url, headers\u001b[38;5;241m=\u001b[39mheaders, data\u001b[38;5;241m=\u001b[39mjson\u001b[38;5;241m.\u001b[39mdumps(payload))\n\u001b[1;32m    157\u001b[0m response \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mjson()\n\u001b[0;32m--> 158\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mchoices\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmessage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSentence\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLLM output: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    160\u001b[0m _outputs\u001b[38;5;241m.\u001b[39mappend(output)\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/json/decoder.py:340\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    338\u001b[0m end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n\u001b[0;32m--> 340\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtra data\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, end)\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Extra data: line 3 column 1 (char 66)"
     ]
    }
   ],
   "source": [
    "# Generation method 2\n",
    "call_llm_to_generate(model=qwen2vl, model_name=qwen2vl_name, dataset=hom_punDataset, method=2, examples=hom_examples, batch_size=10, save=True)\n",
    "call_llm_to_generate(model=qwen2vl, model_name=qwen2vl_name, dataset=het_punDataset, method=2, examples=het_examples, batch_size=10, save=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
